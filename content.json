[{"title":"","date":"2018-09-29T20:04:00.287Z","path":"2018/09/29/README/","text":"My personal BlogCharlesliuyx This is the _posts folder of my blog, for multi-platform editting","tags":[]},{"title":"TensorSpace-3D神经网络可视化框架","date":"2018-09-28T23:00:20.000Z","path":"2018/09/28/TensorSpace-3D神经网络可视化框架/","text":"【阅读时间】文档介绍类文章【阅读内容】TensorSpace - 是一款3D模型可视化框架，支持多种模型，帮助你可视化层间输出，更直观的展示模型的输入输出，帮助理解模型结构，输出方法 Conv2d #FFFF2E 构造器1TSP.layers.Conv2d( &#123; fileter : Int &#125; ) 参数列表⭐️ 必要参数 使用时必须提供，不能为空只要提供了这些参数，就能正常创建实例，控制参数使用默认值 filters : Int 特征提取过滤器的数量 🔧 推荐参数 使用时推荐给定，未给定可以运行，但在体验和易用性上有隐患 name : string 层的命名，强烈建议添加 padding : string 是否使用padding valid(default) 不使用padding，丢弃无法提取特征的部分，会改变输出的形状 same 使用padding，在不够的位置补零 ，输出的形状不变 ⚙️可选参数 根据模型配置参数选择性添加这里的参数对于层的结构（3D可视化外观）没有影响 shape : [Int] 有此参数，会覆盖除了filters的其他参数的影响，Dataformat默认通道值在最后 例如，令shape = [ 28, 28, 6 ]，则表示输出为6个特征图，每幅图大小28*28 kernelSize : Int 卷积核的尺寸，2d卷积核为正方形 strides : Int 卷积移动框的移动步长，在不同方向的步长相等 🎨 外观参数 可覆盖TSP.model.Sequential下的属性进行细调，详情点击 color : color format 层的颜色，默认颜色是亮黄色 #FFFF2E closeButton : Dict 层关闭按钮外观控制列表 display : Bool true 显示按钮 | false 隐藏按钮 ratio : Int 为正常大小的几倍.例如，设为2为正常大小的2倍大1234closeButton &#123; display: false, ratio: 2,&#125; 🎦 动画控制参数 可覆盖TSP.model.Sequential下的属性进行细调，详情点击 initStatus : string 初始化时，本层是否收缩 close(default) : 收缩 open : 展开 animationTimeRatio : Int 张开和伸缩的调整速度，呈倍速关系，例如2就是2倍，数字越大速度越快 参数列表 参数名标签 类型 简介 具体用法细节和例子 filters ⭐️📦 Int 特征提取过滤器的数量 filters: 16 name 🔧 String 层的命名，强烈建议添加 name: &quot;layerName&quot; padding 🔧📦 String 是否使用padding valid[default] 不使用padding，丢弃无法提取特征的部分，会改变输出的形状same 使用padding，在不够的位置补零 ，输出的形状不变 shape ⚙️📦 [Int] 当前层的输出形状 有此参数，会覆盖除了filters的其他参数的影响Dataformat默认通道值在最后例如，shape = [ 28, 28, 6 ]，则表示输出为6个特征图，每幅图大小28*28 kernelSize ⚙️📦 Int 卷积核的尺寸 2d卷积核为正方形 kernelSize: 3 strides ⚙️📦 [Int] 卷积移动框的移动步长 在不同方向的步长相等，默认情况自动推断 strides = [1, 1] color ⚙️🎨 color format 层的颜色 Conv2d默认颜色是亮黄色 #FFFF2E #FFFF2E closeButton* ⚙️🎨 Dict 层关闭按钮外观控制列表 display : Bool true [default] 显示按钮 false 隐藏按钮ratio : Int 为正常大小的几倍，默认为1倍例如，设为2为正常大小的2倍大 initStatus ⚙️🎦 String 初始化时，本层是否收缩 close[default] : 收缩 open : 展开 animationTimeRatio ⚙️🎦 Int 张开和伸缩的调整速度，呈倍速关系 例如2就是2倍，数字越大速度越快 *closeButton 例子1234closeButton &#123; display: false, ratio: 2,&#125; 标签详情 符号 参数性质 说明 ⭐️ 必要 使用时必须提供，不能为空只要提供了这些参数，就能正常创建实例，控制参数使用默认值 🔧 推荐 使用时推荐给定，未给定也可以运行，但在体验和易用性上有隐患 ⚙️ 可选 根据模型配置参数选择性添加这里的参数对于层的结构（3D可视化外观）没有影响 📦 模型 配置卷积层的相关属性，并对输出特征图形状有影响 🎨 外观 可覆盖TSP.model.Sequential下的属性进行细调，详情点击 🎦 动画控制 可覆盖TSP.model.Sequential下的属性进行细调，详情点击 调用结果 创建一个新的对象，往3D场景中添加一个新的3D可视化卷积层 默认颜色: #FFFF2E #FFFF2E 属性.inputShape : [Int] 在 model.init() 后才可拿到数据，否则为undefined 本层输入Tensor的形状，Dataformat默认通道值在最后，例如inputShape = [ 28, 28, 2 ] 表示输入为3个特征图，每个图大小为28*28 .outputShape : [Int] 在 model.init() 后才可拿到数据，否则为undefined 本层输出Tensor的形状，Dataformat默认通道值在最后，例如outputShape = [ 32, 32, 4 ] 表示经过此层处理后，有4个特征图，每个图大小为32*32 .neuralValue : [float32] 载入模型，在 model.predict() 后才可以拿到数据，否则为undefined 本层的层间输出值数组 .name : string 创建后即可取到 本层的自定义名称 .layerType : string 创建后即可取到 本层的类型，返回一个定值，字符串Conv2d 方法.openLayer() : void（1）通过直接和3d场景中物体交互直接点击层打开 （2）代码中通过调用方法打开 12model.init();TSP.layers.Conv2d.openLayer() .closeLayer() : void（1）通过直接和3d场景中物体交互点击按钮关闭 （2）代码中通过调用方法打开 123model.init();// if this layer already openedTSP.layers.Conv2d.closeLayer() 使用样例添加层（1）声明一个Conv2D的实例，方便复用 123456789let convLayer = new TSP.layers.Conv2d(&#123; kernelSize: 5, filters: 6, strides: 1, animationTimeRatio: 2, name: \"conv2d1\", initStatus: \"open\",&#125;);model.add(convLayer); （2）直接添加Conv2D 123456model.add(new TSP.layers.Conv2d(&#123; kernelSize: 5, filters: 16, strides: 1, name: \"conv2d2\"&#125;)); 什么时候用如果你是Keras | TensorFlow | tfjs 框架的使用者，构建模型时使用了卷积层Conv2D（下表列出了可能的使用情景）。一一对应的，在TensorSpace中，你应该使用此API 框架名称 文档格式 样例代码段 Keras keras.layers.Conv2D(filters, kernel_size, strides=(1, 1)) model.add(Conv2D(32, (3, 3))) TensorFlow tf.nn.conv2d(input, filter, strides, padding) x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=’SAME’) TensorFlow.js tf.layers.conv2d (filters, kernelSize) model.add(tf.layers.conv2d({kernelSize: 3, filters: 32, activation: ‘relu’})); 源码tensorspace/src/layer/intemediate/Conv2d.js","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Visualization","slug":"Visualization","permalink":"https://charlesliuyx.github.io/tags/Visualization/"}]},{"title":"【直观详解】通俗易懂了解什么是黎曼猜想","date":"2018-09-21T06:49:58.000Z","path":"2018/09/20/【直观详解】通俗易懂了解什么是黎曼猜想/","text":"【阅读时间】15min - 20min | 9000+字 | 23张动图 | 11张图片【阅读内容】一文搞懂什么是黎曼猜想。在复数域直观可视化黎曼$\\zeta$函数（读作/zita/），解释什么是解析延拓（analytic continuation），探究黎曼猜想和素数的关系 分享者是最大的受益者，感谢各位莅临阅读！ 【说明】本文科普为主，是3B1B视频的笔记。会有很多废话来增加信息冗余度，方便对复分析之类概念不熟悉的读者理解。有基础的读者可以选择跳过一些在你们看来是在说废话的内容。就像重要的事情说三遍一样，废话增加信息的冗余度，减慢节奏，降低认知负担，希望这么做后能有更多人真正看懂什么是黎曼猜想 八卦黎曼 黎曼全名格奥尔格·弗雷德里希·波恩哈德·黎曼 〔德语〕Georg Friedrich Bernhard Riemann（1826 - 1866），德国数学家。黎曼的父亲是个牧师，在他长大之后，就读于哥本哈根大学（德国西南部）神学系 后来在大学听了一场高斯有关最小二乘法的讲座，发现了命中天赋所在。征得父亲同意后，转到柏林大学改修数学，拜入雅可比和狄利克雷门下（雅克比行列式，第一个玩椭圆的人；机器学习中的狄利克雷分布，证明费马大定理 n=5 的人；没错就是这两个大牛） 历史背景不谈历史背景的人物介绍都是耍流氓，好，回过头来看看黎曼生长的德国（当时还不能叫德国）当时在干啥 1806年拿破仑覆灭统治几百年的神圣罗马帝国。1815年以日耳曼民族为主的德意志邦联成立，在这个联邦中，以普鲁士和奥地利最为强大，两大势力得争个高下。一直到1862年脾斯麦执政，才走向一统。最终，1871年，德意志帝国成立，黎曼没能见到这一天 19世纪5、60年代普鲁士完成了工业革命，这必定伴随着思想和科学的蓬勃发展。黎曼是生在一个好时代，这个时代，整个科学领域，一片蓝海，名家辈出，群星璀璨 有意思的是，K12这个名词就发源于普鲁士。K指Kindergarten，及幼儿园，12指12年级，也就是高三。K12泛指基础教育。虽然这工业化的流水线教育模式可能更多的是为了培养流水线式的工业人才或士兵，但不能否认，重视教育则学界兴盛 成就黎曼成就斐然，最有名的当然是黎曼几何（积分），黎曼流形和复分析之父。当然还有本篇文章的主人公，1859年提出的黎曼猜想 黎曼猜想被收录进1900年希尔伯特（Hilbert）提出的23个重大难题，这些难题经过100年的岁月，还剩下6道没有被完全解决 【补充1】因为代数几何中有关椭圆曲线的相关研究还没有兴盛，著名的费马大定理：$x^n + y^n = z^n \\; 当\\;n&gt;2\\;没有整数解$ 未出现在列表中，虽然当时这个猜想也没有被证明 【补充2】在希尔伯特的问题列表中，黎曼猜想并不单独为一题。包含黎曼猜想的第8题是：黎曼猜想及哥德巴赫猜想和孪生素数猜想，这每一个猜想都闻名遐迩。三位一体，由此可见这三个问题之间是存在关联的 21世纪黎曼猜想又被列为千禧年7道世纪难题之一，克雷数学研究所承诺：解决一道题 ➜ 100万美元 P/NP问题 | 霍奇猜想 | 庞加莱猜想（已证明）| 黎曼猜想 | 杨-米尔斯存在性与质量间隙 | 纳维-斯托克斯存在性与光滑性 | 贝赫和斯维讷通-戴尔猜想 黎曼猜想的专业定义先搬运Wiki百科对黎曼猜想的定义：黎曼$\\zeta$函数，$\\zeta(s) = \\frac{1}{1^s} + \\frac{1}{2^s} + \\frac{1}{3^s} + \\frac{1}{4^s} + \\cdots$ 的非平凡零点（在此情况下是指 $s$ 不为-2，-4，-6…等点的值）的实数部分是 $\\cfrac{1}{2}$ 这个表述可以继续简述为：所有黎曼$\\zeta$函数非平凡零点的实数部分是 $\\cfrac{1}{2}$ 【补充（只为严谨，可以跳过）】把上面的式子称为黎曼$\\zeta$函数并不严谨，严谨的来说：定义域必须纳入考虑，才能完整写出黎曼$\\zeta$函数的形式 设一复数s，其实数部份 &gt;1 ${s: Re(s) &gt; 1}$ 且$$\\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s}$$ 它亦可以用积分定义$$\\zeta(s) = \\frac{1}{\\Gamma(s)}\\int_{0}^{\\infty} \\frac{x^{s-1}}{e^x - 1} dx$$ 在区域{s : Re(s) &gt; 1}上，此无穷级数收敛并为一全纯函数。黎曼认识到：$\\zeta$ 函数可以通过解析开拓来扩展到一个定义在复数域 $ s, s≠ 1$上的全纯函数 $\\zeta(s)$ 相信大部分读者如果是第一次看到这个定义，估计头都大了，但是列出不明白概念的清单是学习一个全新事物的有效办法，就按照这个思路来列一个清单 ❓ 黎曼$\\zeta$函数是个什么函数？ 若读者的数学基础为高中，那么博主猜测您有疑惑的是…这个符号，它的含义很简单：按照1 2 3 4每次加1的规律一直重复前一项的形式。举个例子： $$\\zeta(s) = \\frac{1}{1^s} + \\frac{1}{2^s} + \\frac{1}{3^s} + \\frac{1}{4^s} + \\frac{1}{5^s} + \\frac{1}{6^s} + \\frac{1}{7^s} + \\frac{1}{8^s} + \\cdots = \\sum_{n=1}^{\\infty} \\frac{1}{n^s}$$ ❓ 什么是非平凡零点？ 本博文想说清楚的问题 ❓ 实数部分指的什么？难不成还有虚数部分？ 这部分涉及虚数的概念，虚数定义为 $\\sqrt{-1}$ 记为符号 i，接下来就需要一些复数基础，这一点跳不过，但本博文也尝试帮你解决这个问题 ❓ $\\frac{1}{2}$ 这个数是怎么来的？ 本博文想说清楚的问题 从知识构建的角度来说，搞清楚黎曼猜想的知识网父节点有基础复分析（知道复数以及如何分析）和微积分中的求导 还有另一个说法也和黎曼猜想有关，即 $1 + 2 + 3 + \\cdots = - \\cfrac{1}{12}$ ，怎么看这个式子都应该是无穷大啊，为啥等于 $-\\cfrac{1}{12}$ 呢？看完后，你应该就能明白为什么会有这个令人不解的说法 可视化黎曼$\\zeta$函数 第3部分就一步一步展开黎曼猜想这副瑰丽的“画卷”，希望在图穷过程中，能带来给您带来几个Aha时刻，感受数学之美 黎曼$\\zeta$函数首先，我们为了逻辑链的完整，先用一副动图再定义一下我们的主角黎曼$\\zeta$函数，并且假设我们带入$s=2$ 会是什么情况 你可以继续带入其他值，如果 $s&gt;1$ ，可算出一个确定的值，但你会发现如果 $s&lt;1$ ，那这个无穷级数（级数就是一长串数字 or 序列的数学专有名词）就会越加越大，无法收敛，又称发散 参考上面的动图，带入负数，明显越来越大呀？其实这里和定义域的选取有关，黎曼$\\zeta$函数只有在 $s&gt;1$ 的时候能求出值（收敛），这个函数才有意义，那么定义域外的情况怎么处理呢？ 如果你对黎曼猜想研究过，可能看过类似的结论 $\\zeta(-2n) = 0$ 和 $\\zeta(-1) = \\cfrac{1}{12}$ ，这又是为啥呢？ 定义域扩展到复数在传统的定义域中，就是把 $s$ 作为输入带入黎曼$\\zeta$函数，重新映射到数轴上的另一个数上，如下面的动图所示。如果你很好奇为什么这个级数的和是 $\\cfrac{\\pi^2}{6}$，这个数从何而来，另一篇博文会解答这个问题（3B1B的另一个视频的总结笔记），我会晚些时候更新 黎曼做了一个扩展，他说：如果 $s$ 能取到复数会是一种什么情况呢？先添加一个复平面，并另 $s=2+i$，过程参看下面的动图 所谓定义域扩展其实非常好理解：之前 $s=1$，它是一个实数。现在让 $s = 2 + i$ ，变成一个复数，这就是复数域扩展 ❓这里可能会出现两个问题 ➜ ① 什么是复平面？ ② $\\left(\\cfrac{1}{2}\\right)^{2+i}$ 怎么计算？几何含义是什么？ 复平面复数是拥有实部和虚部的表示法，写为 $a + bi$ ，$a$ 为实部，$b$ 为虚部， $\\sqrt{-1}$ 定义为 $i$，称为虚数单位 。而复平面（complex plane）是用水平的实轴与垂直的虚轴建立起来的复数的几何表示，如下图所示（来源维基百科） 研究复数有什么意义？其中有一点和我们这个主题有关，$i$ 虚数单位和幂指数函数勾连起来在复分析中能连接上旋转这个概念。具体来说，参看我的这篇【直观详解】让你永远忘不了的傅里叶变换解析博文 虚数单位为幂指数 这一小节的思路非常重要，不仅仅对理解黎曼猜想有帮助，对信号分析，傅里叶变换等也非常有帮助 $2^x$ ，这个 $x$ 就是幂指数，$2^i$ 即虚数单位为幂指数 第二个问题，我们可以把 $\\left(\\cfrac{1}{2}\\right)^{2+i}​$ 拆开写成 $\\left(\\cfrac{1}{2}\\right)^{2}× \\left(\\cfrac{1}{2}\\right)^{i}​$ ，前面一半很好理解，关键是后面一半怎么理解 这里涉及到一个非常基础并且十分重要的理念：复平面中，纵轴（虚数部分）的幂指数函数的映射关系代表的是旋转。一下子不懂没关系，下面有通过两幅动图来帮助理解，如果还是有疑惑并且十分想了解，参看博文复平面和旋转 首先，下面这副动图表示，假设我们把指数 $i$ 前加一个自变量 $t$ ，就构造了一个函数 $f(t) = \\left(\\cfrac{1}{2}\\right)^{ti}$ ，可以看到左边的黄色点在纵轴（虚轴）上移动，表示的就是引入一个自变量 接着，我们把左边的输入带入 $f(t)$ 得到右边的output像空间，即 $\\left(\\cfrac{1}{2}\\right)^{ti}$ 的值。会有下面一副动图所示的对应关系（移动黄色点，粉色点作为输出联动）。如果改变底数 $\\cfrac{1}{2}$ ➜ $\\cfrac{1}{9}$ ，左边黄点移动的时候，右边粉点的旋转速度变快，这就是幂指数函数在复数域上的映射规律 总结一下，以上对两个问题的阐述是为了建立一个直观概念：幂指数是虚数单位的乘法对应了复平面内的旋转，接下来一张动图就来看看 $\\left( \\cfrac{1}{2} \\right) ^{2+i}$ 是怎么算的：（关注红色线段，即最后的结果） ① 实部把点收缩到 $\\cfrac{1}{4}$ 的位置 ② $\\left( \\cfrac{1}{2}\\right)^i$ 不改变长度（因为是虚部），只旋转一个对应的角度 ⭐️一步一步可视化进行旋转下面这副动图非常重要！先把实部部分加起来，再对每一项还需要乘一个 $(\\cfrac{1}{2})^i$ ，也就是每一个线段都需要进行一个同样角度的旋转 这副动图可以多看几遍，应该是挺好理解的。这里有个很细节的问题：第一段线段没动，是不是意味着没有进行旋转呢？，对应相乘的部分是 $1^i$ ，这可能意味着这个旋转恰好是转过了一圈（时间有限，未能求证，大概率是这样理解） 接下来的动图展示了在 $s$ 变化的过程中，对应的螺旋线的变化情况，这幅动图看的有点上瘾 考虑定义域的话，实数部分要能收敛：即在下图右侧黄色高亮区域， $s$ 的实部大于1 变换黎曼$\\zeta$函数理解这个复函数一个好方法是通过变换来将其可视化，即将复函数看作变换，为了加深理解，在变换黎曼$\\zeta$函数前，先变换一个比较简单的函数$$f(s) = s^2$$按照下面的动图，带入 $2$ 得到 $4$ ，带入 $-1$ 得到 $1$ ，带入 $1i$ 得到 $-1$ 最后把所有的网格都标记彩色，下一幅动图同时变换网格上所有的点，形成新的网格： 同时观察所有点的变换比较吃力，你可以尝试在看的时候关注一个点的变化过程，比如关注 $(-1, 0)$ 这一点：它逆时针旋转了180° 。这副动图给了我们丰富的信息来直观的展现复函数变化到底做了什么 同理，可视化黎曼$\\zeta$函数，如下列动图所示 如果这么漂亮的图像都完全无法激起你继续钻研复分析的兴趣，那么…… 难受的停顿你可能已经发现了，变换的图像左边有一个十分突兀的切面，停顿的很不自然，整个图像一场明显的表露除了一种希望冲破定义域的渴望 那么，我们专门高亮两条线：虚部等于 $i$ 和 $-i$ 的两条横线，然后进行变换。难道你没有冲动去补全它吗？ 解析延拓可以去想象，在 $Re(s)=1$ 的左半边，有一个改良版的函数（即下图蓝紫色的函数），可以完美的补全整个空间 用数学的形式来表示的话，问题就变成了，在左边一半的定义域内，这个函数是什么的问题，如下图（Re表示实部） 此时，我们就可以可视化出开篇提到的那个表达式 $\\zeta(-1) = 1 + 2 + 3 + \\cdots = - \\cfrac{1}{12}$ ，你现在知道这个公式怎么来的了，因为钻了定义域不同函数形式不同的空子，其实上面这个式子的论断是很荒谬的 新的问题又出来了，补全的部分，如果没有条件限制，随便怎么画都行，补全这条路难道走不通? 真的是这样吗？并不是，黎曼$\\zeta$函数自带一个限制（约束）条件：函数是解析函数，补全部分形式处处可导 这里有一个更加优雅的方法来理解处处可导（解析）这个条件 我们先来看 $f(s) = s^2$ ，它的导数形式是 $f’(s) = 2s$ ，在可视化部分来看，处处可导等价于变换后任意两条线段的夹角不变，又称保角（保证交角不变）特性，参看下图 这个规律在所有网格线中都成立，所以，解析的 = 保持交角不变，即可以把解析的理解成保角的。如果你是一个追根究底的人，就能发现其实还是有例外的，比如在原点的交角变换后呈整数倍的关系，没有保角特性 下面给一副平移的动图，解析函数处处保角 黎曼$\\zeta$函数就是一个保角函数，或说解析函数，网格出处垂直，处处可导 因为黎曼$\\zeta$函数是一个解析函数，那么要想在左定义域延拓，又要满足解析的性质，有且仅有一种延拓方法，这也是解析延拓的含义 可视化黎曼$\\zeta$函数总结按照解析延拓的方式进行了补全操作后，我们就走完了黎曼$\\zeta$函数的可视化过程了，总结一下逻辑链 右定义域内假设自变量为复数，扩展到复数域 ➜ 进行变换，获得复数域可视化形态 ➜ 左定义域内无意义，进行解析延拓 另，这里补充左半边延拓的解析形式，数学家们已经给出了解$$\\zeta(s) = 2^s{\\pi}^{s-1}\\sin(\\frac{\\pi s}{2})\\Gamma(1-s)\\zeta(1-s)$$ 素数规律和黎曼猜想再看黎曼猜想在有了上述的直观理解后，我们再反回来看看黎曼猜想 有了变换的思维后，那么在这个变化后哪些点会落在原点呢？这个问题非常关键，因为它和求黎曼$\\zeta$函数的零点等价 首先，所有满足 $Re(s) = -2n$ 的点都会落在原点，这些点被称为平凡零点（根据数学家的传统，他们太容易被发现了，太好理解了，所以被称为“平凡”） 那么非平凡零点呢？我们已知所有的非平凡零点都落在下图的这个临界带（Critical Strip）中。至于原因，如果再仔细看一下这个复数域可视化变换所有点移动的趋势，大概就知道为什么这么说了，简单来说，是复分析变换计算出来的结果 更加令人不可思议的是，这些非平凡零点的具体分布，蕴含着有关素数的海量信息。至于为什么有素数的海量信息，之后会写一篇博文讲讲这其中的奥妙（3B1B的另一个视频的总结笔记） 黎曼猜想就是在说：这些非平凡零点，都在实部 $Re(s)=\\cfrac{1}{2}$ 的这条临界线（Critical Line）上，如下图所示。如果它成立，那么它能让我们深刻理解素数分布的规律，根据最新进展中和其他证明，这个规律应该符合某种分布的均匀分布 假设在变换过程中高亮 $Re(s)=\\cfrac{1}{2}$ 这条线，以我们可以看到的可视化区域（就是动图中前面跳动的一下的部分）的变换过程如下图所示，貌似它并没有过零点？ 其实不然，这个动图只绘出了可视区域内的线段的变换结果，如果我们把这个线段加长（不理解可以参考上面的动图，黄色就是可视化时候原图像的线），就得到了下面一个动图了 其中，如果你能证明所有的非平凡零点都在这条临界线上（也就是原命题中的 $\\cfrac{1}{2}$），那黎曼猜想变成黎曼定理！同时你也证明了成百上千的现代数学结论，当然，还有100万美元的奖金 有趣的是，现在很多现代数学理论的证明，不管黎曼猜想是正确还是不正确都能被证明是正确的。看到一个叫做littlewood定理的证明就是这样，可算是数学奇妙的冰山一角了 素数规律之前提及黎曼猜想中蕴含着海量的素数信息。并在开篇有说到，1900年希尔伯特的23大难题中，黎曼猜想、哥德巴赫猜想和孪生素数猜想同为第8题，寻找素数（分解质因数后只有1和他本身的数），素数分布的规律，质因数分解，还有复分析之间一定是有千丝万缕的联系的 建议大家可以观看李永乐老师的黎曼猜想第二期视频，将素数规律讲的非常好，我这里就当好学生，做一些笔记 素数个数素数到底有多少个呢？这个问题已经被确定回答了，答案是有无穷多个。那是谁证明的呢？由欧几里得（他是公元前300年的人）证明，使用的是反证法，怎么说的呢？ 设质数的个数是有限的，那么就有一个最大的自然数 $p$，可以写成一个素数序列： $2,3,5,\\cdots,p$，令$$q = 2×3×5×\\cdots × p + 1 \\tag 1$$① 假设 $q$ 是质数 ➜ $q\\gt p$ 这和 $p$ 是最大的质数这个假设矛盾 ② 假设 $q$ 是合数（不是质数的数） ➜ $q$ 是有约数的，不是1也不是它本身 ➜ 那就一定是(1)式中 $2×3×5×\\cdots × p$ 中的某一个，但是由（1）式可得，$q$ 除以$2×3×5×\\cdots × p$ 中任何一个数都余1 ➜ 所以肯定不能整除，与假设 $q$ 是合数矛盾 证毕。这个证明简洁而优雅，数学之美牛皮 欧拉乘积公式神人欧拉（1707-1783）出现，推导出了欧拉乘积公式，怎么个说法呢？ 假设 $p$ 表示全体素数，有下面一个公式成立$$\\prod\\limits_{p} (1-p^{-s})^{-1} = \\frac{1}{1-\\cfrac{1}{2^s}}×\\frac{1}{1-\\cfrac{1}{3^s}}×\\frac{1}{1-\\cfrac{1}{5^s}}×\\cdots = \\cfrac{1}{1^s}+\\cfrac{1}{2^s}+\\cfrac{1}{3^s}+\\cdots$$ 那这个公式有什么用呢？它告诉我们，黎曼函数和质数之间有隐含的关系。左边是和所有质数有关的项的乘积，右边是黎曼$\\zeta$函数 素数定理假设有这么一个表达式 $\\pi(x)$ 表示小于 $x$ 素数的个数，有这么一个规律，参见下图 啥意思呢，横坐标就是自变量 $x$ 的取值，蓝色的线是 $\\cfrac{\\pi(x)}{\\frac{x}{\\ln(x)}}$ ，红色的线是 $\\cfrac{\\pi(x)}{\\int_{2}^{x}\\frac{1}{\\ln t} dt}$ ，分母的 $\\int_{2}^{x}\\cfrac{1}{\\ln t} dt$ 又被称成 $Li(x)$ 可以看到，当 $x \\to +\\infty $ ，这个 $\\pi(x)$ 函数是可以写出表达式的 二号神人高斯（1777-1855）研究了一下关于素数密度 $\\rho$ 的问题，也就是1000个数里面，有多少个素数。对的，上面蓝线的规律是高斯最早发现，但当时高斯觉得这个发现貌似并不重要，就没有展开来研究。后来1798年勒让德（1752-1833）发现了下面那个红色曲线的表达式，在学界有个涟漪，高斯在1849年就告诉勒让德，你这不行，是我先发现的啊，所以这公式被称为高斯-勒让德公式$$\\pi(x) \\sim \\frac{x}{\\ln x} \\iff \\pi(x) = \\int_{0}^{x} \\frac{dt}{\\ln t} + C$$$\\sim$ 符号表示趋近于，也就是当 $x \\to +\\infty $ 的意思。 $C$ 是一个常数，这个常数随着 $x$ 的变大而越来越小 再观察上面的图，明显发现红色的线收敛到1的速度更快，所以后面科勒（1870-1924）做了改进和提高。他说，如果黎曼猜想成立（写到这句话可是真不容易），那么这个关系误差式可更加精确，可大大改善素数定理误差的估计$$\\pi(x) = Li（x）+ O(\\sqrt{x}\\ln x)$$$O()$ 被称为渐进符号，一般用来描述无穷级数的余项。在计算和表示算法复杂度方面也很用，比如$O(n^2)$ 其实就是忽略 $n$ 一次项和常数项的意思。因为在 $n$ 非常大时， $n$ 一次项对数值的贡献在量级上远小于 $n^2$ 二次项。这个余项的常数项的具体数值还没有算出来 之后50年，这个素数猜想被证明了出来，变成了素数定理。有趣的是，这份证明只是数学家研究黎曼猜想的边角料 素数，自然底数，虚数单位 $i$ 之间一定是存在的一些难以名状的关联，现在看来，有没有可能是量子力学叠加态在数理逻辑推理中的一种巧合的具象模式呢？静待未来，让数学家们给我们一个答案吧。胡适先生说过：哪管它真理无穷，进一寸有一寸的欢喜，切实能感同身受，可能就是这辈子最大的幸事之一了吧？ 后记和思考就在写作这篇博文的过程中，爵士阿蒂亚的证明过程的手稿已经公布（有点存疑是草稿，原文里面竟然有错别字weakly ➜ weekly），公开大会也已经结束，一张PPT证明黎曼猜想，有点诡异 但其中提及黎曼猜想和量子力学的关联给了我一些启发：素数和微观世界的规律一定有某种关联（在级数和等于 $\\cfrac{\\pi^2}{6}$ 部分的可视化解说里面就有很奇妙的规律关联） 无极生太极，其小无内，其大无外也，两面都是宇宙本源的运行规律，中华民族老祖宗《易经》已经有这种思维方法了，一面是精细结构常数，一面是引力常数 参照弦理论，高维度空间坍缩在很小的尺度内。那能不能猜想，正因为微观世界和高维度尺度更接近，导致被影响的程度也不一样，引力才一直没有统一（而其他三个力的规律已经统一）。相对应的，大尺度上的规律因时间尺度的限制（宇宙的寿命），我们作为人类从观测角度上来说，尺度太小。如果等葛立恒数年后，引力部分也会有一个类似微观世界的规律被发现呢？ 这篇文章提到引力常数 $g$ 更加令人疑惑，是不是可能这个常数本身就是由两个量构成？大的那个符合微观规律，细调的那个因为观测受限（尺度太小）我们无法找到佐证的依据？现在人类追求佐证和实验，有没有可能这条路本身就是障碍？ 最后还是希望直观详解这个系列能激起更多人的好奇心就心满意足了，附上一份目录 博客目录汇总（更新中） 【参考和来源】 所有动图来自：3B1B的视频【官方双语】黎曼ζ函数与解析延拓的可视化李永乐老师1+2+3+4+…=-1/12？李永乐老师讲黎曼猜想（1）Youtube质数有多重要？数学家欧拉和高斯是如何研究质数的 ？李永乐老师讲黎曼猜想（2）wiki百科黎曼猜想 最后，和3B1B视频一样，来个看完彩蛋：黎曼$\\zeta$函数的导数的可视化动图","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Visualization","slug":"Visualization","permalink":"https://charlesliuyx.github.io/tags/Visualization/"}]},{"title":"支持币与去中心化商业模式","date":"2018-08-11T20:23:07.000Z","path":"2018/08/11/支持币与去中心化商业模式/","text":"【阅读时间】【阅读内容】本文阐述了一种全新的支持币区中心话商业模式，以人为本，从人与人的关联出发，辅以各类动态平衡算法，构造一个基于互联网工具的知识经济系统 结构化笔记软件：幕布 介绍愿景提升信息承载物的表意维度，掀起结构化记录的清单革命 表意维度语言，文字，图片，音频，视频都是信息的载体。但语言和文字有一定局限性，具有认知方言特性 【认知方言】有着不同认知的人在看待同一个表意词汇，句子，段落时可能理解出来的重点，含义是不同的 苏格拉底一生未留下文字，他的著作都是由学生柏拉图等进行转述和记录。他认为写作是不可信的 孔子的学生子贡也说：“夫子之言性与天道，不可得而不闻也”。一些思维，一旦写成文字或多或少会产生一些误解和歧义","tags":[{"name":"BlockChain","slug":"BlockChain","permalink":"https://charlesliuyx.github.io/tags/BlockChain/"},{"name":"MuBu","slug":"MuBu","permalink":"https://charlesliuyx.github.io/tags/MuBu/"}]},{"title":"科普OpenAI5v5机器人的表现和算法思路","date":"2018-06-26T00:17:45.000Z","path":"2018/06/25/科普OpenAI5v5机器人的表现和算法思路/","text":"【阅读时间】【阅读内容】","tags":[{"name":"Dota2","slug":"Dota2","permalink":"https://charlesliuyx.github.io/tags/Dota2/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"https://charlesliuyx.github.io/tags/Reinforcement-Learning/"}]},{"title":"那些值得一看的TED演讲附全文文稿笔记","date":"2018-06-21T00:26:25.000Z","path":"2018/06/20/那些值得一看的TED演讲附全文文稿笔记/","text":"【阅读时间】现有12个TED演讲 | 持续更新 - 06/20【阅读内容】这可能是有史以来干货最多的TED分享笔记安利文章，个人林林总总花费时间超过100小时。都是TED的干货笔记，并且有内容精炼，帮你筛选TED优质的演讲 核心思路和愿景我为什么要做这个总结，并且分享？ 内容精炼TED内容精炼推荐，给你有价值的内容，帮你省下选择的时间，我虽然并不能收钱，但是总结过程本身就是我学习的过程，何乐而不为，独乐乐不如众乐乐。 同时这也是对自己知识架构的健壮和拷问，在不断的学习和重复中，能把这些技巧和内容慢慢变成自己的洞见。不断的生产内容，是一种难得的锻炼 分享者才是最大的收获者！我一直笃信这一点 知识数据库TED内容再整理，方便查阅，方便检索，方便重复，最终能变成你知识体系的一部分 这个时代最重要的就是数据，这一些内容也是我个人生产的数据源头，我把它称之为【知识数据库】，只有不断的重复，才能接近智慧 演讲才是精髓TED演讲，还是在演讲上，只有听，才能感受到它的价值：声音的艺术，交流的艺术，表达的艺术，新颖的观点和出色的内容都在演讲这个核心之后 幕布是内容的良好承载物本篇文章非常适合用幕布的形式进行呈现，这里给出链接，欢迎大家分享这个链接，并且最重要的也是这个链接，我强烈建议从这个链接去观看整篇文章（这篇博客更多的文案是为了我自己关于TED演讲推荐知乎回答做的准备） 作为幕布的开发者，布道幕布这种层次化梳理的表现知识承载形式，好的内容是有层次的，应该有更统一的，更直观的，更多维度的内容承载形式。也就是设计幕布七哲学的核心意义 幕布七哲学 TED演讲推荐【01】下一个科学界大突破是什么 【体量】 68主题 | 1440字 | 视频时长 10:51 视频链接 【内容】 以三个生物学界的发明和不断进步的故事为主线 围绕生命科学的技术突破给出了自己的观点 文稿精华总结笔记 【推荐点】 演讲风格有魅力，用词精准漂亮 内容上三个故事都有趣味性，但观点上见仁见智 【02】游戏奖励大脑的七种方式 【体量】 66 主题 | 1137 字 | 视频时长 16:02 视频链接 【内容】 总结游戏设计哲学，提到十分精练的7个原则：用进度条度量进程 | 长期与短期目标 | 奖励成就 | 清晰反馈 | 不确定因素 | 提升注意力窗口 | 好胜心 文稿精华总结笔记 【推荐点】 用词十分漂亮，值得学习 观点很有借鉴意义，从游戏角度来看事物本来就是对人性的一种挖掘 【03】未来这台电脑会帮你种菜 【体量】 67 主题 | 1322字 | 视频时长 15:56 视频链接 【内容】 全新的农业模式，建造数字农场来连接全世界的粮食产业，可以结合大数据互联网等，提高农业的种植效率，强烈推荐！ 文稿内容总结笔记 【推荐点】 非常有创造力的想法，不得不感叹，这世界有太多人在尝试去做一些伟大的任务，远远超出了我们想象 可以帮助你去了解未来世界的可能面貌 【04】下一次工业革命就在眼前 【体量】 51主题 | 812字 | 视频时长 12:27 视频链接 【内容】 下一次工业革命的基本特征进行的阐述，观点角度简练新颖 文稿内容总结笔记 【推荐点】 对整体工业化未来高科技领域有兴趣的读者可以关注 观点简介明了，有启发意义 【05】人工智能将如何推动第二次工业革命 【体量】 142主题 | 4412字 | 视频时长13:44 视频链接 【内容】 凯文凯利就《失控》《必然》几本书种的几个主要观点进行了阐释 主要说明这是一个变革的时代，我们有三点认知需要认清：到底什么是智能 | AI会推动第二次工业革命 | 我们要和AI协作，不是对抗 文稿内容总结笔记 【推荐点】 无论是演讲技术，还是文案，都十分引人深思 从头到尾到泛着大道至简的简约美感，强烈推荐！ 【06】区块链将如何改变金钱与贸易因为整理者是区块链行业者所以只有简单的笔记 【体量】 23主题 | 241字 | 视频时长 18:49 视频链接 【内容】 什么是区块链，区块链有什么应用，区块链的价值在何处 文稿内容总结笔记 【推荐点】 关于区块链的内容非常丰富，适合不太了解区块链并想要被布道的人 例子通俗易懂，能让人对区块链有一个自己的主观认识 【07】怎样说话人们才会听 【体量】 79主题 | 1603字 | 视频时长 9:55 视频链接 【内容】 有关如何说话，总结说话七宗罪：流言蜚语 | 评判 | 消极 | 抱怨 | 借口 | 说谎 | 固执己见 四个说话的正面HAIL：诚实 | 真实 | 正气 | 爱 还有6个用来增强说话力量的工具，音域 | 音色 | 韵律 | 语速 | 音调 | 音量 文稿内容总结笔记 【推荐点】 演讲教科书，声音运用自如，语言精炼，用词精彩，强烈推荐！ TED观看排名第六（2800万+） 【08】如何成为一个更好的交谈者 【体量】 136主题 | 5664字 | 视频时长 11:44 视频链接 【内容】 总结了十条成为更好交谈者的要点：不要三心二意 | 不要好为人师 | 使用开放式问题 | 顺其自然 | 不知道就是不知道 | 不要以己度人 | 尽量别重复 | 少说废话 | 认真倾听 | 简明扼要 文稿内容总结笔记 【推荐点】 演讲技巧和文稿结构同样值得推荐，整个论证过程严丝合缝，举例子 + PPT整合，是一场教学式演讲 TED观看人数排名靠前的演讲，强烈推荐！ 【09】学校扼杀创造力 【体量】 151主题 | 8656字 | 视频时长 19:22 视频链接 【内容】 对学校教育进行了深刻的思考，培养人才需要：多样化 | 充满活力 | 个性化，对现有的体系进行了深刻的批判 文稿内容总结笔记 【推荐点】 TED观看人数排名第一的演讲（5000万+），强烈推荐！ 整个演讲过程十分风趣幽默，充满了西方式幽默哲学 【10】为什么好的领导者会让你感到安全 【体量】 139主题 | 5408字 | 视频时长 11:56 视频链接 【内容】 内容也充实，例子有力量，主要提及了如何才能提升领导力 归纳来说，还是同理心和自身强大的毅力和美好品，承担困难的超强能力 文稿内容总结笔记 【推荐点】 展现了精湛的语言发声艺术和演讲技巧，煽动性相当强 举例子虽然不新颖，但是整体表达的方式值得学习 【11】姿势决定你是谁 - Amy Cuddy 【体量】 134主题 | 7205字 | 视频时长 21:56 视频链接 【内容】 文稿内容总结笔记 演讲中提到了姿势了力量，个人实践表示，是100%有效果的 更高层次的说也就是转念的力量，是一种心理学暗示。只是通过了姿势这种对激素分泌有影响的具象形式表示出来，相信其实更加重要 【推荐点】 TED排名第二观看的演讲（4700万+），强烈推荐！ 虽然在中段稍显结构松散，但是这个演讲最厉害的是真诚和打动人的能力，让您感受到被震撼的力量，内容本身放佛已经不再那么重要，一次演讲能做到让人心潮澎湃，当真厉害！ 【12】伟大的领袖如何激励行动 【体量】 207主题 | 8909字 | 视频时长 17:58 视频链接 【内容】 黄金圈法则的出处（Why How What 思考模式），虽然视频古老，但是内容实沉 文稿内容总结笔记 【推荐点】 TED观看数量第三的演讲（3900万+），强烈推荐！ 演讲技巧也十分有煽动性，节奏清晰 观点上有超强的启发意义和智慧系统贯通的满足感，因为道法自然，说道深处，很多方法论都是相同的，无论是西方还是东方的，都是如此","tags":[{"name":"Note","slug":"Note","permalink":"https://charlesliuyx.github.io/tags/Note/"},{"name":"TEDTalk","slug":"TEDTalk","permalink":"https://charlesliuyx.github.io/tags/TEDTalk/"}]},{"title":"【区块链】共识算法与如何解决拜占庭将军问题","date":"2018-03-03T22:17:06.000Z","path":"2018/03/03/【区块链】如何解决拜占庭将军问题/","text":"【阅读时间】10min 3183 words【阅读内容】什么是拜占庭问题，以及它和区块链的关系，现代区块链技术中常见的共识算法总结 首先，这个问题属于计算机科学领域。解决这个问题才是区块链最大的价值所在，因为这个问题一直是分布式系统的重要难题之一 什么是拜占庭将军问题这个问题的定义者是图灵奖获得者，Lamport大神，分布式系统的关键性奠基人之一。有面包店算法，拜占庭将军问题，Paxos算法等著名成果 问题描述 9个将军带领9支军队，打一场攻城战役。假设每个将军都能独立根据眼前战况做出两种判断：进攻或撤退，要求（或者最终目的是）如何让这9个将军的命令是一致的（一致性，即共识）？要么一起进攻，要么一起撤退（每个将军之间也是互不信任的，也有消灭对方的动机） 最简单的策略即：投票（上图中的红色箭头和绿色箭头为每个将军做出的判断），超过半数支持某个决定，那么所有9个将军一定执行这个决定。如上图，5个将军决定进攻，4个将军决定撤退，那么所有将军都会下令：进攻！ 这种策略需要每个将军把自己的判断通过一种途径（途中灰色箭头）传递到所有其他将军处。相对的，每个将军只有在收到了所有投票结果后，才会下令。如上面的例子，所有将军得到投票：4进攻5撤退，才下令撤退 这个投票策略的最大问题：假设出现了叛徒，如上图所示，会出现两种情况 【1】对自己位置的战场情况进行错误广播（比如他这个地方优势很大，但是投票给撤退） 【2】可以选择靠给不同的将军送去不同的消息破坏整体决定的一致性（导致左边四个将军选择撤退，右边四个将军选择进攻） 问题总结此时总结一下，拜占庭问题的问题到底是什么： 所有将军如何才能达成共识去攻打（或撤退）城堡 根据相关的研究，得出一个【一般性的结论】：如果叛徒的数量大于或等于三分之一 ，那么拜占庭问题不可解，这个三分之一也被称为拜占庭容错，三模冗余是完全无法容错的（也就是说无解，不可能保持一致性） 解释方法使用副官模型即可 推广到计算机系统内，【将军】类比为【计算机】，而计算机因为物理或被感染等其他原因造成的【运行异常】就是【叛徒】，其实整个问题也是为了保证分布式系统的一致性和可用性 传统解决方案在区块链之前，有两种解决方案：口头协议（又称为拜占庭容错算法）和书面协议 通常来说，大多数分布式系统使用的是书面协议确保一致性，中心机构背书。其中有实用拜占庭容错算法（PBFT）最为有名 PBFT概述这个算法说起来也不难理解，他的核心思想是：对于每一个收到命令的将军，都要去询问其他人，他们收到的命令是什么。也就是说利用不断的信息交换让可行的节点确认哪一个记录选择是正确的，即发现其中的背叛者 采用PBFT方法，本质上就是利用通信次数换取信用。每个命令的执行都需要节点间两两交互去核验消息，通信代价是非常高的。通常采用PBFT算法，节点间的通信复杂度是节点数的平方级的 白话PBFT还是用上面的将军的例子来举例，但为了方便我们把问题的定义稍作修改 【问题定义】总共4个将军，有1个是叛徒，每个将军需要在自己的战斗计划中添加一行内容 &lt;什么时间&gt;进攻 【目标】只需要3个将军达成一致在同一时间进攻，就可以攻占城市，否则进攻者全军覆没。最终目标还是统一一个一致的战斗计划，并按照计划同时实施（在去中心化系统中，即【记录】的一致性） 【方法】对于每一个收到命令的将军，都要去询问其他人，他们收到的命令是什么。在判断不出判断者的情况，执行更多的那个命令 【可视化直观】 其中每个将军投降下方的数字就是收到的攻击事件列表，在该规则下，可以看到能保证，当叛徒数量小于1/3维护系统的一致性，即无论是什么情况，都可以防止不一致的决定被执行（至少也是按兵不动，并且很容易定位叛徒是谁） 注意，在这种仅有4个节点的情况下看似复用信道和传递消息的数量不多。但随着结点的增加，时间复杂度和信道使用量级是节点数的平方。大规模网络基本瘫痪，效率太低 区块链解决方案我们知道，区块链最强的地方就在于它的一致性（了解区块链原理，可移步另一篇博客 一文看懂区块链：一步一步发明比特币），同时这正是拜占庭问题的核心 案例拆解我们先假设你已经完全了解了比特币区块链的运行原理，那么我们一步一步建立一个场景看一看区块链是如何解决拜占庭将军问题？ 我们先假设信道一定是可靠的，传令兵死亡之类的事情我们不考虑，毕竟在一个非常复杂的网络中，还可以通过多条的方式连接任意两个节点，可靠性还是值得相信。主要破坏一致性的还是心怀不轨的【间谍】，或者总结为：如何防止【间谍】对整体决策（进攻还是撤退）进行破坏？ 我们按照区块链模型构造一个下图所示的系统 每个将军本地都存储一份【记录】：记录所有将军的决定，比如“1：1”代表1号将军决定进攻 然后构造以下协议内容： 使用数字签名保证身份可可信 所有将军参与挖矿，国王以保证战役胜利为缘由，出资，奖励每一个挖到新区块俩的将军 每一个将军当本地维护的最新确认【记录】中包含了所有1-9号将军的决定后，正式做出自己的决定 在这个案例中，抛弃了代币的设定，因为不存在交易行为，而是由国王出资（保证战争不被间谍影响，我认为国王应该愿意出这笔钱）。在拜占庭时期，因为没有网络，构造上述这样的系统，是完全不可能的。而现在网络链路速度，效率越来越高，让区块链解决一致性问题得以解决 这里就引出了现在区块链的核心问题：应用场景与代价博弈。你要解决的痛点，到底值不值得这样的花费呢？无论是算力消耗，还是资源消耗，亦或是类似于上述案例中的国王出资（区块链代币价值为负数？），都是一种【代价】。完全的信任是不存在的，只有当造假（走捷径获得利润）的成本远远高于得到的利润，才能取得信任（一致性） 必须强调，在传统的拜占庭问题构造的情景中，只能是一个例子，这个应用情景是完全没有没有必要使用区块链来解决的！ 总结互联网技术的存在，让传输过程中，基本没有延迟（或说延迟很小可以基本忽略），解决了通讯延迟的问题 区块链使用链型数据结构 + 算力互相制约使得作假的成本随着时间的加长呈指数上升，解决了一致性问题。当然非对称秘钥部分的密码学，解决了身份确认问题 至少这个系统解决的问题不仅仅是金融领域，去中心化银行系统的问题所在，交易，其实只是其中很小的一部分 区块链共识算法因为技术还在不断发展，可能有其他的算法被建立，但是只要谈到共识这个问题，核心一定是【中心化】和【去中心化】的权衡（Trade-off），而对应的就是效率，可以这么说，一致（信任）是需要成本的，这是本源法则，和线性向量空间的定义处在同一个层级 中本聪很厉害的地方就在于，之前的分布式一致性算法（PBFT）对大体谅节点系统的支持非常差，效率上来说，基本和无法实现是等等同的 下面的思维导图展示了现在基本的区块链共识算法总结 分布式一致性算法即这篇文章前面提到的拜占庭容错。在此基础上，发展出的Paxos是理论上的高效算法，很难实现。而Raft是由Google牵头开发一个Paxos理论实现版本 【去中心化】【大规模节点无法支持】【效率低】 投票机制其中有两个比较有名的【RPCA Ripple共识】【DPOS 股权代理人共识】，在规则和协议上稍有不同，但是核心的Idea还是使用类似人大代表选举的制度来保证新区块的产生不会由同一个人控制，即代表轮流挖矿 NEO使用的是【DBFT】，投票的拜占庭容错算法，算是结合了几个算法的优势和思路，也很有想法！容错能力和PBFT一样，但是效率更高，信道使用冗余更低 【半中心化】【大规模节点支持】【效率高】 PoW 工作量证明最早的共识算法，使用算力来资源消耗来实现共识，详细方法见一步一步发明比特币 为了对抗ASIC矿机等专业化HASH算力硬件，也有一些PoW引入了内存HASH等禁止ASIC算力的方法。但是万变不离其中，最终还是归一化到一个【每Hash Rate/法币花费】博弈和平衡中 【PoC Proof of Capacity】容量证明，挖到区块和你的硬盘空间正相关 【去中心化】【大规模检点支持】【效率低】【资源消耗高】 PoS 权益证明新建区块和你拥有的币的数量呈正相关，类似于利息的激励方式，可以参看权益证明 更多的【PoA Proof of Activity】活动证明，是一种Pow+PoS混合共识方式，基本被证明不靠谱了，提及一下 【去中心化】【大规模节点支持】【效率中】 其他【PoET Proof of Elapsed Time】消逝时间证明：Intel使用HyperLedger建立的锯齿（Sawtooth）项目使用。在一种收信任的执行环境下保证随机的选择用户来生产区块，间隔时间提前约定。很奇怪的Idea，只适合于联盟链 总结欢迎各位读者留言提示更多的不同共识算法，讨论交流，共同进步！ 【完】","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"BlockChain","slug":"BlockChain","permalink":"https://charlesliuyx.github.io/tags/BlockChain/"},{"name":"BGP","slug":"BGP","permalink":"https://charlesliuyx.github.io/tags/BGP/"}]},{"title":"【直观详解】让你永远忘不了的傅里叶变换解析","date":"2018-02-19T07:06:25.000Z","path":"2018/02/18/【直观详解】让你永远忘不了的傅里叶变换解析/","text":"【阅读时间】15-20min 7185 words【阅读内容】使用联想链条和几何直观，辅以从实际需求衍生概念的思考模式，详解什么是傅立叶变换，为什么要做傅立叶变换等，帮助记忆和理解，目的当然是标题所说：让你永远忘不了傅里叶变换这个公式。另，这篇博客还从侧面一定程度上回答了另一个问题：为什么要研究复数 本篇博客为形象展示傅里叶变换和欧拉公式与初等群论两个视频的笔记结合，希望通过此篇让所有读者对傅立叶变换有一个全新的认知，并且宣传一波 3b1b 良心视频系列！重塑对未知和知识的渴求 知乎相关问题链接，小伙伴们求点赞！没有功劳也有苦劳啊！ 欧拉公式与旋转在开始一步一步接近【傅立叶变换】前，先说一下群论 提前说明，此部分有地方会提到【群论】这个概念，但博主并不是要试图把什么环、域、向量空间、代数结构、线性代数群、李群等等一大堆很抽象的概念灌输给大家，我们只是为了利用群论的概念，加深或者说建立一个对【理解傅立叶变换】极度有帮助的直观概念： 幂函数（逆操作对数函数同理）是加法和乘法运算的桥梁，在自变量包含复数时表示旋转 以具体的一个例子来说： $e^{\\pi i}$ 表示的是在单位圆上逆时针在旋转180°这个变换 等等，这不是排行世界上最伟大的十个公式第二名的欧拉公式（上帝公式）嘛？（BTW，我们今天的主角【傅立叶变换】排行第七，这阵容着实强大）$$e^{\\pi i} + 1 = 0$$是的，这第一部分，捎带，会带你更进一步的重新认识这个公式的伟大 对称性 symmetry首先，假设我们有以下陈述： 正方形是对称图形 那么从数学（定义 or 公式）角度上来说，怎么描述【对称】这个概念呢？我们作为【人】，肯定会想，不就是看着左右一样嘛？不够严谨，不够优雅，继续深入，可以这么考虑： 你能对正方形做些什么，并且在这个操作后，保持正方形的形态和操作前相同 我们把具有上述性质的操作都列出来，放在一起，如下面动图所示（左右旋转90°，旋转180°，四个轴对称，不变，这八个操作），就构成了一个有限群【对称群】，更专业的叫法是【八阶二面体群 Dihedral group of order 8】 有了上面的直观理解，还有一个无限群需要了解，即【旋转群】，表示的是所有旋转操作，当然，因为角度可以无限细分，这个操作也是无限的，比如：顺时针旋转 $\\theta$° 此时，能总结出一个巧合的现象，按照顺序进行上面8个操作的某两个，恰好等同于8个操作中的其他的某一个（旋转群同理），如下面的动图所示，把这些组合放到一起，才真正的表达了【群】这个概念 很多不同的概念都能从对称性和对称性的符合构建得到，如下图所示，其中，数字本身有两种表达方式（操作），加法和乘法 对于【数】这个集合来说，加法对应数轴的平移变换（一个操作），乘法对应着数轴的伸缩变换（一个操作） 把这个数轴的概念拓展到平面坐标系，1D ➜ 2D。如果我们要把一个点，比如（1,0）移动到另一个点，应该如何操作？简单的说，只需要先在横轴方向上平移，再在纵轴方向上平移即可（核心思想类比于正方形的几个操作） 同理，除了平移外，使用伸缩加旋转也可以完成同样的事情（将任意一点移动到另一个位置），伸缩是乘法显而易见，但是旋转怎么表示呢？（当然直接改变坐标轴的定义也是能做到的，例子就是极坐标系，但我们并不想这么做），我们构造以下思考链条： 考虑一个特例操作：一个点变到另一个点：（1,0）通过伸缩和旋转到（-1,0），长度不用变，只需旋转即可 此时，注意到了一个形式很有特点的定义： $-1 = i \\times i$ ，-1 就是我们需要的目标位置，那如何从（1,0）出发进行两次同样的操作可以得到（-1,0）呢？（这个操作即 $i$ 这个虚数单位定义的操作）答案即：一个单位 $i$ 表示旋转90°即可 更意外的发现，进行一次 $i$ 操作，如果是逆时针旋转90°，正好会落在二维平面y轴的（0,1）与单位长度不谋而合 更大胆的假设，如果y轴自带虚数单位，如 $i,2i,3i……$ ，就有旋转操作了，是不是就就能通过乘法来描述处在这个平面上的所有变换了？ 以上都是假设和推理，剥丝抽茧后，最关键的部分：如何使用单位 $i$ 表示逆时针旋转90° ，并且给出了一种可能的映射规则，x轴平移表示伸缩，y轴平移表示旋转，这样就可以保证群的特性？（几种操作一定可以组合成其他某个操作，有一个学名：保持群结构） $i$ 怎么可能表示旋转呢？怎么看都像啊，此时陷入了死胡同，不妨换个角度来思考，旋转到底是什么？ 旋转，是沿着一个圆弧（有圆心，转过的角度）运动的过程 如果你对泰勒公式非常熟悉（不熟悉没关系，点开连接看看呗？），就可以通过一系列公式推导得到一个完美桥梁：【幂函数】，形如 $f(x) = a^x$ 如果底数 $a = e$ ，通过泰勒展开式，可以完成一个十分优美的变形，如下： $$ e^x = 1 + x + \\frac{1}{2!} x^2 + \\frac{1}{3!} x^3 + \\cdots \\tag{1} \\\\ sin(x) = x - \\frac{1}{3!}x^3 + \\frac{1}{5!}x^5 + \\cdots \\\\ cos(x) = 1 - \\frac{1}{2!}x^2 + \\frac{1}{4!}x^4 + \\cdots $$ 将 $x = i\\theta $ 带入（1）式（这里的 $\\theta$ 是一个未知数，即自变量），整理项，移动，结合 $cos(x)$ 和 $sin(x)$ 的泰勒展开式，还有虚数单位的定义 $i = -1 \\times -1$ ， 有下列推导： $$ \\begin{align}e^{i\\theta} & = 1 + {i\\theta} + \\frac{1}{2!}({i\\theta})^2 + \\frac{1}{3!} ({i\\theta})^3 + \\frac{1}{4!} ({i\\theta})^4 + \\frac{1}{5!} ({i\\theta})^5 + \\cdots \\\\ &= (1 - \\frac{\\theta^2}{2!} + \\frac{\\theta^4}{4!} + \\cdots) + i(\\theta -\\frac{\\theta^3}{3!} + \\frac{\\theta^5}{5!} + \\cdots) \\\\&= cos(\\theta) + isin(\\theta)\\end{align}\\tag{4} $$ 这个公式有什么用呢？可视化后，如下图所示 假设纵坐标自带虚数单位 $i$ （复平面），那么，$sin(\\theta)$ 为纵坐标（自带虚数单位 $i$ ），$cos(\\theta)$ 为横坐标，则可以发现： $e^{i\\theta}$ 表示一个圆心在原点，半径为1的单位圆（图中是 $\\alpha$ ，因为作图软件的限制，换不成 $\\theta$ ，但不影响） $e^{i\\theta}$ 这个公式等价于一种旋转，$\\theta$ 为旋转角的度数（统一单位，弧度制，即把°转换成实数）$\\theta = 2\\pi$ 即为360°，是单位圆 我们已经优雅的找到了这个桥梁，接下来仔细研究一下它意味着什么 幂函数 Exponentiation幂函数有一个非常重要的特性：加法变乘法，即 $a^{x+y} = a^x \\times a^y$ 也就是说通过幂函数，可以做到使用平移变换来描述伸缩变换，这具体是什么意思呢？参考下面的动图 上方的数轴，表示的是平移变换 -1（左移一个单位） 和 2（右移两个单位）（加法），下方的数轴将两个数作为输入，代入到一个幂函数 $f(x) = 2^x$ 中，对于函数来说，这个输出值，就是两次伸缩变换 （乘法），一次是收缩为原来的 $2^{-1}=\\frac{1}{2}$ ，另一次是拉伸成原来的 $2^2 = 4$ 倍 注意，所谓可以变的意思是说，加法运算可以成立，意味着先往左平移1单位，再往右平移一单位，组合起来的左右就是往右平移一到位（$-1+2 =1$，群论的保持结构特性），而乘法运算成立也要满足这个特性 复平面 Complex Plane至此，构造复平面，把虚数单位 $i$ 加到纵轴上。我们就同时拥有了伸缩和旋转，最关键的是，有了这两个操作，我们同时也可以维持的群的特性（使用乘法） 如下面动图所示，在复平面内，以幂函数为桥梁，实轴横向平移对应伸缩，虚轴纵向平移对应旋转 横坐标红线，横向平移映射到伸缩操作的可视化 纵坐标虚数单位，纵向平移映射到旋转操作的可视化，正为逆时针旋转 现在使用的桥梁是底数为2的幂函数 $f(x) = 2^x$ ，我们知道 $e^{i\\pi}$ 代表的半个圆周，我们希望把底数变成 $e$ ，这样更加方便表达圆的概念 每走一个单位的纵向位移，在圆周上旋转的圆弧长度就是1，参照下面的动图，$e^{\\pi i}$ 恰好代表逆时针旋转180°，并且落在的位置为（-1,0），这就是欧拉公式，或者说是欧拉公式的几何直观可视化 总结这第一部分到底干了啥？其实就是想建立一个观念（或者说常识） $e^{x}$ 在复平面，或者说 $x = ai$ （a为某个常数，就是弧度制的圆周长度）代表的变换是：旋转 如果你之前学过傅立叶变换，那么会明白为什么需要花费这么大篇幅来讲这个，因为公式中， $e^{i\\pi}$ 那是可是相当重要的一部分啊 傅立叶变换正式进入傅立叶变换的部分，老规矩，先做一下基本信息整理 什么是傅立叶变换首先，还是先弄清楚我们理解的目标是什么 傅立叶变换（如果不加限定，这个词对应的是连续傅立叶变换）傅立叶级数 傅立叶变换还有很多其他的内容：离散时间傅立叶变换，离散傅立叶变换，傅立叶逆变换，快速傅立叶变换等，进一步的拉普拉斯变换，小波变换，z变换等 公式表示傅立叶变换，变换作用是时域映射到频域，公式是长这样的： $$ \\hat f (\\xi) = \\int_{-\\infty}^{\\infty} f(x) e^{-2\\pi i x \\xi} dx \\quad \\xi \\;为任意实数 $$ 很多时候，这里的 $\\hat f(\\xi)$ 会写成 $F(w)$ 或 $F(f)$ 表示角速度或者频率，当然后面的公式的量纲也需要对应的修改；后面的自变量 $x$ 大多数时候都是写成 $t$ 表示时间。当然，他们表示的都是同一个东西 联想链条既然是为了【理解】和【记忆】，那么我们还是需要定义一个联想链条： 傅立叶变换 ➜ 分解声音的过程 这么抽象实在是因为拆字法真的很难联想出什么东西来（傅里叶？变换？嗯，很难的样子），只能这样了。 接下来就是精华部分：3b1b的傅立叶变换讲解内的核心内容！在笔记完成后，会给出结合直观理解的完整联想链条，目的当然是【让你永远忘不了】喽，点题！ 【看到】傅立叶变换声音的表示我们是如何记录声音的呢？如果你测量的是扬声器旁的气压，那么它会是一个随时间以正弦函数形态不断震荡的图像，一个标准音 A（下图黄色），它的频率是440Hz，表示每秒钟振动440次，比它低一些的D（下图紫红），是294Hz，振动的慢一些。如果这两个音同时发出，产生的气压随时间曲线怎么决定呢？如下动图，其实就是把所有时间点的振幅加起来 那么如果给你随意一段随时间变化的气压曲线，你如何找到这些原有的组成音符呢？这就是我们的目的，参考下面的动图，感觉有点像是把一盘混好的原料分成组成它的单独的颜色，感觉不那么容易吧？ 下面就需要一步一步把这件事情做出来 可视化方法首先，假设我们有一个每秒钟3拍子的声音信号（440Hz实在太快了），它的图像如下（Intensity为强度，可以同理成气压），并且，我们只关注前面的4.5秒（即图像中画出来的部分） 绕圈记录法：同一事物的不同角度千万不要眨眼！下面是最关键的一步，是【看到】傅立叶变换的核心部分，如下面动图所示 首先把黄色曲线缠绕到一个圆上，大小就是原本信号的振幅 圆周围的图像由白色的箭头绘制而成，速度可变，上图中的白色箭头移动速度是每秒钟转过半圈（这个速度是对于下面的圆形图像来说，每秒钟在圆形图像中转半圈），对应上面的则是虚线表示一圈走到的位置，0.5拍子/秒 此时，有两个频率在起作用，一个是信号的频率：3次震荡/秒，另一个是图像缠绕中心圆的频率，为0.5圈/秒，第二个频率可以自由改变，相当于一个变量，下面的动图直观的展现了缠绕速度变化时的可视化表现 从最开始的 0.79圈/秒（注意这里的速度是指绕单位圆的白色箭头的滑动速度）一直变化到1.55圈/秒，再到最后的恰好是3圈/秒，和原来的信号3拍/秒相同，此时会出现一个非常稳定的图像，我们可以理解成，同步，这个绕圈图像记录了原信号的幅值变化并且每一圈都相同（周期性） 其实，我们只是把一个水平的轴缠绕到一个单位圆上，并用另一个速度的记录标尺（白色箭头）来画图，从另一个角度（维度）来看我们的信号 质心记录法：新维度的特征提取虽然新图像挺好看的，但是现在感觉并没法从中看出什么。也不尽然，我们直观的发现，当白色箭头记录的速度在某些特定的值时，画出来的图形非常稳定，形态清晰。那如何表现这个特征呢？ 从两个角度来思考 （1）自变量是什么？（输入特征） 输入是一个可变化的转圈速度，既然可变，不妨把它看作自变量，即 $f(x)$ 中的 $x$ （2）输出（新的圆圈图）有什么特征？（输出特征） 观察到，当图像很混沌（没有规律，混乱的）时候，图像基本关于原点对称；稳定时，其实是“头重脚轻”的。描述“头重脚轻”最好的方法当然是用【质心】（它描述了物体的空间分布特征） ，下面的动图直观展现了质心特征对图像特征的描述能力（红色点为质心） 考虑到质心其实是一个二维坐标，这里为了简洁和直观，取质心的横坐标来表示质心的特征 【输入（横坐标）】➜【进行采样的（白色箭头）的绕圈速度】 【输出（纵坐标）】➜【圆圈图的质心位置的横坐标】 按照上面的说明来记录绘出图像，记录每个缠绕频率（速度）对应的质心位置，参看下列动图，随着图像的绘制到3圈/秒这个位置的时候，是不是感到似曾相识呢？ 补充一点，在横坐标等于零点处有一个很大的值，只是因为原来的图像没有关于横轴对称，有一个偏移量，直观参看下面动图 我们可以看到，新图像的横坐标写的是【频率 Frequency】，即缠绕圆圈的记录速度，所以强烈建议看到频率，想起速度，并且抽象为围着圆圈跑的速度（个人感受，对理解【频率】的概念有助益） 好！有了这个工具，先把它应用到两个声音的组合图像中看看效果：（这是我最喜欢的一张动图） 什么？还是没看清上面的振动图像如何变成圆圈图的？看下面的动图，缠绕圆圈速度为2圈/秒的白色箭头将时间信息映射到圆圈图中的的可视化。再次重复，白色箭头以一定的速度（频率，一秒几圈）在上图中向右横移，同时，在下面的单位圆内被转换成类似钟表指针移动的圆圈运动，并记录振幅，画出图像 BTW，图形的一部分有点像动画EVA中某个使徒的脸，带给人一种诡异的仪式感。数学之令人敬畏，可能在这一刻熠熠生辉，刺的人睁不开眼 公式表示大家也发现了，我们已经通过这样一个缠绕机器完成了时域到频域的转换，总得来说，参看下面的动图 这是一种【近傅立叶变换】，为什么是【近】，后面会提到。先考虑，那如何数学语言表达这个【转圈记录机制（工具 or 机器）】呢？ 第一步：旋转的表示如下面的动图所示，在这个工具中，非常关键的就是转圈，即表达旋转这种运动，根据第一大部分，这个桥梁，就是复平面，其背后的原理是幂函数结合泰勒公式 更进一步，幂函数中，以 $e$ 为底的函数有着特殊的性质，如下面动图所示，$\\pi$ 单位的 $e^{6.28i}$ 就表示一个单位圆的360°旋转，则 $e^{2\\pi it}$ 表示的就是一秒钟一圈的旋转方程，感觉速度有点太快了，所以加一个 $f$ 频率，控制旋转的速度 ，图中为 $\\frac{1}{10}$ ，合起来表示一秒钟十分之一圈 第二步：缠绕的表示首先，依据下面的动图所示，在傅立叶变换中，我们规定旋转是顺时针的（规定只是为了统一标准，并且有时候也会考虑书写简洁和方便计算），所以先加一个负号。假设原来的函数是 $g(t)$ ，将两者的幅值相乘就能得到缠绕图像， $g(t) e^{-2 \\pi ift}$ ，可以说是相当机智了！ 第三步：质心的表示那如何表示质心这一概念呢？粗略想一下感觉挺难的，但是看起来很难的问题，有一种解决问题的途径是【演绎推理】，先从简单的特例出发，推广到一般，最后证明正确性即可 考虑如何求一个正方形的质心位置，我们只需在边框上取n个等距离分布的点，并且算这几个点的位置的平均值。那么推广到一般情况，也使用类似的采样点的方式解决，如下面动图所示（紫红色的点即采样点），得到 $\\frac{1}{N} \\sum\\limits _{k=1}^N g(t_k) e^{-2 \\pi ift_k} $ 随着采样点的增加，需要使用积分来求解这个问题，如下面动图所示，得到 $\\frac{1}{t_2 - t_1} \\int_{t_1}^{t_2} g(t) e^{-2 \\pi i f t}dt$ 最终步：整理积分限和系数看到常数项系数 $\\frac{1}{t_2 - t_1}$ ，如果忽略表达倍数关系的系数，对应的含义也会发生变化，不再是质心，而是信号存在的时间越久，位置是质心位置乘以一个倍数，它的值就越大。参看下面的动图，持续时长为3秒，那么新的位置就是原来质心位置的三倍；为6秒，就是原来的6倍 而去掉系数的几何直观动图变为（红色箭头为去掉系数后的长度表示），最本质的区别是：可以使得最后绘制的图像更集中在对应的频率的附近，或者说在对应的频率位置的值更大 继续考虑上下限。我们知道，一般傅立叶变换公式的上下限是正负无穷，那它的几何直观是什么呢？参看下面动图，其实就是看看信号持续时间无穷大是什么样子的 说实话，这个动图解答了我大学时代的一个疑惑，音乐文件不都是有时间长度的嘛，我就一直不懂，凭什么对负无穷到正无穷做傅立叶变换？原来真实情况是，负无穷到0，音乐结尾到正无穷，就像上面的动图，其实都没有振动幅值（电信号幅值）与之对应，再结合缠绕圆圈的思想：原来，从音乐开始到结束傅立叶变换和从负无穷到正无穷做傅立叶变换，是特么的一回事啊！（吐槽完毕） 补充相位在表示质心的时候，我们只取用了x轴坐标，下面的图中的蓝色曲线就是纵坐标（y轴 or 虚部）的可视化,红色曲线是横坐标（x轴 or 实部） 那么相位是如何表示的呢？如下面动图所示，其中红色的部分为质心，长度为振幅大小，对应的角度就是相位 原信号的长度再追根究底一些，因为之前已经提到过，假设我们的信号有4.5s。 那么考虑原信号的长度的变化呢？首先，假设信号的长度很长，那么缠绕圆上的线就会更多，每次接近稳定图像质心的变化速度更快（即频域图像更加密集），参看下面动图 那么对应的，如果原信号的长度缩短呢？如下面动图所示，频域图像会更加稀疏。原因同理，当缠绕的内容少的时候，重心变化的速度也相应的变慢了 总得来说，基本就上述内容就详细解释了下面的现象： 时域的信号周期越长，那么频域就越集中，越不容易发生混叠，越容易抽象出时间信号的周期性重复信息，此时自然而然的，周期性这个词就出现了。 另外，可以自己思索一下，比如无穷时间的周期时域信号呢？又比如一个恒定振幅（一个电平）的时域信号呢？其实这里就给出了一个提示有关为什么傅立叶变换有那么多需要考虑的变形了，因为在缠绕这件事情发生的过程中，有几种情况是特别的（这部分3B1B视频并没有讲解，可能需要未来再更新了） 总结讲了这么长，至此全部结束。估计读者都已经晕了，那么，在这里为【看到】傅立叶变换做一个总结，就来总得说说我们从头到尾都干了些啥？参看下面动图 （1）$e^{2\\pi i}$ 表示单位圆，添加自变量即可表示旋转 （2）与原函数相乘缠绕到单位圆上 （3）为求质心的特征，进行积分计算 一步一步写出傅立叶变换公式的联想链条 一个逆时针旋转360°画成的圆 ➜ $e^{2\\pi i}$ 表示运动，需要原函数的自变量，时间 $t$ ➜ $e^{2 \\pi i t}$ 表示旋转速度，需要自变量，频率 $f$ ➜ $e^{2 \\pi i f t}$ 规定变换的采样方向为顺时针，加负号 ➜ $e^{-2 \\pi ift}$ 乘以原函数缠绕到单位圆并记录 ➜ $g(t) e^{-2 \\pi ift}$ （此处使用g符号标识原函数是为了和频率符号区分） 为了计算质心特征，积分 ➜ $\\int_{-\\infty}^{+\\infty} g(t)e^{-2\\pi it \\color{red}{f}}dt$ 自变量为频率 $f$，写出函数表达式 ➜ $\\hat g(\\color{red}{f} \\color{black}{) = \\int_{-\\infty}^{+\\infty} g(t)e^{-2\\pi it \\color{red}{f}}dt}$","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Fourier Transform","slug":"Fourier-Transform","permalink":"https://charlesliuyx.github.io/tags/Fourier-Transform/"}]},{"title":"【直观详解】泰勒级数","date":"2018-02-17T06:29:33.000Z","path":"2018/02/16/【直观详解】泰勒级数/","text":"【阅读时间】10min 3326words【阅读内容】通过构造知识联想链条和直观例子回答什么是泰勒级数，为什么需要泰勒级数，泰勒级数干了什么，如何记忆这个公式 本篇博客回答知乎问题怎样更好的理解，并且记忆泰勒展开式？ （求点赞^_^） For me, mathematics is a collection of examples; a theorem is a statement about a collection of examples and the purpose of proving theorems is to classify and explain the examples John B. Conway 对于我而言，数学就是范例的集合——定理是为了描述范例，证明定理是为了分类并解释范例 约翰·B·康威 微积分的本质 - 10 - 泰勒级数 在遇到一个生僻的概念或者公式时，确认它的几种不同的表述形式（马甲）是很重要，也就是定义问题：我们到底要了解的东西是什么 ＆ 怎么称呼： 泰勒公式（也叫 泰勒展开式、泰勒多项式）泰勒级数 它是微积分学下的一个重要概念，与之有关联的有：如泰勒定理，多元泰勒公式，以拉格朗日型余项为代表的各类余项，审敛法，牛顿差值公式（牛顿级数）（列出为了进行树状知识整合和梳理） 什么是泰勒公式基本定义数学定义，公式各个部分代表什么含义先说清楚 $$ \\begin{align} f(x)_{Taylor} &= \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n!} \\times (x - a)^n \\\\ &= f(a) + \\frac{f'(a)}{1!}(x-a) + \\frac{f^{(2)}(a)}{2!}(x-a)^2+ \\cdots + \\frac{f^{(n)}(a)}{n!}(x-a)^n + R_n(x) \\end{align} $$ $f^{(n)}(a)$ 表示 $f(x)$ 在第 $n$ 阶导数的表达式，带入一个值 $a$ 计算后得到的结果（注意，它是个值）$\\frac{1}{n!}$ 是一个系数（一个值），每一项都不同，第一项 $\\frac{1}{1}$，第二项 $\\frac{1}{2!}$ …… 依此类推$(x-a)^n$ 是一个以 $x$ 为自变量的表达式 求和符号去掉展开写即第二行$R_n(x)$ 是泰勒公式的余项，是 $(x-a)^n$ 的高阶无穷小（此处先不解释，听起来很牛逼，但是跟随例子看完就发现并没有什么玄乎的） 个人粗浅总结，初学者产生记不住的感觉大多数情况下是没有沉下心来想想公式的各部分表示的是什么东西，梳理一下会清晰很多 联想链条 所有的 ＜内容＞➜ 符号都表达【由＜内容＞联想到】（一种牢固记忆的技巧） 联想链条是为了给你一把一个长期记忆的钥匙，很久不用之后，估计只能记住【泰勒公式】四个字了，如何利用这仅有的信息回忆起具体的理解和内容，除了理解透彻，直观，利用图像外，弄一个联想链条也是不错的方法 首先拆字 【公式】 ＜什么公式？＞➜ 【多项式】（Polynomials），把多项式的一般形式写出来，这应该是非常容易理解的概念，即指数不仅仅为2的抛物线的组合 $$ P_{n}(x) = \\sum_{i = 0}^{n} c_ix^i = c_0 + c_1x + c_2x^2 + \\cdots + c_nx^n $$ 【泰勒】＜谐音“太乐” ≈ 如果所有小数都能近似成整数那不是太快乐了？＞ ➜ 近似，获得一个直观理解 泰勒公式通过把【任意函数表达式】转换（重写）为【多项式】形式，是一种极其强大的函数近似工具 为什么说它强大呢？ 多项式非常【友好】，三易，易计算，易求导，易积分 几何感觉和计算感觉都很直观，如抛物线和几次方就是底数自己乘自己乘几次 泰勒公式干的事情就是：使用多项式表达式估计（近似） $f(x)$ 在 $x = a$ 附近的值 那么如何近似呢？使用一个例子来加深理解 怎样理解泰勒公式我们要干的事情，就是改变多项式函数 $P(x) = c_0 + c_1x + c_2x^2$ 中 $c_0, c_1, c_2$ 的值 （只有三项是为图个方便）去近似余弦函数 $f(x) = cos(x)$ ，【近似过程】参考下面的动图 我们需要做的事情（目的）即寻找一条绿色的曲线（多项式的系数 $c_0, c_1, c_2$），在 $x = 0$ 附近（0为上面提到的 $a$）尽可能的与 $f(x) = cos(x)$ 的图像相似（重合） 函数式角度那如何才能找到这三个参数呢？最为显而易见的做法就是希望在 $x=0$ 的位置，两个表达式的切线尽量相等，切线即斜率，也就是求导，比较抽象，一步一步来可视化一下 近似过程 【确定 $c_0$】$x=0$ 带入公式，令 $cos(x) = 1$ ，同理对 $P(x)$ 可以得到 $c_0=1$ 【确定 $c_1$】容易观察到，如果对 $P(x)$ 求导就可以把 $c_1$ 前的自变量去掉。并且，$x=0$ 处 $P(x)$ 已经固定为1，为了更进一步的相似，如果我们让 $x=0$ 处的 $f(x)$ 和 $P(x)$ 的切线斜率也相同不就更近似了？（两种思考模式我觉得都可以） 求导之后可以的到 $c_1 = 0$ 【确定 $c_2$】现在我们已经确定两个值，那么绿色曲线就只能如下图一样移动（固定了 $x=0$ 的函数值和 $x=0$ 处的斜率 ），为了更接近相似的目标，我们希望斜率在变化的过程中，速度也是近似的（滑动的白色和黄色直线）。求二次导数，斜率的变化率相等，确定 $c_2 = -\\frac{1}{2}$ 此时得到表达式 $P(x) = 1 - \\frac{1}{2}x^2$ ，检测一下近似度如何？$cos(0.1) \\approx 1 - \\frac{1}{2}(0.1)^2 = 0.995$ 同时计算器 $cos(0.1) = 0.9950042$ ，其实只取前几项的多项式已经在 $x=a$ 附近的近似这一要求上有很好的效果了 为什么这个【近似过程】写的这么详细，是为了在过程中体会两个关键点 为什么使用多项式来近似因为多项式的求导法则可以控制变量，消去低次项，使得 $x=a$ 未知的 $c_n$ 容易确定，在之前的例子里，如下图所示 $c_0$ 确保了 $x=0$ 时相等，$c_1$ 确保了 $x=0$ 时的斜率相等，$c_2$ 确保了 $x=0$ 时斜率的变化率相等，或者说，随着多项式幂次变高，这种近似就越精确 为什么有个系数 $\\frac{1}{n!}$阶层系数是由一次一次的求导产生的。我们再把项数加两个，参看下图，直观的感受一个 $n!$ 的诞生 首先，低次项会变为0，这样可以很方便的通过计算 $f(x)$ 的 $n$ 次求导的表达式，带入 $x=a$ 即可得到 $c_n$ 的值，阶层其实是多次求导的系数 函数角度总结其实，某一点处的导数值信息 $\\iff$ 那一点附近的函数值信息 这个直观感觉，是很重要的 首先，对于 $cos(x)$ 这个具体例子，把 $x=0$ 位置的多阶导数求出，再使用多项式进行近似，使用的项越多，得到的近似就越准确，参看下面动图 推广到一般函数 $f(x)$ ,下列动图描述了随着项的增加，$x=0$ 附近的越来越准确 最后，推广到 $x=a$ 的情形，完全推导出泰勒展开式的一般形式，如下列动图所示 几何角度首先定义一个函数表示求下列图像中函数图像中填满部分的面积，函数为 $f(x)$ ，面积函数为 $f_{area}(x)$ ，而围成面积区域的曲线即为面积函数的导数 $\\frac{df_{area}}{dx}(x)$ （至于为什么是这样，有一个牛逼的名字叫做，微积分基本定理： $\\int_{a}^{b}f(t)dt = F(b)-F(a)$ ，没那么玄乎，在3B1B的另一个视频内讲解的相当直观。博主也可以写那一期的博客，如果要求的读者多的话），如下图所示 定义一个这样的场景是为了计算这样一件事（如下图所示）：假设我们知道了 $f(a)$ 点的面积，往右扩展很小的距离 $dx$ 要算出新部分的面积（左边绿色已知 + 黄色矩形 + 红色三角形），公式会是什么样的呢？ 设 $dx$ 开始点为 $a$，终点为 $x$ ，则可以得到 【黄色矩形】底边为 $x-a$ ；高为 $\\frac{df_{area}}{dx}(a)$ ； 【红色三角形】底边为 $x-a$；高的计算稍微麻烦，首先，斜边的斜率是 $\\frac{df_{area}}{dx}(x)$ 函数的导数在 $x=a$ 时的函数值（算斜率，求导数即可），而斜率 $k = \\frac{y}{x}$ ，所以得到高为 $\\frac{d^2f_{area}}{dx^2}(a) \\times (x-a)$ （前部分是斜率，后半部分是 $x$ ，需要求的是 $y$ 也是高） 【计算总面积】如下图和公式所示 $$ f_{area}(x) \\approx f(a) + \\frac{df_{area}}{dx}(a)(x-a) + \\frac{1}{2} \\frac{d^2f_{area}}{dx^2}(a)(x-a)^2 $$ 这个公式为啥这么眼熟呢？其实明显就是泰勒展开式的前3项，如果你还要打破沙锅问到底，第4项呢？你可以放大红色三角形，把函数曲线和面积之间的空白部分再次用多个更小的三角形填补，在积分工具的帮助下，可以得到三次项 从几何角度来看，再一次验证了，泰勒公式是近似的 $x=a$ 附近的函数值这一直观理解 余项我们知道，对泰勒公式来说，并没有办法完全逼近待求函数，所以无论如何到最后都会留一点东西，这剩下的东西不好表达，就全都丢到余项中 可以暂时如此理解，不在此迷惑，如果是专业学生，需要深究，建议参看专业教材深入理解其中玄妙 泰勒级数完成对【泰勒公式】的理解后，需要对【级数 Series】这个概念进行一个推广，什么是【级数】呢？ 在数学中，【级数】就是无限多项的和 在把泰勒展开式，扩展到无限项之后，就会出现【收敛 Converge】和【发散 diverge】的概念 收敛收敛，即在泰勒展开式被推广到无限项之后，整体式子的值会越来越趋近于一个定值，比如下图的 $\\frac{1}{2}$ 和 $e$ 发散与收敛相对应的，即发散，式子无法趋近于一个定值，比如 $ln(x)$ 在 $x=1$ 附近，如下图所示，虚线即为能够让多项式的和收敛的最大取之范围，称为【泰勒级数的收敛半径】 总结 泰勒公式干了一件什么事？ 使用多项式表达式估计（近似） $f(x)$ 在 $x = a$ 附近的值 泰勒公式的导数项如何推倒出来的？ 某一点处的导数值信息 $\\iff$ 那一点附近的函数值信息 泰勒公式如何记永远不会忘？ 参照第一条总结，是 $x=a$ 附近，公式 ➜ 多项式，很多项，要用求和写在一起；参照第二条总结，近似信息用的求导；系数就是对 $x=a$ 处求导一次一次放下来；OK，开始写！ $$ f(x) = \\sum_{n=0}^\\infty \\frac{f^{(n)}(a)}{n!} (x-a)^n $$ 并不知道写的对不对，翻到上面Check。OK，很完美，收工！","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Taylor series","slug":"Taylor-series","permalink":"https://charlesliuyx.github.io/tags/Taylor-series/"}]},{"title":"微信跳一跳解题报告","date":"2018-01-19T20:00:21.000Z","path":"2018/01/19/微信跳一跳解题报告/","text":"【阅读时间】5min - 7min 2810 words【内容简介】本文总结了解决问题{如何在跳一跳上快速取得高分的问题}，分为人类解决方案，机器解决方案两部分 问题描述对于微信跳一跳来说，问题描述很直观 问题描述 【玩游戏的心态】游戏本身有没有什么小技巧？ 特殊的加分技巧有哪些？ 如何才能不紧张？ 【解题目的心态】每一次【跳跃】如何不掉下去？ 每一跳，看到目标墩后，需要按多久？ 目标墩和小人的距离是多少？ 游戏机制考虑到这是一个【按得时间越久，跳的距离越远的游戏】，也就是说：不摔死才是王道！问题描述已经如此明了，依次解决就好 加分机制【1】特殊墩。停留2s，可加对应分数 墩类型 加分 井盖 +5 魔方 +10 便利店 +15 音乐盒（注意，有一层透明玻璃的才是） +30 【2】连击跳。连续中心点，+2 +4 …… （是个人都知道） 【3】快速跳。落地之后马上起跳，可以在下一次落地即使也从+1变为+2 按压时间与跳跃距离直说结论，微信跳一跳的按压时间和跳跃距离是大约线性关系（根据其他大神的验证和精确测试，并不是完美线性关系） 打开声音，在蓄力的过程会有嘟嘟声，两次声音之间0.105秒左右，而每次跳跃的距离可以用尺子量出来。拟合（找一条直线看看是否能正好通过所有的点）后是个直线 写成公式的话为$$按压时间 = k \\times 目标墩中心和小人的距离$$其实只要针对你的手机把这个k拟合出来就好（iOS，非Plus系列，k = 2.045） 设计原则对于游戏设计来说，一款优秀的【休闲游戏】一定要满足三点 核心规则简单 容易失误 回报和难度成正比 + 随机奖励 所以，跳一跳能火爆，除了好友圈（这里包含微信强大的整合推广能力）之中的攀比心理，更是在这三点都做的相当出色 解决方案不是每个人都闲着没事做去搞得那么复杂来玩这个游戏，只有两种人会这么做，一种好胜心装逼心爆炸的，另一种就是吃饱了撑着（估计我是两者兼而有之），所以根据现根据前一种人来说，给出的解决方案就是【技巧上】 技巧游戏技巧当然是利用【比较容易达成】的手段来减少失误的方法，这款游戏考验的核心节奏感，控制力 【节奏感】 大家都知道默数一分钟这个游戏吧 能完美默数60s，并且每一秒差距都不大的同学，在这部分就有很高的天赋了。当然还有学乐器，长时间锻炼的同学也是 【控制力】 也叫手残部分。你的APM（每分钟操作次数）速度，控制自己手的能力都考虑其中。 有个游戏是掉尺子抓住，看谁抓的位置更靠上，控制力包含对手的掌控力+反应速度，在这个游戏能取得很棒成绩的同学，你的得分肯定不会低 当然，所有这两点，都是可以通过不断的锻炼（花时间疯狂跳）来提高（这和花时间练琴才能弹的越来越好是一个道理）。只是有些人提高的快，有些同学可能提高的慢，而且很不幸，这种提升是一个Logistic曲线（有天花板） 天赋这东西就是这样，你在这里不擅长，太正常不过。每个人天赋都在不同的领域，所以放宽心才是王道，别太较真（跳一跳跳出一场轰轰烈烈的人生，我的天） 以下就是一些小技巧 用纸贴住分数紧张和手不稳是【失误】的最大元凶，所以贴住分数有奇效 建立自己的模型个人建议自己把距离分为4类 很近（人和墩基本贴起来，一看就觉得好近） 很远（看起来就远的那种） 介于两者之间但是靠很近 介于两者之间但是靠很远 在新墩出现的时候，先用0.5秒分一个类 【4类】原则，宁愿多按一点不少按 【1类】原则，蜻蜓点水千万不多 【2 3类】是最容易出问题的类别，如果失误，根据【第二设计原则】，太正常，如果太容易高分，成就感也会被压缩到很小，所以这是必须的，大侠从头来过就好 小墩对于遇到看起来非常小的墩（小药瓶，天崩地裂） 首先，不要虚，个人虽然没有验证，但是小的目标，感觉能跳上去的阈值也会大一些（或说磁铁or吸附效应），只要按照4分类模型来跳，上垒的可能性还是挺高的 多跳用时间积累，就和练钢琴是一个套路，身体的肌肉根据你看的图像产生反应，如果已经成为一种自然反应，分数肯定不会低 正常方案解决这个问题，就是计算目标墩中心和小人的距离，那么目标墩的位置（目标位置），和小人的位置（起跳点位置）就是待求量 但是这些正常方案都需要在每一跳花费一定时间，非大毅力者不可为 听音法 听到嘟的次数 跳跃的距离范围 cm 1 0 - 1.02 2 1.02 - 1.54 3 1.54 - 2.05 4 2.05 - 2.58 5 2.58 - 3.09 6 3.09 - 3.61 7 3.61 - 4.12 8 4.12 - 4.64 9 大于 4.64 量距离 ➜ 查表 ➜ 专心听 ➜ 数嘟嘟声 ➜ 到次数放 （不做评价，累）优势是如果每次都很准，+32 +64 可能都不是梦想吧？ 滑动屏幕法 然后，匀速的速度是 5.1 cm/s （这个方法，感觉比靠感觉更容易失误）这个方法并没有每次都可以跳到中心的特点，个人感觉这个网站找到的方法有点做负功 程序方案市面上很多的【外挂】或者说【脚本】基本都是这个方案，还有一些直接截包的可能更加暴力一些 手机和电脑交互方法电脑手机连接起来，并且手机可以和电脑交换数据（API），比如电脑可以控制按压时间，电脑可以控制手机截图之类的基本交互操作 Android有adb工具，iOS有WebDriverAgent 算法流程图截图 ➜ 算出小人位置 ➜ 算出目标墩位置 ➜ 计算按压时间 ➜ 利用API操控手机按压 ➜ 跳 其实流程图非常简单，直接单线程就搞定了（没有循环结构和分支结构，都不用用其他软件画了） 主要如何计算小人位置，和计算目标墩位置两部分是关键 多尺度搜索利用OpenCV，图像处理，可以把所有小人抠图出来和图像进行匹配，最后选取置信度最高的，找出小人位置。代码如下 1234567891011121314151617181920212223def multi_scale_search(pivot, screen, range=0.3, num=10): H, W = screen.shape[:2] h, w = pivot.shape[:2] found = None for scale in np.linspace(1-range, 1+range, num)[::-1]: resized = cv2.resize(screen, (int(W * scale), int(H * scale))) r = W / float(resized.shape[1]) if resized.shape[0] &lt; h or resized.shape[1] &lt; w: break res = cv2.matchTemplate(resized, pivot, cv2.TM_CCOEFF_NORMED) loc = np.where(res &gt;= res.max()) pos_h, pos_w = list(zip(*loc))[0] if found is None or res.max() &gt; found[-1]: found = (pos_h, pos_w, r, res.max()) if found is None: return (0,0,0,0,0) pos_h, pos_w, r, score = found start_h, start_w = int(pos_h * r), int(pos_w * r) end_h, end_w = int((pos_h + h) * r), int((pos_w + w) * r) return [start_h, start_w, end_h, end_w, score] 目标墩也可以使用类似的方法。只是要想办法减小搜索空间，加速。因为目标墩的数量还是比较多的。还可以搜索那个中心的小白点，速度越快越准（主需要等那个白点出现就好） 机器学习方案为什么题都已经解了，还需要深究呢？这里主要目的是【提高用户体验感】，考虑到繁多的手机种类和尺寸，特别是iOS中截图的图像压缩问题，需要一套更加稳定的算法，机器学习算法就应运而生了 输入经过裁剪的截图，只保留小人和目标墩（包含关键信息即可，做法使用OpenCV颜色为界，和小人位置为界即可），最终图像大小为 640*720 像素点 输出两个值，目标墩的位置【x，y】（小人位置使用多尺度匹配搜索效率足够高了） 模型结构考虑是图像处理。选择卷积网 - CNN，5层，基本规模，训练完成模型体积约700M 训练数据根据跳一跳截图大约3000章，保证包含所有目标墩的形态，并且使用多尺度算法进行标注（所谓标注就是算出目标墩【x，y】值，并记录，和这幅图匹配起来） 模型增强第一层网络经过数据训练后误差在10像素以内。为了进一步提升精度，之后，以此点中心，根据原训练图，再截图320*320大小的图片，同时加入像素偏移（噪声，防止过拟合），再次利用另一个CNN来训练计算坐标 Pipeline示意图 最终结果小于1像素，Inference时间0.5秒左右 Trick由于微信的反作弊系统，需要添加一些人为随机因素绕过反作弊系统，基本思路 Target_pos + 正态分布随机数，保证连跳次数最多5-6次 间隔时间为&gt;2s的随机数 按压位置随机 至此，微信跳一跳算是通关了。倒腾不息，生命不止。游戏中学习，兴趣中学习是我辈福气！ 游戏是放松的一种方式，这种休闲游戏的好处就是，上瘾不易，每个人都有自己的天花板，每周刷新后跳跳看看可能会变成常态，总之孔子说的好，过犹不及 【参考文献】 思路借鉴于知乎相关问题下的回答 代码借鉴Prinsplield大神的AI跳一跳项目，只添加绕过反作弊系统的代码 感谢大神们提供的学习资料！","tags":[{"name":"CNN","slug":"CNN","permalink":"https://charlesliuyx.github.io/tags/CNN/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Web","slug":"Web","permalink":"https://charlesliuyx.github.io/tags/Web/"}]},{"title":"Dota2-A帐效果","date":"2017-11-06T01:53:21.000Z","path":"2017/11/05/Dota2-A帐效果/","text":"【阅读时间】百科类型文章【内容简介】这是一份对Dota2 所有A帐效果的总结表，以供方便查阅 版本信息：更新到7.07b 升级效果 英雄 技能 升级效果 亚巴顿 回光返照 （1）持续时间+1s【4/5/6 ➜ 5/6/7】（2）900范围将队友承受伤害的50%转移到亚巴顿身上 炼金术士 无 可以将A帐使用，附加一个A帐状态，获得属性和技能升级效果 远古冰魄 冰晶爆轰 持续时间【8/9/10 ➜ 17】持续魔法伤害【256/288/320 ➜ 544 】450（及时伤害）16%即死】 敌法师 法术护盾 反弹一次指向性施法 CD【12s】 斧王 战斗饥渴淘汰之刃 战斗饥渴减少目标输出30%在一次成功的斩杀后给【700范围】的敌人英雄施加战斗饥渴（减速加速12% 与伤害 ） 祸乱之源 蚀脑 蚀脑CD 1.5s 并可以穿透魔法免疫 蝙蝠骑士 燃烧枷锁 再抓住一个在目标附近400码范围内的目标，造成每秒100点的魔法伤害 兽王 野性咆哮 增加施法距离【600 ➜ 950】减少冷却CD【85/75/70 ➜ 45】 嗜血狂魔 割裂 2充能点数 40sCD 赏金猎人 投掷飞镖 2次反弹并短暂晕眩时间增长【0.1 ➜ 0.75】 酒仙 元素分离 土熊猫踩，火熊猫醉拳，风熊猫酒雾，CD独立 钢背兽 粘稠鼻涕 技能以自身为中心750范围施法 半人马战行者 野蛮冲撞 减伤40%并可穿越地形 混沌骑士 混沌之军 CD【130s ➜ 110s】 且可以对1200码内的友军释放 陈 神圣劝化上帝之手 （1）允许招远古（2）增加控制远古单位的数量 1/2/3 发条技师 发射狗爪 减少CD【70/55/40s ➜ 12s】 水晶室女 极寒领域 处在大中2.5的单位受到冰封禁制的作用 黑暗贤者 复制之墙 增加幻想伤害【60%/75%/90% ➜ 100%/120%/140%】 戴泽 薄葬 450范围施法 干扰者 静态风暴 增加持续时间【5s ➜ 7s】且施加锁闭（封物品） 末日使者 末日 增加持续时间【15s ➜ 16s】并在目标与Doom在900码内，持续时间不缩短 卓尔游侠 精准光环 增加随机375码范围内的两个攻击目标造成50%伤害，带攻击特效 大地之灵 残岩魔咒 把目标变成岩石，持续3s 撼地者 强化图腾 900码跳动释放，落地生效 上古巨神 裂地沟壑 造成缴械，减速/缴械时间增长【3/4/5s ➜ 4/5/6】 魅惑魔女 推进 攻击距离增加【550 ➜ 740】 谜团 黑洞 大招时同时释放一个同等级午夜凋零 虚空假面 时间结界 减少冷却CD【120/110/100 ➜ 60】 矮人直升机 高射火炮 增加技能，对600范围内目标每1.1s自动攻击一次 哈斯卡 牺牲 增加伤害 【34%/38%/42/% ➜ 65%】减少冷却CD【12s ➜ 4s】 祈求者 元素召唤 3元素+1等级 减少大招冷却【6s ➜ 2s】 魔法消耗减少【60 ➜ 0】 杰奇洛 烈焰焚身 增加距离【1400 ➜ 1800】增加伤害【100/140/180 ➜ 125/175/225】增加持续时间【10s ➜ 30】 主宰 无敌斩 增加斩击次数【3/6/9 ➜ 6/9/12】减少冷却CD【130/120/110s ➜ 70s】 光之守卫 灵魂形态冲击波 （1）永久大招，白天飞行视野（2）冲击波造成等量治疗 昆卡 幽灵船 拖拽行进过程中200码范围的敌人此时开船位置在昆卡身前，沉默位置相同 军团指挥官 决斗 持续时间增长【4/4.75/5.5 ➜ 6/7/8】决斗双方不能受到第三方攻击伤害 拉席克 脉冲新星 每1.75s释放一个闪电风暴（700码，英雄优先，不弹射，200伤害） 巫妖 连环霜冻 增加伤害【280/370/460 ➜ 370/460/550】 增加施法距离【750 ➜ 850】弹射次数无限 减速/减攻速增加【30% 30 ➜ 50% 50】 噬魂鬼 感染 获得吸收技能，被吸收的友方英雄可以获得和噬魂鬼一样的感染加血并且可以选择释放造成300的魔法700码AOE伤害 莉娜 神灭斩 魔法伤害变为纯粹伤害 无视魔法免疫 莱恩 死亡一指 增加伤害【600/725/850 ➜ 725/875/1025】减低魔法消耗【200/420/650 ➜ 200/420/625】降低冷却时间【160/100/400 ➜ 100/60/20】作用主目标325码AOE范围的其他目标 德鲁伊 熊灵 熊灵没有距离限制可以随时攻击，本体死亡熊不死亡 露娜 月食 增加月光数量【5/8/11 ➜ 6/12/18】目标承受最大月光数量【5 ➜ 无上限】减少月光间隔【0.6s ➜ 0.3s】减少持续时间【2.4/4.2/6 ➜ 1.8/3.6/5.4】允许释放月食跟随队友或施法距离2500码释放释放675码搜索范围并获得地面视野 马格纳斯 冲击波 冲击波会收回，击中的敌人会有2s 60%减速最远距离和速度增加50%回归的冲击波对小兵造成减半伤害 美杜莎 秘术异蛇 击中目标造成1s石化，每次弹射时间增加0.2s 米波 分则能成 增加一个实体 米拉娜 星落 每9s自动师释放一次星落 变体精灵 混源 获得一个友军的幻象，继承变体精灵的属性并且可以释放非大招A帐技能 娜迦海妖 海妖之歌 每秒恢复10%最大生命，持续时间【7s】 先知 自然之怒 增加伤害【110/140/170 ➜ 135/170/205】杀死一个小兵召唤一个树人，一个英雄召唤一个大树人（102攻击力，1650生命值） 瘟疫法师 死神镰刀 减少冷却CD【100/85/70 ➜ 55/40/25】 暗夜魔王 黑夜降临 夜晚获得飞行视野 司夜刺客 钻地 钻地状态40%伤害减免 1.5%最大生命值/最大魔法值恢复，并加强技能发力燃烧和穿刺施法距离增加75%减少穿刺CD【13s ➜ 7s】尖刺外壳及时晕眩300码范围内的敌人 食人魔魔法师 未精通的火焰爆轰 使用60%当前魔法值释放一个火焰爆轰，6s CD 全能骑士 守护天使 增加持续时间【6/7/8s ➜ 8/9/10】全屏作用，影响建筑 神谕者 涤罪之焰 减少CD【2.25s ➜ 1s】施法动作时间减少【0.3s ➜ 0.1s】 殁境神蚀者 星体禁锢 2点充能点数 CD【12s】伤害区域叠加增加施法距离300码 幻影长矛手 灵魂之矛 在敌人目标间弹射，弹射范围400码，5次弹射 凤凰 超新星 被破坏次数增加【5/8/11 ➜ 7/10/13】可以带一个人复活，500码施法距离，死亡一起死亡 帕克 梦境缠绕 增加伤害【100/150/200 ➜ 200/250/300】增加断开晕眩时间【1.5/2.25/3 ➜ 1.5/3/4.5】持续时间增加【6 ➜ 8】无视魔免 帕吉 肉勾 减少CD【4s】增加伤害【90/180/270/360 ➜ 180/270/360/450】 帕格纳 生命汲取 增加每秒汲取量【150/200/250 ➜ 180/240/300】无CD 痛苦女王 超震声波 增加伤害【290/380/470 ➜ 325/440/555】减少CD【135s ➜ 40s】 剃刀 风暴之眼 减少打击间隔【0.7/0.6/0.5 ➜ 0.6/0.5/0.4】可以攻击建筑 力丸 绝杀秘技 大招可以跟随队友移动1000码范围，增加大招持续时间【4/5/6s ➜ 7/8/9s】 拉比克 技能偷窃 偷来的技能有A帐效果减少CD【20/18/16 ➜ 2s】增加施法距离【1000 ➜ 1400】 沙王 掘地穿刺 增加施法距离【350/450/550/650 ➜ 700/900/1100/1300】在刺中的目标上附加腐蚀毒 暗影恶魔 邪恶净化 3点能量点数，充能时间【40s】，持续时间5s给目标附加破被动效果 影魔 收集灵魂魂之挽歌 增加灵魂的数量【15/22/29/36 ➜ 22/30/38/46】大招会收回造成二次伤害，并且给予40%的伤害值的治疗 暗影萨满 群蛇守卫 守卫同时全伤害攻击2个目标增加攻击距离【600 ➜ 825】 沉默术士 智慧之刃 无视魔免，并对沉默的单位造成100%的额外伤害 天怒法师 双风 每次释放技能都和自动搜索以目标为中心700码内的单位再释放一次（优先英雄单位） 斯拉克 暗影之舞 减少CD【60s ➜ 30s】并且可以对325码范围内的友军生效 狙击手 暗杀 变成以目标为中心400范围的范围技能造成2.8倍物理伤害，并附加爆头（被动）效果 裂魂人 幽冥一击 增加施法距离【700 ➜ 850】减少CD【60s ➜ 20s】对以目标为中心250码范围内的目标触发巨力冲击（被动） 风暴之灵 电子涡流 以蓝猫为中心475码范围施法，需要目标视野才能有效 斯温 神之力量 900码光环效果，获得75%/100%/125%的额外伤害加成（给予Sven增加的攻击力） 工程师 遥控地雷雷区标示 增加伤害【300/450/600 ➜ 450/600/750】增加事施法距离【500 ➜ 700】在标识附近125码放置3中地雷，并且免疫真实视野 潮汐猎人 巨浪 变为长条形对地施法，1800码长度，240码宽度 伐木机 锯齿飞轮 再获得一个独立额外的大招 修补匠 激光导热飞弹 激光可以在目标650码的敌方英雄身上反射 增加导弹数量【2 ➜ 4】 树精卫士 森林之眼 获得一个给树木附加的技能，800码飞行视野并在范围内有大招效果并可以造成伤害（175/s） 巨牙海民 海象飞踢 将一个单位踢飞1400码，造成40%减速，350吗，魔法伤害，持续4s 孽主 衰退光环 增加被动持续时间【 30/40/50/60 ➜ 70/80/90/100】己方死亡英雄会获得一半加成 不朽尸王 腐朽 增加力量偷取【4 ➜ 10】 熊战士 激怒 在被控制时可以释放，减少CD【50/40/30 ➜ 30/24/18】 复仇之魂 移形换位 允许对非英雄目标施法，减少CD【45s ➜ 10s】死亡后有一个可以操控一个复仇幻象（不能使用物品，其他都都完全相同） 剧毒术士 剧毒新星 增加每秒伤害【30/55/80 ➜ 60/85/110】减少CD【140/120/100 ➜ 140/120/60】 冥界亚龙 蝮蛇突袭 增加施法距离【500 ➜ 900】减少CD【80/50/30 ➜ 10】减少魔法消耗【125/175/250 ➜ 125】 维萨吉 佣兽 增加一头佣兽 术士 地狱火 2个地狱火，每个地狱火的生命和伤害变为原来的75%每个被击杀获得金钱变为原来的50% 编织者 回到过去 减少CD【60/50/40 ➜ 16】可以对友军释放，法法距离【1000码】 风行者 集中火力 攻击减少变少【50%/40%/30% ➜ 30%/15%/0】减少CD【60s ➜ 15s】 寒冬飞龙 严寒灼烧 变成一个开关技能，开启每秒消耗40点魔法不再有对某个目标的攻击次数上限 巫医 死亡守卫 攻击可以弹射4次，具有真实视野 冥魂大帝 重生 在1200码内的友方英雄死亡后还可以获得7s时间复活 宙斯 雷云 全图可以放置一个雷云并对最近的敌人释放雷击间隔时间2.25s，持续时间35s，摧毁需要次数（近战/远程/非英雄 4/8/16）（间隔可以被减CD效果影响） 推荐出的英雄 推荐英雄 其中，有质变效果的： 远古冰魄，发条技师，干扰者，谜团，祈求者，光之守卫，莉娜，瘟疫法师，暗夜魔王，帕克，帕格纳，暗影恶魔，暗影萨满，工程师，伐木机，树精卫士，维萨吉，术士，巫医，嗜血狂魔，米拉娜，司夜刺客，不朽尸王 看情况出的英雄 看情况出 无A帐效果 无A帐效果 新版本中加上小小，石鳞剑士，邪影芳灵无A帐效果","tags":[{"name":"Dota2","slug":"Dota2","permalink":"https://charlesliuyx.github.io/tags/Dota2/"},{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://charlesliuyx.github.io/tags/Data-Analysis/"},{"name":"Wiki","slug":"Wiki","permalink":"https://charlesliuyx.github.io/tags/Wiki/"}]},{"title":"深入浅出看懂AlphaGo元","date":"2017-10-19T03:54:32.000Z","path":"2017/10/18/深入浅出看懂AlphaGo元/","text":"【阅读时间】21min - 24min 10999字【内容简介】AlphaGo1.0详解链接，这篇AlphaGo Zero论文原文超详细翻译，并且总结了AlphaGo Zero的算法核心思路，附带收集了网上的相关评论 在之前的详解：深入浅出看懂AlphaGo中，详细定义的DeepMind团队定义围棋问题的结构，并且深入解读了AlphaGo1.0每下一步都发生了什么事，就在最近，AlphaGo Zero横空出世。个人观点是，如果你看了之前的文章，你就会觉得这是一个水到渠成的事情 另，如果你只对这个事件感兴趣的，而不想了解论文和技术细节，链接奉上，欢迎跳过到最后评论和总结部分（但这部分网上的大牛太多了，知乎答案内最高票对结合围棋的分析很漂亮！建议阅读） 上限置信区间算法（UCT），一种博弈树搜索算法，是AlphaGo中一个重要组成部分：MCTS搜索算法中的核心 法国南巴黎大学的数学家西尔万·热利(SylvainGelly)与巴黎技术学校的王毅早(YizaoWang，音译)将UCT集成到一个他们称之为MoGo的程序中。该程序的胜率竟然比先前最先进的蒙特卡罗扩展算法几乎高出了一倍。 2007年春季，MoGo在大棋盘比赛中也击败了实力稍弱的业余棋手，充分展示了能力。科奇什（UCT算法发明者）预言，10年以后，计算机就能攻克最后的壁垒，终结人类职业棋手对围棋的统治。今年是2017年，AlphaGo系列横空出世。10年，总有着天才的人具有先知般的远见。详见UTC算法 【小发现】看完论文发现，这篇文章的接受时间是2017年4月7号，审核完成时间是2017年9月13号，而在乌镇对阵柯洁（2017年5月23号）用的可能是AlphaGo Master（这里没法证据来证明到底是AlphaGo Zero还是AlphaGo Master）。这个团队也是无情啊，人类再一次感觉被耍了，根据Elo得分，Deepmind团队可能在赛前就透露过吧，即使是Master也有4858分啊，对于一个棋手来说，我感受到的是风萧萧兮易水寒决绝的背影。为柯洁的勇气打Call，当真围棋第一人，天下无双 论文正文内容详细解析先上干货论文：Mastering the Game of Go without Human Knowledge ，之后会主要以翻译论文为主，在语言上尽量易懂，避免翻译腔 AlphaGo Zero，从本质上来说完全不同于打败樊麾和李世石的版本 算法上，自对弈强化学习，完全从随机落子开始，不用人类棋谱。之前使用了大量棋谱学习人类的下棋风格） 数据结构上，只有黑子白子两种状态。之前包含这个点的气等相关棋盘信息 模型上，使用一个神经网络。之前使用了策略网络（基于深度卷积神经网）学习人类的下棋风格，局面网络（基于左右互搏生成的棋谱，为什么这里需要使用左右互搏是因为现有的数据集不够，没法判断落子胜率这一更难的问题）来计算在当前局面下每一个不同落子的胜率 策略上，基于训练好的这个神经网，进行简单的树形搜索。之前会使用蒙特卡洛算法实时演算并且加权得出落子的位置 AlphaGo Zero 的强化学习问题描述在开始之前，必须再过一遍如何符号化的定义一个围棋问题 围棋问题，棋盘 19×19=361 个交叉点可供落子，每个点三种状态，白（用1表示），黑（用-1表示），无子（用0表示），用 $\\vec s$ 描述此时棋盘的状态，即棋盘的状态向量记为 $ \\vec s$ （state首字母）$$\\vec s = (\\underbrace{1,0,-1,\\ldots}_{\\text{361}})$$假设状态 $\\vec s$ 下，暂不考虑不能落子的情况， 那么下一步可走的位置空间也是361个。将下一步的落子行动也用一个361维的向量来表示，记为 $\\vec a$ （action首字母）$$\\vec a = (0,\\ldots,0,1,0,\\ldots)$$公式1.2 假设其中1在向量中位置为39，则 $\\vec a$ 表示在棋盘3行1列位置落白子，黑白交替进行 有以上定义，我们就把围棋问题转化为。 任意给定一个状态 $\\vec s$ ，寻找最优的应对策略 $\\vec a$ ，最终可以获得棋盘上的最大地盘 简而言之 看到 $\\vec s$ ，脑海中就是一个棋盘，上面有很多黑白子 看到 $\\vec a$ ，脑海中就想象一个人潇洒的落子 网络结构新的网络中，使用了一个参数为 $\\theta$ （需要通过训练来不断调整） 的深度神经网络$f_\\theta$ 【网络输入】19×19×170/1值：现在棋盘状态的 $\\vec s$ 以及7步历史落子记录。最后一个位置记录黑白，0白1黑，详见 【网络输出】两个输出：落子概率（362个输出值）和一个评估值（[-1,1]之间）记为 $f_{\\theta}(\\mathbf {\\vec s}) = (\\mathbf p,v)$ 【落子概率 $\\mathbf p$】 向量表示下一步在每一个可能位置落子的概率，又称先验概率 （加上不下的选择），即 $p_a = Pr(\\vec a|\\mathbf {\\vec s})$ （公式表示在当前输入条件下在每个可能点落子的概率） 【评估值 $v$】 表示现在准备下当前这步棋的选手在输入的这八步历史局面 $\\vec s$ 下的胜率（我这里强调局面是因为网络的输入其实包含历史对战过程） 【网络结构】基于Residual Network（大名鼎鼎ImageNet冠军ResNet）的卷积网络，包含20或40个Residual Block（残差模块），加入批量归一化Batch normalisation与非线性整流器rectifier non-linearities模块 改进的强化学习算法自对弈强化学习算法（什么是强化学习，非常建议先看看强化学习的一些基本思想和步骤，有利于理解下面策略、价值的概念，推荐系列笔记） 在每一个状态 $\\vec s$ ，利用深度神经网络 $f_\\theta$ 预测作为参照执行MCTS搜索（蒙特卡洛搜索树算法），MCTS搜索的输出是每一个状态下在不同位置对应的概率 $\\boldsymbol \\pi$ （注意这里是一个向量，里面的值是MCTS搜索得出的概率值），一种策略，从人类的眼光来看，就是看到现在局面，选择下在每个不同的落子的点的概率。如下面公式的例子，下在$(1,3)$位置的概率是0.92，有很高概率选这个点作为落子点 $$ \\boldsymbol \\pi_i = (\\underbrace{0.01,0.02,0.92,\\ldots}_{\\text{361}}) $$ MCTS搜索得出的落子概率比 $f_\\theta$ 输出的仅使用神经网络输出的落子概率 $\\mathbf p$ 更强，因此，MCTS可以被视为一个强力的策略改善（policy improvement）过程 使用基于MCTS提升后的策略（policy）来进行落子，然后用自对弈最终对局的胜者 $z$ 作为价值（Value），作为一个强力的策略评估（policy evaluation）过程 并用上述的规则，完成一个通用策略迭代算法去更新神经网络的参数 $\\theta$ ，使得神经网络输出的落子概率和评估值，即 $f_{\\theta}(\\mathbf {\\vec s}) = (\\mathbf p,v)$ 更加贴近能把这盘棋局赢下的落子方式（使用不断提升的MCST搜索落子策略$\\boldsymbol \\pi$ 和自对弈的胜者 $z$ 作为调整依据）。并且，在下轮迭代中使用新的参数来进行自对弈 在这里补充强化学习的通用策略迭代（Generalized Policy Iteration）方法 从策略 $\\pi_0$ 开始 策略评估（Policy Evaluation）- 得到策略 $\\pi_0$ 的价值 $v_{\\pi_0}$ （对于围棋问题，即这一步棋是好棋还是臭棋） 策略改善（Policy Improvement）- 根据价值 $v_{\\pi_0}$，优化策略为 $\\pi_{0+1}$ （即人类学习的过程，加强对棋局的判断能力，做出更好的判断） 迭代上面的步骤2和3，直到找到最优价值 $v_*$ ，可以得到最优策略 $\\pi_*$ 【a图】表示自对弈过程 $s_1,\\ldots,s_T$。在每一个位置 $s_t$ ，使用最新的神经网络 $f_\\theta$ 执行一次MCTS搜索 $\\alpha_\\theta$ 。根据搜索得出的概率 $a_t \\sim \\boldsymbol \\pi_i$ 进行落子。终局 $s_T$ 时根据围棋规则计算胜者 $z$$\\pi_i$ 是每一步时执行MCTS搜索得出的结果（柱状图表示概率的高低） 【b图】表示更新神经网络参数过程。使用原始落子状态 $\\vec s_t$ 作为输入，得到此棋盘状态 $\\vec s_t$ 下下一步所有可能落子位置的概率分布 $\\mathbf p_t$ 和当前状态 $\\vec s_t$ 下选手的赢棋评估值 $v_t$ 以最大化 $\\mathbf p_t$ 与 $\\pi_t$ 相似度和最小化预测的胜者 $v_t$ 和局终胜者 $z$ 的误差来更新神经网络参数 $\\theta$ （详见公式1） ，更新参数 $\\theta$ ，下一轮迭代中使用新神经网络进行自我对弈 我们知道，最初的蒙特卡洛树搜索算法是使用随机来进行模拟，在AlphaGo1.0中使用局面函数辅助策略函数作为落子的参考进行模拟。在最新的模型中，蒙特卡洛搜索树使用神经网络 $f_\\theta$ 的输出来作为落子的参考（详见下图Figure 2） 每一条边 $(\\vec s,\\vec a)$ （每个状态下的落子选择）保存的是三个值：先验概率 $P(\\vec s,\\vec a)$，访问次数 $N(\\vec s,\\vec a)$，行动价值 $Q(\\vec s,\\vec a)$。 每次模拟（模拟一盘棋，直到分出胜负）从根状态开始，每次落子最大化上限置信区间 $Q(\\vec s,\\vec a) + U(\\vec s,\\vec a)$ 其中 $U(\\vec s,\\vec a) \\propto \\frac{P(\\vec s,\\vec a)}{1 + N(\\vec s,\\vec a)}$ 直到遇到叶子节点 $s’$ 叶子节点（终局）只会被产生一次用于产生先验概率和评估值，符号表示即 $f_\\theta(s’) = (P(s’,\\cdot), V(s’))$ 模拟过程中遍历每条边 $(\\vec s, \\vec a)$ 时更新记录的统计数据。访问次数加一 $N(\\vec s,\\vec a) += 1$；更新行动价值为整个模拟过程的平均值，即 $Q(\\vec s, \\vec a) = \\frac {1}{N(\\vec s, \\vec a)}\\Sigma_{\\vec s'|\\vec s, \\vec a \\Rightarrow \\vec s'}V(\\vec s')$ ，$\\vec s’|\\vec s, \\vec a \\Rightarrow \\vec s’$ 表示在模拟过程中从 $\\vec s$ 走到 $\\vec s’$的所有落子行动 $\\vec a$ 【a图】表示模拟过程中遍历时选 $Q+U$ 更大的作为落子点 【b图】叶子节点 $s_L$ 的扩展和评估。使用神经网络对状态 $s_L$ 进行评估，即 $f_\\theta(s_L) = (P(s_L,\\cdot), V(s_L))$ ，其中 $\\mathbf P$ 的值存储在叶子节点扩展的边中 【c图】更新行动价值 $Q$ 等于此时根状态 $\\vec s$ 所有子树评估值 $V$ 的平均值 【d图】当MCTS搜索完成后，返回这个状态 $\\vec s$ 下每一个位置的落子概率 $\\boldsymbol \\pi$，成比例于 $N^{1/\\tau}$（$N$为访问次数，$\\tau$ 为控温常数） 更加具体的详解见：搜索算法 MCTS搜索可以看成一个自对弈过程中决定每一步如何下的依据，根据神经网络的参数 $\\theta$ 和根的状态 $\\vec s$ 去计算每个状态下落子位置的先验概率，记为 $\\boldsymbol \\pi = \\alpha_\\theta(\\vec s)$ ，幂指数正比于访问次数 $\\pi_{\\vec a} \\propto N(\\vec s, \\vec a)^{1/\\tau}$，$\\tau$ 是温度常数 训练步骤总结使用MCTS下每一步棋，进行自对弈，强化学习算法（必须了解通用策略迭代的基本方法）的迭代过程中训练神经网络 神经网络参数随机初始化 $\\theta_0$ 每一轮迭代 $i \\geqslant 1$ ，都自对弈一盘（见Figure-1a） 第 $t$ 步：MCTS搜索 $\\boldsymbol \\pi_t = \\alpha_{\\theta_{i-1}}(s_t)$ 使用前一次迭代的神经网络 $f_{\\theta_{i-1}}$，根据MCTS结构计算出的落子策略 $\\boldsymbol \\pi_t$ 的联合分布进行【采样】再落子 在 $T$ 步 ：双方都选择跳过；搜索时评估值低于投降线；棋盘无地落子。根据胜负得到奖励值Reward $r_T \\in \\{-1,+1\\}$。 MCTS搜索下至中盘的过程的每一个第 $t$ 步的数据存储为 $\\vec s_t,\\mathbf \\pi_t, z_t$ ，其中 $z_t = \\pm r_T$ 表示在第 $t$ 步时的胜者 同时，从上一步 $\\vec s$ 迭代时自对弈棋局过程中产生的数据 $(\\vec s, \\boldsymbol \\pi, z)$ （$\\vec s$ 为训练数据，$\\boldsymbol \\pi, z$ 为标签）中采样（这里的采样是指选Mini-Batch）来训练网络参数 $\\theta_i$， 神经网络 $f_{\\theta_i}(\\vec s) = (\\mathbf p, v)$以最大化 $\\mathbf p_t$ 与 $\\pi_t$ 相似度和最小化预测的胜者 $v_t$ 和局终胜者 $z$ 的误差来更新神经网络参数 $\\theta$ ，损失函数公式如下 $$ l = (z - v)^2 - \\boldsymbol {\\pi}^T \\log(\\mathbf p) + c \\Vert \\theta \\Vert ^2 \\tag 1 $$ 其中 $c$ 是L2正则化的系数 AlphaGo Zero训练过程中的经验最开始，使用完全的随机落子训练持续了大概3天。训练过程中，产生490万场自对弈，每次MCTS大约1600次模拟，每一步使用的时间0.4秒。使用了2048个位置的70万个Mini-Batches来进行训练。 训练结果如下，图3 【a图】表示随时间AlphaGo Zero棋力的增长情况，显示了每一个不同的棋手 $\\alpha_{\\theta_i}$ 在每一次强化学习迭代时的表现，可以看到，它的增长曲线非常平滑，没有很明显的震荡，稳定性很好 【b图】表示的是预测准确率基于不同迭代第$i$轮的 $f_{\\theta_i}$ 【c图】表示的MSE（平方误差） 在24小时的学习后，无人工因素的强化学习方案就打败了通过模仿人类棋谱的监督学习方法 为了分别评估结构和算法对结构的影响，得到了，下图4 【dual-res】表示 AlphaGo Zero（20个模块），策略头和评估值头由一个网络产生【sep-res】表示使用20个残差模块，策略头和评估值头被分成两个不同的网络【dual-conv】表示不用ResNet，使用12层卷积网，同时包括策略头和评估值头【sep-conv】表示 AlphaGo Lee（击败李世乭的）使用的网络结构，策略头和评估值头被分成两个不同的网络 头的概念详见网络结构 AlphaGo Zero学到的知识在训练过程中，AlphaGo Zero可以一步步的学习到一些特殊的围棋技巧（定式），如图5 中间的黑色横轴表示的是学习时间 【a图】对应的5张棋谱展现的是不同阶段AlphaGo Zero在自对弈过过程中展现出来的围棋定式上的新发现 【b图】展示在右星位上的定式下法的进化。可以看到训练到50小时，点三三出现了，但再往后训练，b图中的第五种定式高频率出现，在AlphGa Zero看来，这一种形式似乎更加强大 【c图】展现了前80手自对弈的棋谱伴随时间，明显有很大的提升，在第三幅图中，已经展现出了比较明显的围的倾向性 具体频率图见：出现频率随训练时间分布图 AlphaGo Zero的最终实力之后，最终的AlphaGo Zero 使用40个残差模块，训练接近40天。在训练过程中，产生了2900万盘的自对弈棋谱，使用了310万个Mini-Batches来训练神经网络，每一个Mini-Batch包含了2048个不同的状态。（覆盖的状态数是63亿（$10^{10}$），但和围棋的解空间 $2^{361} \\approx 10^{108}$ 相比真的很小，也从侧面反映出，围棋中大部分选择都是冗余的。在一个棋盘局面下，根据先验概率，估计只有15-20种下法是值得考虑的） 被评测不同版本使用计算力的情况，AlphaGo Zero和AlphaGo Master被部署到有4个TPUs的单机上运行（主要用于做模型的输出预测Inference和MCTS搜索），AlphaGo Fan（打败樊麾版本）和AlphaGo Lee（打败李世乭版本） 分布式部署到机器群里，总计有176GPUs和48GPUs（Goolge真有钱）。还加入了raw network，它是每一步的仅仅使用训练好的深度学习神经网的输出 $\\mathbf p_a$ 为依据选择最大概率点来落子，不使用MCTS搜索（Raw Network裸用深度神经网络的输出已经十分强大，甚至已经接近了AlphaGo Fan） 下图6展示不同种AlphaGo版本的棋力情况 【a图】随着训练时间棋力的增强曲线 【b图】裸神经网络得分3055，AlphaGo Zero得分5185，AlphaGo Master得分4858，AlphaGo Lee得分3738，AlphaGo Fan得分3144 最终，AlphaGo Zero 与 AlphaGo Master的对战比分为89：11，对局中限制一场比赛在2小时之内（新闻中的零封是对下赢李世乭的AlphaGo Lee） 论文附录内容我们知道，Nature上的文章一般都是很强的可读性和严谨性，每一篇文章的正文可能只有4-5页，但是附录一般会远长于正文。基本所有你的技术细节疑惑都可以在其中找到结果，这里值列举一些我自己比较感兴趣的点，如果你是专业人士，甚至想复现AlphaGo Zero，读原文更好更精确 围棋领域先验知识AlphaGo Zero最主要的贡献是证明了没有人类的先验知识机器也可以在性能上超越人类。为了阐释清楚这种贡献来自于何处，我们列举一些AlphaGo Zero使用到的知识，无论是训练过工程中的还是MCTS搜索中的。如果你想把AlphaGo Zero的思路应用的到解决其他游戏问题上，这些内容可能需要被替换 围棋基本规则无论实在MCTS搜索中的模拟还是自对弈的过程，都依赖游戏最终的胜负规则，并且在落子过程中，根据规则还可以排除一部分不可以落子的点（比如已经落子的点，无法确认在AlphaGo Zero还有气为零的点不能下这个规则，因为不记录气的信息了。但可以写一个函数来判断当前局面 $\\vec s$ 下下一步所有可能的落子点，不一定非得计算这个信息，这个过程可以完全多线程） Tromp-Taylor规则在AlphaGo Zero中使用的是PSK（Positional Superko）禁全同规则（中国，韩国及日本使用），只要这一手（不包括跳过）会导致再现之前的局面，就禁止。 旋转与镜面对于围棋来说，几个状态 $\\vec s$ 在经过旋转或反射后是完全相同的，这种规律可以用来优化训练数据和MCTS搜索中的子树替换策略。并且因为贴目（黑棋先下优势贴目7目半）规则存在，不同状态 $\\vec s$ 换颜色也是相同的。这个规则可以用来使用当前下子的棋手的角度来表示棋盘 除了以上的三个规则，AlphaGo Zero 没有使用其他任何先验知识，它仅仅使用深度神经网络对叶子节点进行评估并选择落子位置。它没有使用任何Rollout Policy(这里指的应该是AlphaGo之前版本的快速走子策略)或者树形规则，MCTS搜索也没有使用其他的标准启发式规则或者先验常识规则去进行增强 整个算法从随机初始化神经网络参数开始。网络结构和超参数选择 见下一节。MCTS搜索的超参数 $c_{puct}$ 由高斯过程优化决定，为了优化自对弈的性能，使用了一个神经网络进行预训练。对于一个大规模网络的训练过程（40个残差模块，40天），使用一个小规模网络（20个残差模块，3天）来反复优化MCTS搜索的超参数 $c_{puct}$。整个训练过程没有任何人工干预 自对弈训练工作流AlphaGo Zero的工作流由三个模块构成，可以异步多线程进行： 深度神经网络参数 $\\theta_i$ 根据自对弈数据持续优化 持续对棋手 $\\alpha_{\\theta_i}$ 棋力值进行评估 使用表现最好的 $\\alpha_{\\theta_*}$ 用来产生新的自对弈数据 优化参数每一个神经网络 $f_{\\theta_i}$ 在64个GPU工作节点和19个CPU参数服务器上进行优化。 每个工作节点的批次（Batch）大小是32，每一个mini-batch大小为2048。每一个 mini-batch 的数据从最近50万盘的自对弈棋谱的状态中联合随机采样。 神经网络权重更新使用带有动量（momentum）和学习率退火（learning rate annealing）的随机梯度下降法（SGD），损失函数见公式1 学习率退火比率见下表 步数（千） 强化学习率 监督学习率 0-200 $10^{-2}$ $10^{-1}$ 200-400 $10^{-2}$ $10^{-2}$ 400-600 $10^{-3}$ $10^{-3}$ 600-700 $10^{-4}$ $10^{-4}$ 700-800 $10^{-4}$ $10^{-5}$ &gt;800 $10^{-4}$ - 动量参数设置为0.9 方差项和交叉项的权重相同，原因是奖励值被归一化到 $r \\in [-1,+1]$ L2正则化系数设置为 $c = 10^{-4}$ 优化过程每1000个训练步数执行一次，并使用这个新模型来生成下一个Batch的自对弈棋谱 评估器为了保证生成数据的质量（不至于棋力反而下降），在使用新的神经网络去生成自对弈棋谱前，用现有的最好网络 $f_{\\theta_*}$ 来对它进行评估 【评估神经网络 $f_{\\theta_i}$ 的方法】使用 $f_{\\theta_i}$ 进行MCTS搜索得出的 $\\alpha_{\\theta_i}$ 的性能（得到 $\\alpha_{\\theta_i}$ 的MCTS搜索过程中使用 $f_{\\theta_i}$ 去估计叶子节点的位置和先验概率，详见MCTS搜索这一节） 每一个评估由400盘对局组成，MCTS搜索使用1600次模拟，将温度参数设为无穷小 $\\tau \\Rightarrow 0$（目的是为了使用最多访问次数的落子下法去下，追求最强的棋力），如果新的选手 $\\alpha_{\\theta_i}$ 在这400盘中胜率大于55%，将这个选手更新为最佳选手 $\\alpha_{\\theta_*}$ ，用来产生下一轮的自对弈棋谱，并且设为下一轮的比较对象 自对弈通过评估器，现在已经有一个当前的最好棋手 $\\alpha_{\\theta_*}$，使用它来产生数据。每次迭代中， $\\alpha_{\\theta_*}$ 自对弈25000盘，其中每一步MCTS搜索模拟1600次（模拟的每次落子大约0.4秒，这里的一次表示的就是MCTS搜索中走到叶子节点，得出胜负结果） 前30步，温度 $\\tau = 1$，与MCTS搜索中的访问次数成正比，目的是保证前30步下法的多样性。在之后的棋局中，温度设为无穷小。并在先验概率中加入狄利克雷噪声 $P(\\vec s, \\vec a) = (1 - \\epsilon) p_{\\vec a} + \\epsilon \\eta_{\\vec a}$ ，其中 $\\eta \\sim Dir(0.03)$ 且 $\\epsilon = 0.25$。这个噪声保证所有的落子可能都会被尝试，但也可能下出臭棋 投降阈值 $v_{rerign}$ 自动设为错误正类率（如果AlphaGo没有投降可以赢的比例）小于5%，为了测量错误正类(false positives)，在10%的自对弈中关闭投降机制，必须下完 监督学习为了进行对比，我们还使用监督学习训练了一个参数为 $\\theta_{SL}$ 神经网络。神经网络的结构和AlphaGo Zero相同。数据集 $(\\vec s, \\boldsymbol \\pi, z)$ 随机采样自KGS数据集，人类的落子策略位置即设置 $\\pi_a = 1$ 。使用同样的超参数和损失函数，但是平方误差的系数为0.01，学习率图参照上表的第二列。其他超参数和上一节相同 比AlphaGo1.0z中使用两种网络，使用这种结构的网络，可以有效的防止过拟合。并且实验也证明这个网络结构的的效果要好于之前的网络 MCTS搜索算法这一部分详解的AlphaGo Zero的算法核心示意图Figure2 AlphaGo Zero使用的是比AlphaGo1.0中更简单的异步策略价值MCTS搜索算法（APV-MCTS）的变种 搜索树中的节点 $\\vec s$ 包含一条边 $(\\vec s,\\vec a)$ 对应所有可能的落子 $\\vec a \\in \\mathcal A(\\vec s)$ ，每一条边中存储一个数据，包含下列公式的四个值$${N(\\vec s,\\vec a), W(\\vec s,\\vec a), Q(\\vec s,\\vec a), P(\\vec s,\\vec a)}$$ $N(\\vec s,\\vec a)$ 表示MCST搜索模拟走到叶子节点的过程中的访问次数$W(\\vec s,\\vec a)$ 表示行动价值（由路径上所有的 $v$ 组成）的总和$Q(\\vec s,\\vec a)$ 表示行动价值的均值$P(\\vec s,\\vec a)$ 表示选择这条边的先验概率（一个单独的值） 多线程（并行）执行多次模拟，每一次迭代过程先重复执行1600次Figure 2中的前3个步骤，计算出一个 $\\boldsymbol \\pi$ ，根据这个向量下现在的这一步棋 Selcet - Figure2aMCTS中的选择步骤和之前的版本相似，详见AlphaGo之前的详解文章，这篇博文详细通俗的解读了这个过程。概括来说，假设L步走到叶子节点，当走第 $t &lt; L$ 步时，根据搜索树的统计概率落子 $$ \\vec a_t = \\operatorname*{argmax}_{\\vec a}(Q(\\vec s_t, \\vec a) + U (\\vec s_t, \\vec a)) $$ 其中计算 $U (\\vec s_t, \\vec a)$ 使用PUCT算法的变体 $$ U(\\vec s, \\vec a) = c_{puct}P(\\vec s, \\vec a) \\frac{\\sqrt{\\Sigma_{\\vec b} N(\\vec s, \\vec b)}}{1 + N(\\vec s, \\vec a)} $$ 其中 $c_{puct}$ 是一个常数。这种搜索策略落子选择最开始更趋向于高先验概率和低访问次数的，但逐渐的会更加趋向于选择有着更高行动价值的落子 $c_{puct}$ 使用贝叶斯高斯过程优化来确定 Expand and evaluate - Figure 2b将叶子节点 $\\vec s_L$ 加到队列中等待输入至神经网络进行评估， $f_\\theta(d_i(\\vec s_L)) = (d_i(p), v)$ ，其中 $d_i$ 表示一个1至8的随机数来表示双方向镜面和旋转（从8个不同的方向进行评估，如下图所示，围棋棋型在很多情况如果从视觉角度来提取特征来说是同一个节点，极大的缩小了搜索空间） 队列中8个不同位置组成一个大小为8的mini-batch输入到神经网络中进行评估。整个MCTS搜索线程被锁死直到评估过程完成（这个锁死是保证并行运算间同步）。叶子节点被展开(Expand)，每一条边 $(\\vec s_L,\\vec a)$被初始化为 $$ {N(\\vec s_L,\\vec a) = 0 ;\\; W(\\vec s_L,\\vec a) = 0; \\; Q(\\vec s_L,\\vec a) = 0\\\\P(\\vec s_L,\\vec a) = p_a} $$ 这里的 $p_a$ 由将 $\\vec s$ 输入神经网络得出 $\\mathbf p$ （包括所有落子可能的概率值 $p_a$），然后将神经网络的输出值 $v$ 传回（backed up） Backup - Figure 2c沿着扩展到叶子节点的路线回溯将边的统计数据更新（如下列公式所示） $$ N(\\vec s_t, \\vec a_t) = N(\\vec s_t, \\vec a_t) + 1 \\\\ W(\\vec s_t, \\vec a_t) = W(\\vec s_t, \\vec a_t) + v \\\\ Q(\\vec s_t, \\vec a_t) = \\frac{W(\\vec s_t, \\vec a_t) }{N(\\vec s_t, \\vec a_t) } $$ 注解：在 $W(\\vec s_t, \\vec a_t)$ 的更新中，使用了神经网络的输出 $v$，而最后的价值就是策略评估中的这一状态的价值 $Q(\\vec s, \\vec a)$ 使用虚拟损失（virtual loss）确保每一个线程评估不同的节点。实现方法概括为把其他节点减去一个很大的值，避免其他搜索进程走相同的路，详见 Play - Figure 2d完成MCTS搜索（并行重复1-3步1600次，花费0.4s）后，AlphaGo Zero才从 $\\vec s_0$ 状态下走出第一步 $\\vec a_0$，与访问次数成幂指数比例 $$ \\boldsymbol \\pi(\\vec a|\\vec s_0) = \\frac {N(\\vec s_0,a)^{1/\\tau}}{\\Sigma_{\\vec b} N(\\vec s_0, \\vec b)^{1/\\tau}} $$ 其中 $\\tau$ 是一个温度常数用来控制探索等级（level of exploration）。它是热力学玻尔兹曼分布的一种变形。温度较高的时候，分布更加均匀（走子多样性强）；温度降低的时候，分布更加尖锐（多样性弱，追求最强棋力） 搜索树会在接下来的自对弈走子中复用，如果孩子节点和落子的位置吻合，它就成为新的根节点，保留子树的所有统计数据，同时丢弃其他的树。如果根的评价值和它最好孩子的评价值都低于 $v_{resign}$ AlphaGo Zero就认输 MCTS搜索总结与之前的版本的MCTS相比，AlphaGo Zero最大的不同是没有使用走子网络（Rollout），而是使用一个整合的深度神经网络；叶子节点总会被扩展，而不是动态扩展；每一次MCTS搜索线程需要等待神经网络的评估，之前的版本性能评估（evaluate）和返回（backup）是异步的；没有树形策略 至于很重要的一个关键点：每一次模拟的中的叶子节点L的深度 【个人分析】是由时间来决定，根据论文提到的数据，0.4秒执行1600次模拟，多线程模拟，在时限内能走到的深度有多深就是这个叶子节点。可以类比为AlphaGo 1.0中的局面函数（用来判断某个局面下的胜率的），也就是说不用模拟到终盘，在叶子节点的状态下，使用深度神经网的输出 $v$ 来判断现在落子的棋手的胜率 网络结构网络输入数据输入数据的维度 19×19×17，其中存储的两个值0/1，$X_t^i = 1$表示这个交叉点有子，$0$ 表示这个交叉点没有子或是对手的子或 $t&lt;0$。使用 $Y_t$ 来记录对手的落子情况。 从状态 $\\vec s$ 开始，记录了倒退回去的15步，双方棋手交替。最后一个19×19的存储了前面16步每一个状态对应的棋子的黑白颜色。1黑0白 【个人理解】为了更加直观的解释，如果是上面的局部棋盘状态 $\\vec s$，接下里一步是黑棋落子，走了4步，那么输入数据是什么样的呢？ $$ \\mathbf X_2= \\begin{pmatrix} & \\vdots & \\vdots & \\\\ \\cdots & 0 & 1 & \\cdots \\\\ \\cdots & 1 & 0 & \\cdots \\\\ & \\vdots & \\vdots &\\end{pmatrix} \\mathbf Y_2= \\begin{pmatrix} & \\vdots & \\vdots & \\\\ \\cdots & 1 & 0 & \\cdots \\\\ \\cdots & 0 & 1 & \\cdots \\\\ & \\vdots & \\vdots &\\end{pmatrix} \\\\ \\mathbf X_1= \\begin{pmatrix} & \\vdots & \\vdots & \\\\ \\cdots & 0 & 0 & \\cdots \\\\ \\cdots & 1 & 0 & \\cdots \\\\ & \\vdots & \\vdots &\\end{pmatrix} \\mathbf Y_1= \\begin{pmatrix} & \\vdots & \\vdots & \\\\ \\cdots & 0 & 0 & \\cdots \\\\ \\cdots & 0 & 1 & \\cdots \\\\ & \\vdots & \\vdots &\\end{pmatrix} \\\\ \\mathbf C = \\begin{pmatrix} & \\vdots & \\vdots & \\\\ \\cdots & 1 & 0 & \\cdots \\\\ \\cdots & 0 & 1 & \\cdots \\\\ & \\vdots & \\vdots &\\end{pmatrix} $$ 同理，如果有8步的话，也就是16个对应的 $X$ 和 $Y$ 加一个 $C$ 来表示现在的棋盘状态（注意，这里面包含的历史状态）。这里的数据类型是Boolean，非常高效，并且表达的信息也足够 至于使用八步的原因。个人理解，一方面是为了避免循环劫，另一方面，选择八步也可能是性能和效果权衡的结果（从感知上来说当然信息记录的越多神经网络越强，奥卡姆剃刀定理告诉我们，简单即有效，一味的追求复杂，并不是解决问题的最佳途径） 深度神经网结构整个残差塔使用单独的卷机模块组成，其中包含了19或39个残差模块，详细结构参数如下图所示 过了深度卷积神经网络后接策略输出与评估值输出，详细结构参数如下图所示 数据集GoKifu数据集，和KGS数据集 图5更多细节 Figure 5a中每种定式出现的频率图 Figure 5b中每种定式出现的频率图 总结与随想AlphaGo Zero = 启发式搜索 + 强化学习 + 深度神经网络，你中有我，我中有你，互相对抗，不断自我进化。使用深度神经网络的训练作为策略改善，蒙特卡洛搜索树作为策略评价的强化学习算法 之后提出一些我在看论文时带着的问题，最后给出我仔细看完每一行论文后得出的回答，如有错误，请批评指正！ 问题与个人答案训练好的Alpha Zero在真实对弈时，在面对一个局面时如何决定下在哪个位置？评估器的落子过程即最终对弈时的落子过程（自对弈中的落子就是真实最终对局时的落子方式）：使用神经网络的输出 $\\mathbf p$ 作为先验概率进行MCTS搜索，每步1600次（最后应用的版本可能和每一步的给的时间有关）模拟，前30步采样落子，剩下棋局使用最多访问次数来落子，得到 $\\boldsymbol \\pi$ ，然后选择落子策略中最大的一个位置落子 AlphaGo Zero的MCTS搜索算法和和上个版本的有些什么区别？最原始MCTS解析，AlphaGo Lee加上策略函数和局面函数改进后的MCTS解析 对于AlphaGo Zero来说 最大的区别在于，模拟过程中依据神经网络的输出 $\\mathbf p$ 的概率分布采样落子。采样是关键词，首先采样保证一定的随机特性，不至于下的步数过于集中，其次，如果模拟的盘数足够多，那这一步就会越来越强 其次，在返回（Bakcup）部分每一个点的价值（得分），使用了神经网络的输出 $v$。这个值也是策略评估的重要依据 AlphaGo Zero 中的策略迭代法是如何工作的？策略迭代法（Policy Iteration）是强化学习中的一种算法，简单来说：以某种策略（ $\\pi_0$ ）开始，计算当前策略下的价值函数（ $v_{\\pi_0}$ ）；然后利用这个价值函数，找到更好的策略（Evaluate和Improve）；接下来再用这个更好的策略继续前行，更新价值函数……这样经过若干轮的计算，如果一切顺利，我们的策略会收敛到最优的策略（ $\\pi_*$ ），问题也就得到了解答。 $$ \\pi_0 \\xrightarrow{E} v_{\\pi_0} \\xrightarrow{I} \\pi_1 \\xrightarrow{E} v_{\\pi_1} \\xrightarrow{I} \\pi_2 \\xrightarrow{E} \\cdots \\xrightarrow{I} \\pi_* \\xrightarrow{E} v_* $$ 对于AlphaGo Zero来说，详细可见论文，简单总结如下 策略评估过程，即使用MCTS搜索每一次模拟的对局胜者，胜者的所有落子（$\\vec s$）都获得更好的评估值 策略提升过程，即使用MCTS搜索返回的更好策略 $\\boldsymbol \\pi$ 迭代过程，即神经网络输出 $\\mathbf p$ 和 $v$ 与策略评估和策略提升返回值的对抗（即神经网络的训练过程） 总的来说，有点像一个嵌套过程，MCST算法可以用来解决围棋问题，这个深度神经网络也可以用来解决围棋问题，而AlphaGo Zero将两者融合，你中有我，我中有你，不断对抗，不对自我进化 AlphaGo Zero 最精彩的部分哪部分？ $$ l = (z - v)^2 - \\boldsymbol {\\pi}^T \\log(\\mathbf p) + c \\Vert \\theta \\Vert ^2 $$ 毫无悬念的，我会选择这个漂亮的公式，看懂公式每一项的来历，即产生的过程，就读懂了AlphaGo Zero。这个公式你中有我，我中有你，这是一个完美的对抗，完美的自我进化 第二我觉得很精彩的点子是将深度神经网络作为一个模块嵌入到了强化学习的策略迭代法中。最关键的是，收敛速度快，效果好，解决各种复杂的局面（比如一个关于围棋棋盘的观看角度可以从八个方向来看的细节处理的很好，又如神经网络的输入状态选择了使用历史八步） 随想和评论量子位汇集各家评论 不是无监督学习，带有明显胜负规则的强化学习是强监督的范畴 无需担心快速的攻克其他领域，核心还是启发式搜索 模型简约漂亮，充满整合哲学的优雅，可怕的是效果和效率也同样极高 AlphaGo项目在经历了把书读厚的过程后，已经取得了瞩目的成就依旧不满足现状，现通过AlphaGo Zero把书读薄，简约而不简单，大道至简，九九归一，已然位列仙班了 随着AlphaGo Zero的归隐，DeepMind已经正式转移精力到其他的任务上了。期待这个天才的团队还能搞出什么大新闻！ 对于围棋这项运动的影响可能是：以后的学围棋手段会发生变化，毕竟世界上能复现AlphaGo Zero的绝对很多，那么AlphaGo Zero的实力那就是棋神的感觉，向AlphaGo Zero直接学习不是更加高效嘛？另，围棋受到的关注也应该涨了一波，是利好 感觉强化学习会越来越热，对于和环境交互这个领域，强化学习更加贴近于人类做决策的学习方式。个人预测，强化学习会在未来会有更多进展！AlphaGo Zero 可能仅仅是一个开头 以上！鞠躬！","tags":[{"name":"AlphaGo","slug":"AlphaGo","permalink":"https://charlesliuyx.github.io/tags/AlphaGo/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://charlesliuyx.github.io/tags/Deep-Learning/"}]},{"title":"【直观详解】线性代数中的转置正交正规正定","date":"2017-10-17T18:01:27.000Z","path":"2017/10/17/【直观详解】线性代数中的正交正规正定转置/","text":"【阅读时间】20min 4589 words【内容简介】从【直观理解】线性代数的本质笔记出发，继续讨论几个线性代数中的概念，正交，正规，正定及转置的直观解释。旨在能帮助读者在看完后不会忘记什么是正交矩阵，什么是正规矩阵，转置部分进行了深入挖掘，希望找出一些几何直观的解释 在之前的【直观理解】线性代数的本质的笔记中，详细讨论了特征值与特征向量的几何直观意义 起初，研究线性代数，也是因为深入了解矩阵（变换）对机器学习中的很多优美公式的推导和理解有帮助。上篇笔记中，3B1B团队的讲解内容中没有涉及几个线性代数中的概念，且这些概念在做矩阵分解时会被用到。以上一篇笔记中的直观理解为基础（矩阵 = 变换）在这里做一个整理和记录 正交矩阵可能很多人已经有一个概念：正交（Orthogonal） = 垂直。但我们知道，正交的一定垂直，垂直的不一定正交（比如空间中两个不相交直线垂直）。提及垂直，首先出现你脑海中的特点是什么呢？我想是勾股数 $a^2 + b^2 = c^2$ ， 还有 $\\cos (\\frac {\\pi}{2}) = 1$ 那什么是正交矩阵呢？在讲这个概念之前，变换中有一种特殊变换：旋转变换。这种变换除了原点外没有特征向量，特征值恒为1，不对网格进行伸缩。或者说，这个变换保证了新列空间内和原列空间内所有对应向量的长度不变 三维情况下，单位矩阵（对角线为1，其他为0，即基向量构成的矩阵）$\\mathbf E = \\left [ \\begin{smallmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0&amp;0&amp;1 \\end{smallmatrix} \\right ]$ 如下图所示 $\\mathbf E$ 中的三个基向量分别记为 $\\mathbf {X_a}= \\left [ \\begin{smallmatrix} 1 \\\\ 0 \\\\ 0 \\end{smallmatrix} \\right ] $ ，$\\mathbf {Y_a} = \\left [ \\begin{smallmatrix} 0 \\\\ 1 \\\\ 0 \\end{smallmatrix} \\right ] $，$\\mathbf {Z_a} = \\left [ \\begin{smallmatrix} 0 \\\\ 0 \\\\ 1 \\end{smallmatrix} \\right ] $ ，用下标a来表示。之后对这个矩阵 $\\mathbf E$ 应用一个旋转变换，以 $(0,-0.6,0.8)$ 为旋转轴，转90°。得到三个新的向量，用下标b来表示，记为 $\\mathbf {X_b}= \\left [ \\begin{smallmatrix} 0 \\\\ 0.8 \\\\ 0.6 \\end{smallmatrix} \\right ] $ ，$\\mathbf {Y_b} = \\left [ \\begin{smallmatrix} -0.8 \\\\ -0.36 \\\\ 0.48 \\end{smallmatrix} \\right ] $，$\\mathbf {Z_b} = \\left [ \\begin{smallmatrix} -0.6 \\\\ 0.48 \\\\ -0.64 \\end{smallmatrix} \\right ] $ 根据基变换原理，易得旋转变换的矩阵表达式 $\\mathbf R = \\left [ \\begin{smallmatrix} 0&amp;-0.8 &amp;-0.6 \\\\ 0.8&amp;-0.36&amp;0.48 \\\\ 0.6&amp;0.48&amp;-0.64 \\end{smallmatrix} \\right ]$ 计算得特征向量为 $(0,-0.75,1)$，发现这条向量即旋转轴！ 此时我们考虑从 $\\mathbf R$矩阵下变到 $\\mathbf E$的变换矩阵是多少，即求 $\\mathbf R$ 矩阵的逆 $$ \\mathbf R^{-1} = \\left [ \\begin{matrix} 0 & 0.8 & 0.6 \\\\-0.8 & -0.36 & 0.48 \\\\-0.6 & 0.48 & -0.64 \\end{matrix}\\right] $$ 观察形式大家就可以发现一个有趣的特点 $\\mathbf R^{-1} = \\mathbf R^T$ 正交矩阵有一个几何直观的特点，表示一个旋转变换，并且矩阵的逆和矩阵的转置相等 正定与半正定矩阵根据特征值和特征向量这篇笔记中的内容，我们知道特征值是对一个变换（矩阵）特性的有力表征，公式 $\\mathbf A \\mathbf{\\vec v} = \\lambda \\mathbf{\\vec v}$ 表示了变换中被留在张成空间内的向量就是特征向量的符号表达，其中 $\\vec v$ 是特征向量，$\\lambda$ 即特征值 我们对上式进行一些数学恒等变换，左乘 $\\vec v^T$，得到 $$ \\vec v^T \\mathbf A \\vec v = \\vec v^T \\lambda \\vec v = \\lambda \\vec v^T \\vec v \\tag{2-1} $$ 此时我们会发现一些巧合，先来看看正定矩阵的正规定义：若一个 n×n的矩阵 $\\mathbf M$ 是正定的，当且仅当队友所有的非零实系数的向量 $\\vec v$，都有 $\\vec v^T \\mathbf M \\vec v &gt; 0$ 我们暂时不考虑复数情况（在机器学习预见复数域的内容较少），结合上面的二公式，发现保证 $\\vec v^T \\mathbf M \\vec v &gt; 0$ 即使得 $\\lambda \\vec v^T \\vec v&gt;0$，其中 $\\vec v^T \\vec v$一定大于等于0（由于 $\\vec v$ 是一个1×n的向量，转置进行矩阵相乘实际效果计算元素的平方和），所以可以推出即正定矩阵就是使得特征值大于0 再回到正定矩阵的定义公式 $\\vec x^T \\mathbf M \\vec x &gt; 0$，我们已经有深刻的理解 $\\mathbf M \\vec x$ 表示对向量 $\\vec x$ 进行变换，记变换后的向量为 $\\vec y = \\mathbf M \\vec x$ ，则我们可以把正定矩阵的公式写成 $$ \\vec x^T \\vec y > 0 \\tag{2-2} $$ 这个公式是不是很熟悉呢？它是两个向量的内积，对于内积，有公式： $$ \\cos(\\theta) = \\frac{\\vec x^T \\vec y}{\\Vert \\vec x \\Vert * \\Vert \\vec y \\Vert} \\hat {\\jmath} $$ $\\Vert \\vec x \\Vert \\; \\Vert \\vec y \\Vert$ 表示 $\\vec x$ 和 $\\vec y$的长度，$\\theta$ 是它们之间的夹角。根据2-2式，可以得到 $\\cos(\\theta) &gt; 0$，即它们之间的夹角小于90度 总结：如果说一个矩阵正定，则表示，一个向量经过此矩阵变换后的向量与原向量夹角小于90度 当然，加一个【半】字，是指这个小于变成小于等于 正规矩阵矩阵中还有一张形状特殊的矩阵，被称为正规矩阵，定义为：如果矩阵 $\\mathbf A$ 满足 $\\mathbf A^T \\mathbf A = \\mathbf A \\mathbf A^T$ 更多的，如果矩阵 $\\mathbf U$ 满足 $\\mathbf U^T \\mathbf U = \\mathbf U \\mathbf U^T = \\mathbf I$，其中 $\\mathbf I$ 是单位矩阵，则称矩阵 $\\mathbf U$ 为酉矩阵 从变换的角度来看正规矩阵，先做一个变换 $\\mathbf A$ 再做一个变换 $\\mathbf A^T$。并且交换两个矩阵的位置，最终结果相同 矩阵的转置什么是转置在前面的三个描绘矩阵不同矩阵的概念中，多次使用了转置的概念。从矩阵形态的角度看，转置是将 $\\mathbf A$ 的所有元素关于一条从第1行第1列元素出发的向右下方45度的射线作镜面翻转（下面的动图更加直观） 那么，从矩阵是表示变换的集合角度如何理解转置呢？ 为什么转置试图从另一个角度来理解其实也是为了回答另一个问题：为什么要定义转置这种操作呢？你可能会说，这就是一个【对角线镜像对称交换的操作】，从形式上来理解对一般人已完全足够。 这里要深究的原因也只是为了克服在学习机器学习的过程中，公式里若出现转置符号，无法完全理解带来的生涩感（俗称强迫症），对博主来说，一个直接动机源于SVD算法 首先，考虑矩阵的列向量有具体的列空间的含义（对应 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 的变换位置），若进行转置操作，列空间的性质会被完全破坏，或者说，转换成了一个新的列空间 非方阵考虑矩阵转置的几何含义是无意义的，或者说，对出现过矩阵转置的公式的进一步理解是没有帮助的 特别的，如果是向量形式（1×n的矩阵），转置很多时候出现，是为了进行二次型运算（即平方运算），设 $\\mathbf x = \\{x_1, x_2,\\ldots, x_n\\}$ 是一个1×n的矩阵 很多机器学习的教材中这里会是列矩阵，因为要切合列空间的概念。对于机器学习来说，这里的 $x_1$ 代表的数据的特征维度 计算二次型： $\\mathbf x \\mathbf x^T = x_1^2 + x_2^2 + \\ldots + x_n^2$ ，记过为一个数，表示的是距离 方阵从列空间的概念，转置是一种非常特殊的旋转。这种旋转结合了横向镜面等特性，详细可以参看下图 从这幅图可以看出，如果想从几何变换的角度（类似3b1b的方法）来理解转置相当困难。此时，需要考虑换一种思考角度，性质和数学计算（这可能也是解决一个数学问题最常用的两种手段，性质寻找共性，并推广演绎，数学计算导出同样问题的不同表现形式，并总结规律） 性质找规律首先先用一个简单的例子，二维可视化，并寻找规律，得到下面图像 观察得到，和旋转有一定的关系的，但是其实这个几何意义已经十分抽象了，为了追根之底，从数学的角度进行一些更有意思的探究 数学计算为了发掘 $\\mathbf A$ 和 $\\mathbf A^T$ 之间的关系，我们可以设有转换矩阵 $\\mathbf T$，其描述了从 $\\mathbf A$ 到 $\\mathbf A^T$ 的变换，写成公式为：$$\\mathbf T \\mathbf A = \\mathbf A^{T} \\tag 1$$同时我们假设 $\\mathbf A$ 是一个2×2的矩阵，如下所示所示（列的不同为字母的不同a和b，行的不同为小标的1和2） $$ \\mathbf A= \\begin{bmatrix} a_{1} & b_{1} \\\\ a_{2} & b_{2} \\\\ \\end{bmatrix} \\quad \\mathbf A^T= \\begin{bmatrix} a_{1} & a_{2} \\\\ b_{1} & b_{2} \\\\ \\end{bmatrix} \\tag{2} $$ 保证变换不会压缩维度，即 $det(\\mathbf A) \\neq 0$，利用（1）式，未知数，消元计算后，可以算出 $\\mathbf T$，其中 $det(\\mathbf A) = a_1b_2 - b_1a_2$ $$ \\mathbf T = \\frac{1}{det(\\mathbf A)} \\begin{bmatrix} a_{1} b_{2} - a_{2}^2 & a_{1} (a_{2} - b_{1}) \\\\ b_{2} (b_{1} - a_{2}) & a_{1} b_{2} - b_{1}^2 \\\\ \\end{bmatrix} \\tag{3} $$ 前面的 $\\frac{1}{det(\\mathbf A)}$ 作为一个常数，保证了 $det(\\mathbf A^{T})=det(\\mathbf A)$，将后面关键的变换矩阵写成 $$ \\mathbf T' = \\begin{bmatrix} a_{1} b_{2} - a_{2}^2 & a_{1} (a_{2} - b_{1}) \\\\ b_{2} (b_{1} - a_{2}) & a_{1} b_{2} - b_{1}^2 \\\\ \\end{bmatrix} \\tag {4} $$ 把 （4）式进行整理，也写成矩阵相乘的形式，得到下式 $$ \\mathbf T'=\\begin{bmatrix} \\begin{bmatrix} a_{1} & a_{2} \\\\ \\end{bmatrix} \\begin{bmatrix} b_{2} \\\\ -a_{2} \\\\ \\end{bmatrix} & \\begin{bmatrix} a_{1} & a_{2} \\\\ \\end{bmatrix} \\begin{bmatrix} -b_{1} \\\\ a_{1} \\\\ \\end{bmatrix} \\\\ \\begin{bmatrix} b_{1} & b_{2} \\\\ \\end{bmatrix} \\begin{bmatrix} b_{2} \\\\ -a_{2} \\\\ \\end{bmatrix} & \\begin{bmatrix} b_{1} & b_{2} \\\\ \\end{bmatrix} \\begin{bmatrix} -b_{1} \\\\ a_{1} \\\\ \\end{bmatrix} \\\\ \\end{bmatrix} \\tag{5} $$ 对于 $\\mathbf A$ 的列空间来说，有 $\\mathbf a = \\left [ \\begin{smallmatrix} a_1 \\\\ a_2 \\end{smallmatrix} \\right ]$ 和 $\\mathbf b = \\left [ \\begin{smallmatrix} b_1 \\\\ b_2 \\end{smallmatrix} \\right ]$ 观察到 $\\mathbf T$ 中包含列空间的两个项，把 $\\mathbf T’$ 整理得 $$ B'=\\begin{bmatrix} {\\mathbf a}^{T} \\begin{bmatrix} b_{2} \\\\ -a_{2} \\\\ \\end{bmatrix} & {\\mathbf a}^{T} \\begin{bmatrix} -b_{1} \\\\ a_{1} \\\\ \\end{bmatrix} \\\\ {\\mathbf b}^{T} \\begin{bmatrix} b_{2} \\\\ -a_{2} \\\\ \\end{bmatrix} & {\\mathbf b}^{T} \\begin{bmatrix} -b_{1} \\\\ a_{1} \\\\ \\end{bmatrix} \\\\ \\end{bmatrix}= \\begin{bmatrix} {\\mathbf a}\\cdot \\begin{bmatrix} b_{2} \\\\ -a_{2} \\\\ \\end{bmatrix} & {\\mathbf a}\\cdot \\begin{bmatrix} -b_{1} \\\\ a_{1} \\\\ \\end{bmatrix} \\\\ {\\mathbf b}\\cdot \\begin{bmatrix} b_{2} \\\\ -a_{2} \\\\ \\end{bmatrix} & {\\mathbf b}\\cdot \\begin{bmatrix} -b_{1} \\\\ a_{1} \\\\ \\end{bmatrix} \\\\ \\end{bmatrix} \\tag{6} $$ 令 $\\mathbf c = \\begin{bmatrix}b_{2} \\\\ -a_{2} \\\\ \\end{bmatrix} $ $\\mathbf d = \\begin{bmatrix}-b_{1} \\\\ a_{1} \\\\ \\end{bmatrix} $ ，观察后发现规律： $\\mathbf c$ 变换到 $\\mathbf A^T_{\\hat {\\jmath}}$ 为逆时针旋转90° $\\mathbf d$ 变换到 $\\mathbf A^T_{\\hat {\\imath}}$ 为顺时针旋转90° $\\mathbf c$ 和 $\\mathbf d$ 组成的列空间设为 $\\mathbf C$，写成公式为 $$ \\mathbf C = \\begin{bmatrix}\\mathbf c & \\mathbf d\\end{bmatrix} = \\begin{bmatrix} b_2 & -b_1 \\\\ -a_2 & a_1\\end{bmatrix} \\tag{7} $$ 将（7）式左乘 $\\mathbf A$ 得到下式 $$ \\mathbf A\\mathbf C= \\begin{bmatrix} a_{1} & b_{1} \\\\ a_{2} & b_{2} \\\\ \\end{bmatrix} \\cdot \\begin{bmatrix} b_{2} & -b_{1} \\\\ -a_{2} & a_{1} \\\\ \\end{bmatrix} = \\begin{bmatrix} det(\\mathbf A) & 0 \\\\ 0 & det(\\mathbf A) \\\\ \\end{bmatrix} =det(\\mathbf A) \\mathbf I \\tag{8} $$ 再将（8）式所有乘以 $\\mathbf A$ 的逆矩阵 $\\mathbf A^{-1}$ 得到$$\\mathbf C = \\mathbf A^{-1} det(\\mathbf A) \\tag{9}$$所以，构造的这个 $\\mathbf C$ 矩阵和 $\\mathbf A$ 矩阵的逆矩阵有关 另外，（4）式，即 $\\mathbf T’$ 矩阵还可以被写成 $$ \\mathbf T'=\\begin{bmatrix} \\begin{bmatrix} \\begin{bmatrix} a_{1} & b_{1} \\\\ a_{2} & b_{2} \\\\ \\end{bmatrix} & \\begin{bmatrix} b_{2} \\\\ -a_{2} \\\\ \\end{bmatrix} \\end{bmatrix} \\\\ \\begin{bmatrix} \\begin{bmatrix} a_{1} & b_{1} \\\\ a_{2} & b_{2} \\\\ \\end{bmatrix} & \\begin{bmatrix} -b_{1} \\\\ a_{1} \\\\ \\end{bmatrix} \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} \\begin{bmatrix} \\begin{bmatrix} a_{1} & b_{1} \\\\ a_{2} & b_{2} \\\\ \\end{bmatrix} & \\mathbf c \\end{bmatrix} \\\\ \\begin{bmatrix} \\begin{bmatrix} a_{1} & b_{1} \\\\ a_{2} & b_{2} \\\\ \\end{bmatrix} & \\mathbf d \\end{bmatrix} \\end{bmatrix} \\tag{10} $$ 或者可以写成 $$ \\begin{align} \\mathbf T' & = \\begin{bmatrix} \\begin{bmatrix} a_{1} & a_{2} \\\\ \\end{bmatrix} & \\begin{bmatrix} b_{2} & -b_{1} \\\\ -a_{2} & a_{1} \\\\ \\end{bmatrix} \\\\ \\begin{bmatrix} a_{2} & b_{2} \\\\ \\end{bmatrix} & \\begin{bmatrix} b_{2} & -b_{1} \\\\ -a_{2} & a_{1} \\\\ \\end{bmatrix} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\mathbf a^{T} & \\begin{bmatrix} \\mathbf c & \\mathbf d \\\\ \\end{bmatrix} \\\\ \\mathbf b^{T} & \\begin{bmatrix} \\mathbf c & \\mathbf d \\\\ \\end{bmatrix} \\\\ \\end{bmatrix} \\\\ & = \\begin{bmatrix} \\mathbf c^{T}\\mathbf C \\\\ \\mathbf d^{T}\\mathbf C \\\\ \\end{bmatrix} = det(\\mathbf A) \\begin{bmatrix} \\mathbf a^{T}\\mathbf A^{-1} \\\\ \\mathbf b^{T}\\mathbf A^{-1} \\\\ \\end{bmatrix} \\end{align} \\tag{11} $$ 由 $\\mathbf T \\mathbf A = \\mathbf A^T$ 可得 $$ \\mathbf T= \\begin{bmatrix} \\mathbf a^T \\mathbf A^{-1} \\\\ \\mathbf b^T \\mathbf A^{-1} \\end{bmatrix} \\tag{12} $$ 这时候，可以得出一个结论，矩阵的转置的过程和矩阵的逆是有关系的，是矩阵逆的一个更加复杂的表现形式 有了这个作为基础，考虑一下具有对称性，构造 $\\mathbf A \\mathbf A^T$，这个复合变换有着很好的对称性和分解性（接下来为了方便，默认非粗体表示的矩阵变换），因为 $$ AA^T = \\begin{bmatrix} a_1 & b_1 \\\\ a_2 & b_2\\end{bmatrix}\\begin{bmatrix} a_1 & a_2 \\\\ b_1 & b_2\\end{bmatrix} = \\begin{bmatrix} a_1^2 + b_1^2 & a_1a_2+b_1b_2 \\\\ a_1a_2+b_1b_2 & a_2^2 + b_2^2\\end{bmatrix} \\tag{13} $$ 考虑任何不进行压缩维度变换的矩阵都可以进行特征分解，则有 $$ AA^{T} = R_{AA^{T}} \\Lambda_{AA^{T}} (R^{-1})_{AA^{T}} \\tag{14} $$ 之后左乘（这里补充一下，所有的左乘操作在几何意义上来说，就是附加了一个新的变换）$A^{-1}$，得到 $$ A^{T} = A^{-1} R_{AA^{T}} \\Lambda_{AA^{T}} (R^{-1})_{AA^{T}} \\tag{15} $$ 根据之前（1）式的例子进行计算，发现 $R_{AA^{T}} = (R^{-1})_{AA^{T}}$，并且 $R_{AA^T}$ 首先将空间关于 y 轴对称，之后旋转 $\\alpha$ 角度，所以假设定义 $$ R_{AA^{T}}^{'} = \\begin{bmatrix} cos \\alpha & -sin \\alpha \\\\ sin \\alpha & cos \\alpha \\\\ \\end{bmatrix} \\tag{16} $$ 利用上面的推导，带入（15）式，得到： $$ A^{T}=A^{-1} R_{AA^{T}}^{'} \\begin{bmatrix} -1 & 0 \\\\ 0 & 1 \\\\ \\end{bmatrix} \\Lambda_{AA^{T}} R_{AA^{T}}^{'} \\begin{bmatrix} -1 & 0 \\\\ 0 & 1 \\\\ \\end{bmatrix} \\tag{17} $$ 用更加清晰的符号改写（17）式得到 $$ A^{T}=A^{-1} R_{\\alpha} M_y \\Lambda R_{\\alpha} M_y \\tag{18} $$ $M_y$ 表示的是关于y轴对称$R_\\alpha$ 表示逆时针旋转 $\\alpha$ 度$\\Lambda$ 表示一种伸缩变换 这里的（18式）结论和上面的作图规律，还有（12）式从某种程度上来说有相似的地方 其实应该从矩阵分解的部分来再次思索矩阵转置的意义，可详见什么是PCA,SVD 最终感觉关于转置还是研究的不够透彻，可能需要拔高到另一个层面去理解会更加直观，但是限于水平，只能至此（并不是数学专业的学生） 总结 $\\mathbf A \\mathbf A^T$ 的对称特性非常有用 正交 = 旋转 = 垂直，并且逆等于转置 正规矩阵的定义和转置息息相关，但是从形式上来看，约束条件是逐步变弱的，其实这些特征都是描述了空间一个变换的一些变化模式，比如旋转，伸缩的特殊模式，只需要有一个基本的直观理解，在机器学习领域就已经足够用了","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Linear Algebra","slug":"Linear-Algebra","permalink":"https://charlesliuyx.github.io/tags/Linear-Algebra/"}]},{"title":"程序员技能图谱","date":"2017-10-14T02:08:30.000Z","path":"2017/10/13/程序员技能图谱/","text":"【阅读时间】百科类【内容简介】原文来自StuQ的开源项目。核心目的是提纲挈领不同领域的技术栈。高层次来说，技术无止境，终身学习会越来越重要，一份由行内的资深人士总结的技术，工具和概念图谱有很强的指导作用，帮你节省时间。低层次来说，一份知识图谱也同领域聊天中装逼的神器，名词概念是有专业隔离的 记录工具使用幕布，方便一键生成思维导图，并且快速搜索查找 人工智能AI机器学习 大数据总 Hadoop Web 前端总 移动性能优化 HTML5开发 Angular 2 Server 后端架构师 OpenResty 直播技术 CDN技术 DNS排查 云计算总 容器 Container 微服务 MicroService 微服务架构和实践 智能运维总 DBA DevOps Kubernetes 安全总 测试总 移动无线测试 移动开发iOS开发 Android App 开发 Android ROM 开发 Android 架构师 智能硬件嵌入式开发 开发语言总 Golang Clojure Python Haskell Node.js Ruby Java 开发工具Git 技能清单CTO技能","tags":[{"name":"Acknowledge Graph","slug":"Acknowledge-Graph","permalink":"https://charlesliuyx.github.io/tags/Acknowledge-Graph/"},{"name":"Program","slug":"Program","permalink":"https://charlesliuyx.github.io/tags/Program/"},{"name":"IT","slug":"IT","permalink":"https://charlesliuyx.github.io/tags/IT/"}]},{"title":"【直观详解】线性代数的本质","date":"2017-10-07T05:56:56.000Z","path":"2017/10/06/【直观详解】线性代数的本质/","text":"【阅读时间】1小时左右 words 14069words【内容简介】将只停留在数值运算和公式的线性代数推进到可视化几何直观（Visual Geometric Intuition）的领悟上，致敬3B1B的系列视频的笔记，动图也都来自于视频。内容涉及到基变换，叉积，逆矩阵，点积，特征向量与特征值。每一章节都有一句经典的名言，非常有启发性 在笔记开始之前，想象学习一个事物（概念）的场景：我们需要学习正弦函数，$\\sin (x)$，非常不幸的是，你遇到了一本相当装逼的教材，它告诉你，正弦函数是这样的： $$ \\sin (x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} + \\cdots + (-1)^n\\frac{x^{2n+1}}{(2n+1)!} + \\cdots $$ 的确很厉害的样子，并且，计算器就是这样算 $\\sin (x)$，知道了这个的确“挺酷的”。对你来说，你的作业可能就是回家把 $x= \\frac{\\pi}{6}$ 带到公式里面，发现，好神奇！竟然越算越接近0.5。此时你对$\\sin (x)$与三角形之间的几何直观只有一些模糊的概念。这样的学习过程就十分悲催了。为什么呢？ 再假设一个场景，接下来，物理课，正弦函数随处可见，下图场景中，其他人能快速的大概估计出这个值是0.7。而刚“学过”正弦函数的你，内心戏可能是这样的：这些人忒diao了吧？你莫不是在玩我？ 你可能会觉得这些做物理的人脑子也太强了，我弱爆了。其实，你需要只是一个几何直观的灌输而已，这也从侧面佐证了一个好的老师（这里的好老师真的不是他本身的学术能力有多强，而在于他擅不擅长站在学习者的角度不断的修正教学方法。甚至，模拟学生的学习过程提前预知所需要的基础概念）是有多么重要。 教学不同层次的人：初学，入门，掌握，理解，都是不同的。解释的角度，方式都完全不同。更加不幸的是，为了能更加通用的用理论来描述现实生活中的规律，人类已经做了很多工作，我们常说：越通用，越抽象（越难以理解）。这对于初学者来说堪称一段噩梦 这个例子比较极端，但只为强调一件事：直观理解很重要，或者说，学习方法很重要。好的学习方法即你如何直观的去理解（可能是几何的，或是现实中的具体例子）一个抽象的事物，并层次化的建立知识与知识间的联系，构建并健壮属于自己的知识图谱。个人观点是，这种【学习方法】是最高效的。它唯一的难度在于，需要一定的基础知识打底，一定的量变结合方法论（点拨或领悟）就是质变。换句话说，想躺着学习？不存在的 根据生物学家我们知道，人对具体的事物（动画＞图形＞数字＞未建立直观理解的文字）更敏感，记忆速度更快。这篇笔记的对象3B1B团队生产的内容目的就是从为了帮助人们建立直观概念的角度来教学，在如今中国应试教育风行的大背景下，它会超越你的认知：学习如追番般期待，真不是一个调侃！ 我是极度反对现代大学的线性代数课程中（甚至数学类课程）的教学方法的，在计算上（做题）花费了大量时间。而工程中，有计算机，绝不会有任何一个人去笔算矩阵的逆或特征值。如果现在的老师反驳：做计算的目的是为了让你通过大量的联系（重复）去记牢概念，我也一直坚信：学习知识的最快捷径是带有思考的重复，但那是带思考的重复，有一些直观的方法在帮助你理解和记忆上比做题有效率的多 注解，因为这是一篇个人笔记，我个人已经深刻理解的内容，或者我觉得是很基本的内容我会略过或默认。好消息是，我自己也是一个理解力非常捉急的人，所以还是会比较详细的 什么是矩阵？矩阵 = 变换的数字表达 向量究竟是什么 引入一些数作为坐标是一种鲁莽的行为 ——赫尔曼·外尔 The introduction of numbers as coordinates is an act of violence - Hermann Weyl 这部分，讲向量，扎实的读者完全可以跳过 向量的定义 What对于向量的这个概念，大家一定并不陌生，但是这次让我们从数学，物理，计算机三个角度来看待如何定义这个【向量】这个概念 物理专业角度 向量是空间中的箭头 决定一个向量的是：它的长度和它所指的方向 计算机专业角度 向量是有序的数字列表 向量不过是“列表”一个花哨的说法 向量的维度等于“列表”的长度 数学专业角度从数学来说，它的本质就是通用和抽象，所以，数学家希望概括这两种观点 向量可以是任何东西，只需要保证：两个向量相加及数字与向量相乘是有意义的即可 向量加法和向量乘法贯穿线性代数始终，十分重要 可以通过上图直观的感受到数学家（这个很牛逼的灰色的$\\pi$）在想什么，有种【大道】的逼格。左边是物理角度，右边是计算机角度，但是很抱歉，我能用一些抽象的定义和约束让你们变成一个东西 坐标系把向量至于坐标系中，坐标正负表示方向，原点为起点，可完美把两个不同的角度融合 向量加法 物理：首尾相连 Motion 计算机：坐标相加 向量乘法 物理：缩放 Scaling 计算机：坐标和比例相乘 线性组合、张成的空间与基 数学需要的不是天赋，而是少量的自由想象，但想象太过自由又会陷入疯狂 ——安古斯·罗杰斯 Mathematics requires a small dose, not of genius, but of an imaginative freedom which, in a larger dose, would be insanity - Angus K. Rodgers 本部分继续加深一个概念，为何向量加法与向量乘法是那么重要，并从始至终贯穿整个线性代数 线性组合这个概念再好理解不过，空间中不共线的两个不为零向量都可以表示空间中的任意一个向量，写成符号语言就是：$a \\mathbf{\\vec v} + b \\mathbf{\\vec w}$ 至于为什么被称为“线性”，有一种几何直观：如果你固定其中一个标量，让另一个标量自由变化，所产生的向量终点会描出一条直线 空间的基 Basis对于我们常见的笛卡尔坐标系，有一个最直观一组基：$\\{{\\hat {\\imath}} ,{\\hat {\\jmath}} \\}$ ，即单位向量：${\\hat {\\imath}}=(1,0)$ 和$\\hat {\\jmath} =(0,1)$ ，通过 ${\\hat {\\imath}}$ 和 ${\\hat {\\jmath}}$的拉伸与相加可以组成笛卡尔坐标系中的任意一个向量 张成的空间 Span同理，举一反三的来说，我们可以选择不同的基向量，并且这些基向量构成的空间称为：张成的空间。张成二字比较拗口，可以类比为延展或扩展。直观来看，就是所有动图中的网格。笛卡尔坐标系就是一个由单位坐标$\\{{\\hat {\\imath}} ,{\\hat {\\jmath}} \\}$ 张成的空间 所有可以表示为给定向量（基）线性组合（刚刚讲了这个概念）的向量的集合，被称为给定向量（基）张成的空间 如果你继续思考一下，会发现一个特点：并不是每一组给定向量都可以张成一个空间，若这两个向量共线（2D），共面（3D），它们就只能被限制在一个直线或面中，类似于“降维打击”。通过这个直观的思考可以引出一个概念：线性相关 线性相关关于什么是线性相关，有两种表达 【表达一】你有多个向量，并且可以移除其中一个而不减小张成的空间（即2D共线或3D共面），我们称它们（这些向量）线性相关 【表达二】其中一个向量，可以表示为其他向量的线性组合，因为这个向量已经落在其他向量张成的空间之中 如果从统计学角度来说，这些向量之中有冗余。这一堆向量中，我们只需要其中几个（取决于维度）就可以表示其他所有的向量。 向量空间一组基的严格定义有了这些对名次（概念）的直观理解，来看看数学家们是如何严谨的定义向量空间的一组基： 向量空间的一组基是张成该空间的一个线性无关向量集 用这样的步骤来慢慢导出这个定义，个人感觉，远比在课堂的第一分钟就将这句让你迷惑的话丢给你好的多，抽象的东西只有在慢慢推倒中你才能发现它的精巧之处，非常优雅且迷人 矩阵与线性变换 很遗憾，Matrix（矩阵）是什么是说不清的。你必须得自己亲眼看看 ——墨菲斯 Unfortunately, no one can be told what the Matrix is. You have to see it yourself -Morpheus 矩阵，最直观的理解当然是一个写成方阵的数字 $\\left[\\begin{smallmatrix} 1&2 \\\\ 3&4 \\end{smallmatrix}\\right]$，这几节的核心是为了说明：矩阵其实就是一种向量变换（至于什么是变换下面会讲），并附带一种不用死记硬背的考虑矩阵向量乘法的方法 变换【变换】本质上是【函数】（左）的一种花哨的说法，它接受输入内容，并输出对应结果，矩阵变换（右），同理，如下图 那既然两者意思相同，为何还要新发明一个词语装逼呢？其实不然，搞学术，不严谨就会出现纰漏。常说编程出现Bug，其实就是不严谨的一种体现，在写Code前，没有考虑到可能性的全集（虽然在一些大型程序中，考虑全集的做法有时候是没必要的，这是一对关于编程困难程度和不出的bug的博弈Trade-off），但是【变换】这个名词和不严谨其实没什么关系…… 【变换】的表达方法暗示了我们可以用运动的方法来理解【向量的函数】这一概念，可以用可视化的方法来展现这组【变换】即输入-输出关系 这世界上有非常多优美的变换，如果你将他们编程，并可视化，就能得到下图 线性变换我们说具有以下两个性质的就是线性变换（直观可视化如下图）： 直线在变换后仍然保持为直线，不能有所弯曲 原点必须保持固定 一点扩展，如果保持保持直线但原点改变就称为：仿射变换（Affine Transformation） 一句话总结来说是：线性变换是“保持网格线平行且等距分布”的变换 如何用数值描述线性变换这里需要使用上一节提到的工具，空间的基，也就是单位向量（基向量）：${\\hat {\\imath}}=(1,0)$ 和$\\hat {\\jmath} =(0,1)$ 你只需要关注两个基向量 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 变换后的位置即可。例如， $\\hat {\\imath}$ 变换到 $(3,1)$ 的位置， $\\hat {\\jmath}$ 变换到$(1,2)$ 的位置，并把 $\\hat {\\imath}$ 变换后坐标立起来作为方阵的第一列（绿色表示）， $\\hat {\\jmath}$ 变换后的坐标立起来作为方阵的第二列（红色表示）。 构成了一个矩阵：$\\begin{bmatrix} \\color{green}3&\\color{red}1 \\\\ \\color{green}1&\\color{red}2 \\end{bmatrix}$，假设我们想要知道目标向量$(-1,2)$进行变换后的位置，那么这个矩阵就是对变换过程最好的描述，一动图胜千言 Step1：绿色 $\\hat {\\imath}$ （x轴）进行移动（变换）Step2：红色 $\\hat {\\jmath}$ （y轴）进行移动（变换）Step3：目标向量x轴坐标值与 $\\hat {\\imath}$ 变换后向量进行向量乘法Step4：目标向量y轴坐标值与 $\\hat {\\jmath}$ 变换后向量进行向量乘法Step5：两者进行向量加法，得到线性变换结果 更加一般的情况，我们用变量来代替其中的具体值：绿色代表 $\\hat {\\imath}$ 变换后的向量，红色代表 $\\hat {\\jmath}$ 变换后的向量 $$ \\begin{bmatrix} \\color{green}{a}&\\color{red}b \\\\ \\color{green}c&\\color{red}d \\end{bmatrix} \\begin{bmatrix}x\\\\y\\end{bmatrix} = \\underbrace{x \\begin{bmatrix}\\color{green}a\\\\\\color{green}c \\end{bmatrix} + y \\begin{bmatrix} \\color{red}b\\\\\\color{red}d\\end{bmatrix}}_{\\text{直观的部分这里}} = \\begin{bmatrix} \\color{green}{a}\\color{black}{x}+\\color{red}{b}\\color{black}{y}\\\\\\color{green}{c}\\color{black}{x}+\\color{red}{d}\\color{black}{y}\\end{bmatrix} $$ 上面的公式就是我们常说的矩阵乘法公式，现在，不要强行背诵，结合可视化的直观动图，你一辈子都不会忘记的 【线性】的严格定义在给出一个数学化抽象的解释前，先做一下总结： 【线性变换】是操纵空间的一种手段，它保持网格线平行且等距分布，并保持原点不动 【矩阵】是描述这种变换的一组数字，或者说一种描述线性变换的语言 在数学上，【线性】的严格定义如下述公式，这些性质，会在之后进行讨论，也可以在这里就进行一些思考，为什么说向量加法和向量乘法贯穿线性代数始终，毕竟是线性代数，很重要的名次就是线性二字 $$ L(\\mathbf {\\vec v} + \\mathbf {\\vec w}) = L(\\mathbf{\\vec v}) +L(\\mathbf{\\vec w}) \\qquad “可加性” \\\\ L(c \\mathbf{\\vec v}) = c L(\\mathbf{\\vec v}) \\qquad “成比例” $$ 矩阵乘法与线性变换复合 据我的经验，如果丢掉矩阵的话，那些涉及矩阵的证明可以缩短一半 ——埃米尔·阿廷 It is my experience that proofs involving matrices can be shortened by 50% if one throws the matrices out -Emil Artin 复合变换如果对一个向量先进行一次旋转变换，再进行一次剪切变换（ $\\hat {\\imath}$ 保持不变第一列为（1,0）， $\\hat {\\jmath}$ 移动到坐标（1,1）） ，如下图所示 那么如果通过旋转矩阵和剪切矩阵来求得这个符合矩阵呢？为了解决这个问题，我们定义这个过程叫做矩阵的乘法 矩阵乘法在这里我们发现，矩阵乘法的变换顺序是从右往左读的（这一个常识很重要，你得明白这一点，有基本概念），进一步联系和思考发现，和复合函数的形式，如 $f(g(x))$ ，是一致的 那么如何求解矩阵乘法呢？对线性代数有印象的同学你们现在能马上记起来那个稍显复杂的公式吗？如果有些忘记了，那么，现在，就有一个一辈子也忘不了的直观解释方法： M1矩阵的第一列表示的是 $\\hat {\\imath}$ 变换的位置，先把它拿出来，M2矩阵看成对这个变换过的 $\\hat {\\imath}$ 进行一次变换（按照同样的规则），就如上图所示。同理，针对 $\\hat {\\jmath}$ 一样的操作过程，就可以得出这个表达式。这里我也不把它写出来了，按照这种思路，并且把上面的动图多看几遍，如果还能忘记，那就要去补一补基本的对几何图形的反应能力了（这也是一种能力，包括三维想象力，心算能力，都和记忆或肌肉一样，不锻炼，是不可能躺着被提高的） 计算规则证明有了上面的想法，可以自己尝试证明一下，矩阵乘法的交换律是否成立？矩阵乘法的结合律呢？你会发现，原来这么直观，根本不需要动笔算 三维空间对三维空间内扩展的话，你会发现，显示生活中的每一种形态改变都能用一个3*3的矩阵来表示这个变换，这在机器人，自动化操作领域是非常重要的，因为你可以把现实生活很难描述的动作通过一个矩阵来表示，是一个连接数字和现实的重要桥梁和工具 行列式 计算的目的不在于数字本身，而在于洞察其背后的意义 ——理查德·汉明（没错，是发明汉明码的那个人） The purpose of computation is insight, not numbers - Richard Hamming 行列式定义的由来我们注意到，有一些变换在结果上拉伸了整个网格，有一些则是压缩了，那如何度量这种压缩和拉伸呢？或者换一种更容易思考的表达，某一块面积的缩放比例是多少？ 其实，根据我们之前讲的基向量，我们只需要知道 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 组成的面积为1的正方形面积缩放了多少就代表所有的情况。因为线性变换有一个性质：网格线保持平行且等距分布 所以，这个特殊的缩放比例，即线性变换对面积产生改变的比例，就是行列式 特别的，我们可以发现，如果一个矩阵的行列式为0，意味着它把这个空间降维了，并且矩阵的列线性相关 其中，正负表达的是方向，类似于纸的翻面。从数学来说，$\\hat {\\jmath}$ 起始状态在 $\\hat {\\imath}$ 的左侧，如果经过变换，变为在右侧，就添加负号。三维情况下，右手定位为正 计算行列式为了连接行列式的计算公式和几何直观，我们现考虑 $\\begin{bmatrix} \\color{green}a&\\color{red}b \\\\ \\color{green}c&\\color{red}d \\end{bmatrix}$ 其中的b c 为0，那么，a表示 $\\hat {\\imath}$ 在x轴缩放比例，d表示 $\\hat {\\jmath}$ 在y轴缩放比例，ad表示拉伸倍数，同理来说，bc表示的就是压缩倍数，两者的和就是缩放比例。如果你还是对公式念念不忘，那么下面这张图可能可以帮到你 行列式直观理解的好处在这里，可以思考一下如何证明 $det(M_1M_2) = det(M_1)det(M_2)$ ，你会发现太简单不过了 逆矩阵、列空间与零空间 提出正确的问题比回答它更难 ——格奥尔格·康托尔 To ask the right question is harder than to answer it - Georg Cantor 首先，这一节并不涉及计算的方法，相关名次有：高斯消元法 Gaussian elimination、行阶梯形 Row echelon form。这里着眼的是对抽象的概念建立一个几何直观的理解，计算的任务就交给计算机去做 矩阵的用途 描述对空间的操作，3*3矩阵描述的三维变换 可以帮助我们解线性方程组（Linear System of equation） 线性方程组 上图就是一个整理好的线性方程组，一般形式 $\\mathbf A \\mathbf{\\vec x} = \\mathbf{\\vec v}$ ，其中 $\\mathbf{\\vec x}$ 是待求向量。使用之前的几何直观来翻译个公式即，$\\mathbf{\\vec x}$ 经过 $\\mathbf A$ 矩阵变换后，恰好落在 $\\mathbf{\\vec v}$ 上，如下图 既然使用了 $\\mathbf A$ 这个矩阵变换，那么之前讲解的概念：行列式应用在这里就很有意思了。根据之前提到的，行列式直观来说就是矩阵变换操作面积的缩放比例。我们可以思考，$det(\\mathbf A) = 0$ 意味着缩放比例为0，即降维了。很大可能找不到解，唯一的可能性，比如平面压缩成直线，这个直线恰好落在 $\\mathbf{\\vec v}$ 上才有解。这也是为什么计算行列式的值可以判断方程是否有解的几何直观 接下来思考如何求 $\\mathbf{\\vec x}$ 。逆向思考，从 $\\mathbf{\\vec v}$ 出发，进行某一个矩阵变换，恰好得到 $\\mathbf{\\vec x}$ 。而这个反过来的矩阵变换，就称为 $\\mathbf A$ 矩阵的逆矩阵，写成公式是：$$\\mathbf A^{-1}\\mathbf A\\mathbf{\\vec x} =\\mathbf A^{-1} \\mathbf{\\vec v} \\implies \\mathbf{\\vec x} =\\mathbf A^{-1} \\mathbf{\\vec v}$$ 逆矩阵所谓逆，就是反过来的意思。根据基向量代表整个空间，已经变换过的 $\\hat {\\imath}’$ 和 $\\hat {\\jmath}’$ 如何通过一个矩阵变换，变回 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ ，这个矩阵就是逆矩阵 ，写作 $\\mathbf A^{-1}$，直观理解如下图 逆矩阵乘原矩阵等于恒等变换，写作 $\\mathbf A \\mathbf A^{-1} = \\mathbf I$ 。$\\mathbf I$ 矩阵表示基向量，对角线元素为1，其余为0（矩阵说对角线，默认为左上方到右下方） 列空间其实这只是之前一直在提到过的概念，在线性方程组中，这么描述：所有可能得输出向量 $\\mathbf A \\mathbf{\\vec v}$ 构成的集合被称为A的列空间。这么说不太好理解，可以从名称“列空间”入手，矩阵的列是什么呢？我们之前已经多次强调了，就是 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 变换后的坐标。即矩阵的列表示基向量变换后的坐标（位置），变换后的向量张成的空间就是所有可能得输出向量 简单说即：列张成的空间 = 列空间，即矩阵的列所张成的空间，如下图。 检查一下自己是否完全理解，就思考下面句话为什么这么说：零向量一定在列空间中（列空间很好理解） 秩 Rank秩这个概念相信很多学习线性代数的同学，因为中文字，秩，本身就不熟（和正则化有点类似），所以秩也就非常难以理解了。秩是秩序，联想为秩序的程度。但是，因为你已经看了这个教程，矩阵的秩在现在你拥有的几何直观下，理解起来，当真的小菜一碟 我们已经建立了一种深刻的认知：矩阵 = 变换，那么变换后空间的维度，就是这个矩阵的秩。更加精确的定义是：列空间的维数（如果你可以焕然大：原来这两句话是一个意思，那么我觉得你对矩阵的理解已经有了质的提高） 零空间变换后落在原点的向量的集合，称为这个矩阵（再次强调矩阵 = 变换的数字表达）的零空间或核，如果感觉没理解，可以看看下图 【图1】二维压缩到一个直线（一维），有一条直线（一维）的点被压缩到原点【图2】三维压缩到一个面（二维），有一条直线（一维）的点被压缩到原点【图3】三维压缩到一条线（一维），有一条直线（二维）的点被压缩到原点 【注意】压缩就是变换，变换就是矩阵，其实说的就是矩阵 满秩 = 列空间 + 零空间 总结 从几何角度理解线性方程组的一个高水平概述 每个方程组都有一个线性变换与之联系，当逆变换存在时，你就能用这个逆变换求解方程组 不存在逆变换时，列空间的概念让我们清楚什么时候存在解 零空间的概念有助于我们理解所有可能得解的集合是什么样的 非方阵 在这个小测试里，我让你们求一个2*3矩阵的行列式。让我感到非常可笑的是，你们当中竟然有人尝试去做 ——佚名 On this quiz, I asked you to find the determinant of a 2*3 matrix. Some of you, to my great amusement, actually tried to do this - no name listed 几何意义首先从一个特例出发，考虑3×2（3行2列）矩阵的几何意义，从列空间我们得知，第一列表示的是 $\\hat {\\imath}$ 变换后的位置（现在是一个有三个坐标的值，即三维），第二列同理是 $\\hat {\\jmath}$ 。总结来说，3×2矩阵的几何意义是将二维空间映射到三维空间上 此时从特例到一般化推倒，我们可以得到一个结论：n*m 的几何意义是将m维空间（输入空间）映射到n维空间（输出空间）上 注意这里的输入空间，输出空间的概念，阅读方向同样也是从右向左的（靠右的是输入，靠左的是输出） 非方阵乘法如果你已经学过线性代数的大学课程，你可能有一些影响，并不是任意两个非方阵都可以进行矩阵乘法，必须满足一些条件，例如，$\\mathbf M_1 \\mathbf M_2$（非方阵）计算中，假设 $\\mathbf M_2$ 为2×3的矩阵，那么 $\\mathbf M_1$的列必须等于 $\\mathbf M_2$ 的行，否则这个乘法是没法计算的。 当我们有了变换的几何直观后，这个概念只要自己思考推倒一次，也是一辈子都忘不了的 直观解释是：矩阵的行是这个变换的输出空间维数，而列是变换的输入空间维数。矩阵乘法从右向左读，第一个变换的 $\\mathbf M_2$ 的输出向量的维度（ $\\mathbf M_2$ 的行）必须和第二个变换 $\\mathbf M_1$ 的输入向量（ $\\mathbf M_1$ 的列）维度相等，才可以计算。也就是说，类似于插头和插座的关系，我只有三头插座，你来一个双头插头肯定没法用的 非方阵行列式这里有一个很好玩的概念，非方阵的行列式呢？都不是一个维度的变换，如同归零者和咱们谈判一样，你和我谈缩放比例？不存在的 点积与对偶性 卡尔文：你知道吗，我觉数学不是一门科学，而是一种宗教霍布斯：一种宗教？卡尔文：是啊。这些公式就像奇迹一般。你取出两个数，把它们相加时，它们神奇地成为了一个全新的数！没人能说清这到底是怎么发生的。你要么完全相信，要么完全不信 什么是点积个相同维数的向量，或是两个相同长度的数组。求它们的点积，就是将相应坐标配对，求出每一对坐标的乘积，然后将结果相加，一动图胜千言 几何直观来说，$\\mathbf{\\vec v} \\cdot \\mathbf{\\vec w}$ 可以想象成向量 $\\mathbf{\\vec w}$ 朝着过原点和向量 $\\mathbf{\\vec v}$ 的直线上的正交（垂直）投影，然后把投影的长度和向量 $\\mathbf{\\vec v}$ 的长度乘起来就是点击的值。其中正负号代表方向，两个向量成锐角，大于0；钝角，小于0 点积的顺序你可能会发现，顺序在线性代数其实是很重要的，而对于 $\\mathbf{\\vec v} \\cdot \\mathbf{\\vec w}$ 和 $\\mathbf{\\vec w} \\cdot \\mathbf{\\vec v}$ 它们的结果是相同，为什么呢？ 解释的方法为：首先假设 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 长度相同，利用对称轴，两个向量互相的投影相等；接下来如果你缩放其中一个到原来的两倍，对称性被破坏，但是缩放比例没变，最终乘法的结果也没变，一动图胜千言 点积与投影这个时候问题就来了，这种直观的乘法与加法的组合运算：点积为何和投影长度的乘积有关？这个问题非常有意思，因为回答这个问题的过程用到了十分精彩的直觉和思维方式 首先，需要建立多维空间到一维空间的线性变换（描述为1×n的矩阵，列代表对应的基向量压缩到一维空间的位置），即函数（自变量对应多维空间，$f(x)$ 最后的输出为一维空间，也就是数轴上的点，一个确定的数）的概念。一动图胜千言 你会发现，n×1 表示的是坐标和1×n表示的多维到一维的变换（矩阵）之间有某种联系，即将向量转化为数的线性变换和这个向量本身有着某种关系 接下来，我们想象一个情景，这个被压缩成的一条线（数轴）放置在一个坐标系（二维空间）中，且空间所有向量都经过一个变换被压缩到这个数轴上。记这个数轴的单位向量为 $\\mathbf{\\vec u}$，一动图胜千言 再然后，我们需要考虑的问题变为，坐标系中的 $\\hat {\\imath}$ 与 $\\hat {\\jmath}$ 是如何被压缩到这条直线上的呢（基向量表征整个空间的变换）？即求一个1×2的矩阵内的值，第一列表示 $\\hat {\\imath}$ 变换后的位置（在这条数轴上），第二列表示 $\\hat {\\jmath}$ 变换后的位置。可以直接给出结论，这个变换的数值恰好就是 $\\mathbf{\\vec u}$ 在这个坐标系中的坐标 $(u_x,u_y)$ ，推倒方法使用到了对称性，一动图胜千言 动图中的白色虚线就是对称轴，目的就是确定变换后 $\\hat {\\imath}$ 与 $\\hat {\\jmath}$ 的位置，即描述变换的矩阵（再次重复，列表示坐标，行表示变换）。 推倒完毕，把这个过程总结成一个动图 矩阵的向量乘积和点积的计算公式一样，且恰好由压缩这一变换理念，与投影正好联系了起来。关键点，在于压缩变换 = 投影 对偶性在证明的过程中，有一个很关键的点就是使用了对称轴（对称理念）。在数学中，对偶性定义为：两种数学事物之间自然而又出乎意料的对应关系。刚刚推倒的内容是数学上“对偶性”的一个实例，即无论何时你看到一个二维到一维的变换，空间中会存在为一个向量 $\\mathbf{\\vec v}$ 与之相关 总结 点积是理解投影的有力几何工具 方便检验两个向量的指向是否相同 更进一步，两个向量点乘，就是将其中一个向量转化为线性变换 向量放佛是一个特定变换的概念性记号。对一般人类来说，想象空间中的向量比想象这个空间移动到数轴上更加容易 叉积 每一个维度都很特别 ——杰弗里·拉加里亚斯 从他（格罗滕迪克）和他的作为中，我还学到了一点：不以高难度的证明为傲，因为难度高意味着我们还不理解。理想的情况是能够绘出一幅美景，而其中的证明显而易见 二维情况下的叉积类比$\\mathbf{\\vec v}$ 与 $\\mathbf{\\vec w}$ 张成的平行四边形的面积，即 $\\mathbf{\\vec v} \\times \\mathbf{\\vec w}$ ，结果方向的确定考虑 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 的相对位置关系，与其相同，为正；否则，为负 通过这个定义，结合几何直观，我们可以发现几个有趣的结论 越接近垂直的 $\\mathbf{\\vec v}$ 与 $\\mathbf{\\vec w}$ 构成的面积越大 并且叉积的分配律成立 真正的叉积定义真正的叉积是在三维情况下被定义出来的：通过两个三维向量（$\\mathbf{\\vec v}$ 与 $\\mathbf{\\vec w}$ ）产生一个新的三维向量 $\\mathbf{\\vec p}$ ，向量 $\\mathbf{\\vec p}$ 的长度就是 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 组成平行四边形的面积 ，向量的方向与平行四边形（所在平面）垂直，并用右手定则确定方向，食指为 $\\mathbf{\\vec v}$ ，中指为 $\\mathbf{\\vec w}$ ，大拇指即 $\\mathbf{\\vec p}$ 叉积计算公式 其中 $\\hat {\\imath}$ $\\hat {\\jmath}$ $\\hat k$ 三个基向量后的数字就是对应向量 $\\mathbf{\\vec p}$ 的坐标值 第一次学这个计算方法的时候，估计没几个人能想清楚它为什么是这样的形式，甚至老师也说不清，只是告诉学生，我们这么记下来，定义是这样的定义的。但是，既然是直观讲解，必须把这里的来由探明清楚 叉积计算的几何直观在开始前，先再次加深一次对偶性的概念：每当你看到一个（多维）空间到数轴的线性变换时，它都与那个空间中的唯一一个向量对应。即应用线性变换到某个向量和与这个向量点乘等价 恰好，叉积的运算过程给出了对偶性的一个绝佳的实例：根据 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 定义一个从三维空间到数轴的特定线性变换，找到这个变换的对偶向量，这个对偶向量就是 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 的叉积 首先，我们知道三维情况的，求一个3×3矩阵的行列式，就是求这三个向量张成的平行六面体的体积，然后，把第一列（向量）换成一个自变量，后两列（两向量）记为 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ ，那么我们就有 这样形式的函数 $f()$ ，如右图所示，即平行六面体随白色向量 $(x,y,z)$ 的随机游走而不断改变。然后，问题就变成了，我们需要根据 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 找到一个变换（一个矩阵，或者说函数），使得上述等式成立。 并且因为 $f()$ 是线性的，可以利用对偶性，一动图胜千言 对偶性：即应用线性变换到某个向量和与这个向量点乘等价，即我们可以把1×3的变换（矩阵用来描述变换），立起来（转置），并写成点乘的形式。并把这个向量记为 $\\mathbf{\\vec p}$ 其中向量的颜色左右对应，并且行列式的值就是右图中平行四面体的体积，然后，我们就把问题进一步变成了：寻找向量 $\\mathbf{\\vec p}$ 使得上述等式成立 根据点积的性质得知，当你把一个向量与其他向量点积的几何解释是，把其他向量投影到 $\\mathbf{\\vec p}$ 上，然后将投影长度与 $\\mathbf{\\vec p}$ 的长度相乘。而我们知道，对于一个平行六面体来说，体积等于底面积乘以高，高于底面积垂直，所以，作为背投影对象的 $\\mathbf{\\vec p}$ 必须和 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 构成的平面垂直，方面已经找到 至于长度，可以看到，一个向量与其他向量点积的几何解释是，把其他向量投影到 $\\mathbf{\\vec p}$ 上，然后将投影长度与 $\\mathbf{\\vec p}$ 的长度相乘，其中投影长度就是 $(x,y,z)$ 向量的长度，根据公式的形式，可以观察得，向量 $\\mathbf{\\vec p}$ 的长度作为第二项，只有当长度等于平行四面体面积时，上述公式（图片中的点积=行列式值的公式）才能成立。 至此，又一次利用对偶性发现了一些事物之间自然而又出乎意料的对应关系。通过几何直观来了解计算公式的由来，也是一种加深印象，深刻理解的有效途径 总结在这里总结一下涉及到的过程，也可以通过阅读看看是否直观的理解每句话来判断掌握程度 首先定义了一个三维空间到数轴的线性变换（函数 $f()$ ），它是根据向量 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 来定义的 接着通过两种不同的方式来考虑这个变换的对偶向量 这种计算方法引导你在第一列中插入 $\\hat {\\imath}$ $\\hat {\\jmath}$ $\\hat k$ ，然后计算行列式 在几何直观上，这个对偶向量一定与 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 垂直，并且其长度与这两个向量张成的平行四边形的面积相同 基变换 数学是一门赋予不同事物相同名称的艺术 ——昂利·庞加莱 Mathematics is the art of givinh the same name to different things -Henri Poincare 坐标系与基向量坐标系指：发生在向量与一组数之间的任意转化，如果假设有一个向量，使用 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 来描述是 $[\\begin{smallmatrix} 3 \\\\ 2 \\end{smallmatrix}]$ ，我们把这种描述称为：我们的语言。如果有另一组基向量，$\\hat {\\imath}’ = [\\begin{smallmatrix} 2 \\\\ 1 \\end{smallmatrix}] $ 和 $\\hat {\\jmath}’ = [\\begin{smallmatrix} -1 \\\\ 1 \\end{smallmatrix}]$ （写成列向量的形式是为了形式上的统一）来描述同样一个向量变成 $[\\begin{smallmatrix} \\frac{5}{3} \\\\ \\frac{1}{3} \\end{smallmatrix}]$ ，我们把这种语言记为：詹妮弗的语言 基变换我们在之前的解释中已经说明了，在不同的【语言】之间的转化使用矩阵向量乘法，在上面的例子中，转移矩阵是 $\\mathbf T = [\\begin{smallmatrix} 2 &amp; -1 \\\\ 1 &amp; 1 \\end{smallmatrix}]$ ，矩阵的列表示用我们的语言表达詹妮弗的基向量，称为基变换。 反过来，就是求转移矩阵的逆 $\\mathbf T^{-1}$ ，称为基变换矩阵的逆，作用是可以表示从詹妮弗的基向量转换回我们的语言需要做的变换。 如何转化一个矩阵接下来使用一个具体的例子：变换左旋转90°，在我们的语言中，和詹妮弗的语言分别是如何互相转换的来加深印象 左乘基变换矩阵（矩阵的列代表的是用我们的语言描述詹妮弗语言的基向量）：需要被转换的詹妮弗的语言：$[\\begin{smallmatrix} -1 \\\\ 2 \\end{smallmatrix}]$ ➜ 使用我们的语言描述来描述同一个向量 左乘线性变换矩阵（表示的变化为：左旋转90°）：➜变换的后的向量（还是以我们的语言来描述） 左乘基变换矩阵的逆：➜变换后的向量（用詹妮弗的语言来描述） 这三个矩阵合起来就是用詹妮弗语言描述的一个线性变换 总结表达式 $\\mathbf A^{-1} \\mathbf M \\mathbf A$ 暗示着一种数学上的转移作用 中间的 $\\mathbf M$ 代表一种你所见的转换（例子中的90°旋转变换） 两侧的矩阵 $\\mathbf A$ 代表着转移作用（不同坐标系间的基向量转换），即就是视角上的转换 矩阵乘积仍然表示着同一个变换，只不过从其他人的角度来看 这给了很多域变换的应用一个直观的理解，把这简单的几行记录清晰， 特征向量与特征值在这一部分中，你会发现，前面提到的所有几何直观：线性变换，行列式，线性方程组，基变换会穿插其中。不仅给了你一个机会检验之前的理解是否深刻（在这一节，会添加一些超链接，方便你进行复习和定位），更多的，现在，是拼装起来感受成就感的时刻了！ What首先，我们假设坐标系的一个基变换（对 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 张成的空间做一个线性变换 ），即 $\\hat {\\imath}’ = [\\begin{smallmatrix} 3\\\\ 0 \\end{smallmatrix}] $ 和 $\\hat {\\jmath}’ = [\\begin{smallmatrix} 1 \\\\ 2 \\end{smallmatrix}]$ 。在变换的过程中，空间内大部分的向量都离开了它所张成的空间（即这个向量原点到终点构成的直线） ，还有一部分向量留在了它所张成的空间，矩阵对它仅仅是拉伸或者压缩而已，如同一个标量。 如上图，是给出例子中，x轴所有向量被伸长为原来的3倍，一个明显留在张成空间内的例子。另一个比较隐藏的，是$(-1,1)$这个向量，其中的任意一个向量被伸长为原来的2倍 变换中被留在张成空间内的向量，就是特征向量（上例x轴和$(-1,1)$） 其中每个向量被拉伸或抽缩的比例因子，就是特征值（上例3和2） 正负表示变换的过程中是否切翻转了方向 Why三维情况，如果能找到这个不变的向量，即旋转轴（特征值必须为1） 理解线性变换的作用的关键（或者说更好的描述一个变换），更好的方法是求出它的特征向量和特征值 How从计算角度来看特征值和特征向量，里面包含了很多对以前只是回顾和整合 根据特征向量和特征值的定义，使用数学的方法来表示即$$\\mathbf A \\mathbf{\\vec v} = \\lambda \\mathbf{\\vec v}$$ $\\mathbf A$ 是求特征值和特征向量的变换矩阵；$\\mathbf{\\vec v}$ 是特征向量；$\\lambda$ 是特征值；目标是找 $\\mathbf{\\vec v}$ 和 $\\lambda$ 至于为何会用这个式子来定义特征向量和特征值呢，我们继续观察这个式子中的 $\\lambda \\mathbf{\\vec v}$ ，考虑到右边是一个矩阵乘法，我们希望左右都是一个矩阵乘法，这样方便等价和计算。观察发现，$\\lambda \\mathbf{\\vec v}$ 就是给 $\\mathbf{\\vec v}$ 中每一个元素都乘以 $\\lambda$ 。对角矩阵 $\\mathbf I$ 且对角线元素为 $\\lambda$ 的矩阵也能有同样的变换结果，得到下列表达式$$\\mathbf A \\mathbf{\\vec v} = (\\lambda \\mathbf I ) \\mathbf{\\vec v} \\implies (\\mathbf A - \\lambda \\mathbf I ) \\mathbf{\\vec v} = 0$$观察这个等式你会发现：可以把 $\\mathbf A - \\lambda \\mathbf I$ 矩阵看成一个对 $\\mathbf{\\vec v}$ 矩阵的变换，目的是把 $\\mathbf{\\vec v}$ 压缩到更低的维度。而空间压缩对应的恰好就是变换矩阵的行列式为0（期待你在品读这句话的时候感受到满满的成就感，实在有难度，再结合下图） 上图显示随 $\\lambda$ 可视化的变化情况，从这幅图中，使用的例子是 $[\\begin{smallmatrix} 2 &amp; 2 \\\\ 1 &amp; 3 \\end{smallmatrix}]$ ，特征值恰好是1 特征向量的特殊情况旋转变换解出特征值能发现答案是 $\\pm i$ ，没有特征向量存在，即特征值出现复数的情况一般对应于变换中的某种旋转 剪切变换Shear变换。x轴不变，只有一个特征值，为1（$(\\lambda-1)^2=0$） 伸缩变换特征值只有一个，但是是空间中所有的向量都是特征向量 特征基对角矩阵：只有对角线非零的矩阵。解读它的方法是：所有的基向量都是特征向量。因为之前提到过，矩阵的第一列是 $\\hat {\\imath}$ ，第二列是 $\\hat {\\jmath}$ ，往后同理。这样就能发现，如果一列只有对应的位置非零，那么这个坐标轴本身就就是特征向量 一组基向量（同样是特征向量）构成的集合被称为一组：特征基 对角矩阵有一个好处是计算方便，多次矩阵乘法非常容易 这时我们就希望利用对角矩阵（基向量为特征向量）的便于计算的特性，利用上一节提到的基向量变换的方法，把特征向量作为基，对每一个矩阵进行变换后再进行计算，最后再左乘变换矩阵的逆求回原矩阵得到结果，如下图所示 但需要说明的是，并不是所有的矩阵都能对角化，比如Shear变换，它的特征向量不够多，不足以张成一个空间 抽象向量空间线性代数的一切概念，如行列式和特征向量，它们并不受所选坐标系的影响，但是这两者是暗含于空间中的性质 这里所说的空间是什么意思呢？ 函数与向量从某种意义上来说，函数实际上也只是另一种向量，对于函数来说，也有可加性，可比性 $$ (f+g)(x) = f(x)+g(x)\\\\(2f)(x) = 2f(x) $$ 你能发现，这两个性质和向量加法与向量乘法是息息相关的。所以我们对于矩阵中所有定义的概念和方法，都可以相对应的应用到函数中。如函数的线性变换：函数接受一个函数，并把它变成另一个函数。如微积分中可以找到一个形象的例子——导数。关于这一点，你听到的可能是【算子】，而不是【变换】，但他们所要表达的思想是一样的 以导数为例，既然两者是一个东西，那么我们可不可以使用矩阵来描述多项式空间呢？ 如上图，以取 $x$ 的不同幂次方作为基函数，然后既可以写出求导变换的矩阵。这更进一步佐证了开篇提到的关键句子，矩阵 = 变换的数字表达 线性代数 函数 线性变换 线性算子 点积 内积 特征向量 特征函数 如上表一样，相同的概念只是在不同的领域有着不同的名称罢了。 有很多类似向量的不同事物，只要你处理的对象具有合理的数乘和相加的概念，线性代数中所有关于向量，线性变换和其他的概念都应该使用与它。作为数学家，你可能希望你发现的规律不只对一个特殊情况适用，对其他类似向量的事物都有普适性 向量空间这些类似向量的事物，比如箭头、一组数、函数等，他们构成的集合被称为：向量空间 向量加法和向量数乘的规则 - 被称为公理，如下图 它仅仅是一个待查列表，以保证向量加法和数乘的概念确实是你所希望的那样。这些公理是一种媒介，用来连接数学家和所有想要把这些结论应用于新的向量空间的人 仅仅根据这些公理描述一个空间，而不是集中于某一个特定的向量上。简而言之，这就是为什么你阅读的每一本教科书都会根据可加性和成比例来定义线性变换 总结对于【向量是什么】这个问题，数学家会直接忽略不作答。向量的形式并不重要，只要相加和数乘的概念遵守八条公理即可。就和问“3”究竟是什么一样。在数学中，他被看作是所有三个东西的集合的抽象概念，从而让你用一个概念就能推导出所有三个东西的集合。向量也是如此，它有很多种体现，但是数学把它抽象成【向量空间】这样一个无形（抽象）的概念 普适的代价是抽象（abstractness is the price of generality）。学习的过程只能来源于解决问题，来源于带有思考的不断重复，但如果你具备了正确的直观，你会再以后的学习中更加高效","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Linear Algebra","slug":"Linear-Algebra","permalink":"https://charlesliuyx.github.io/tags/Linear-Algebra/"}]},{"title":"【直观详解】什么是PCA、SVD","date":"2017-10-06T01:31:12.000Z","path":"2017/10/05/【直观详解】什么是PCA、SVD/","text":"【阅读时间】【内容简介】 在说明一个解释型内容的过程中，我一直坚信，带有思考的重复的是获取的知识的唯一捷径，所以会加入很多括号的内容，即另一种说法（从不同角度或其他称呼等），这样有助于理解。加粗的地方我也认为是比较重要的关键字或者逻辑推导，学习有一个途径就是划重点，做笔记。 What &amp; Why PCA（主成分分析）PCA，Principal components analyses，主成分分析。广泛应用于降维，有损数据压缩，特征提取和数据可视化。也被称为Karhunen-Loeve变换 从降维的方法角度来看，有两种PCA的定义方式，这里需要有一个直观的理解：什么是变换（线性代数基础），想整理一下自己线性代数的可以移步我的另一篇文章：【直观详解】线性代数的本质 但是总的来说，PCA的核心目的是寻找一个方向（找到这个方向意味着二维中的点可以被压缩到一条直线上，即降维），这个方向可以： 最大化正交投影后数据的方差（让数据在经过变换后更加分散） 紫色的直线 $u_1$ 即是关于 ${x_1,x_2}$ 二维的正交投影的对应一维表示PCA定义为使绿色点集的方差最小（方差是尽量让绿色所有点都聚在一坨）其中的蓝线是原始数据集（红点）到低纬度的距离，这可以引出第二种定义方式 最小化投影造成的损失（下图中所有红线（投影造成的损失）加起来最小） 投影造成的损失 PCA 主成分分析主要目的是为了减少数据维数，其中Auto-encoder也是一种精巧的降维手段 What &amp; Why SVD（奇异值分解）SVD，Singular Value Decomposition，奇异值分解。最直观的解释如下图所示 我们知道，矩阵描述的是一种变换（如果对这个概念有疑惑的，欢迎移步我的博客笔记：线性代数的本质）奇异值分解是矩阵分解的其中一种。换句话说，从上图的圆变换为右边的椭圆，通过一个 $\\mathbf M$ 矩阵就可以做到，但是，我们知道，非方阵是很不好处理的，我们希望，可以把 $\\mathbf M$ 矩阵表示的变换，分解为其他几种变换的组合（注意，分解之后，被分解的分量包含 $\\mathbf M$ 的信息，我们可以使用这些分量来进行操作），这几个变换我们希望是方阵，或者有特殊的性质。 $$ \\mathbf M = \\mathbf U \\cdot \\mathbf \\Sigma \\cdot \\mathbf V^* $$ $\\mathbf M$ 是一个m×n阶矩阵（输入为n维向量，输出为m维向量 $\\mathbf U$ 的列组成一套基向量，m×m阶矩阵，为$\\mathbf M \\mathbf M^*$ 的特征向量 $\\mathbf \\Sigma$ 对角矩阵，对角线上的值称为奇异值，可视为在输入与输出之间进行的标量的“伸缩尺度控制”。为 $\\mathbf M \\mathbf M^*$ 或 $\\mathbf M^* \\mathbf M$ 的非零特征值的平方根 $\\mathbf V^*$ 是 $\\mathbf V$ 的共轭转置（实数域即 $\\mathbf V^T$），n×n阶矩阵，$\\mathbf V$ 的列组成一套基向量，为 $\\mathbf M^* \\mathbf M$ 的特征向量 这里我们发现这个 $\\mathbf U$ 还有 $\\mathbf V$ 都是方阵，恰好满足之前的需求 且有 $\\mathbf U \\mathbf U^T = \\mathbf I_n$ 同时 $\\mathbf V \\mathbf V^T = \\mathbf I_m$ ，所以 $\\mathbf U$ 和 $\\mathbf V$ 是正交矩阵，而我们知道，正交矩阵对应的变换，就是旋转变换 对于 $\\mathbf \\Sigma$ 来说，我们知道特征值就是表示的度量伸缩程度的因子，即上图中的伸缩压缩程度（图中很直观的体现了这一点） 总结，SVD就是把一个非方阵（压缩变换）分解为一个旋转➜伸缩压缩➜旋转三个变换（矩阵），如上图所示 How PCA","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"}]},{"title":"【直观详解】什么是正则化","date":"2017-10-04T01:10:12.000Z","path":"2017/10/03/【直观详解】什么是正则化/","text":"【阅读时间】7min - 9min【内容简介】主要解决什么是正则化，为什么使用正则化，如何实现正则化，外加一些对范数的直观理解并进行知识整理以供查阅 Why &amp; What 正则化我们总会在各种地方遇到正则化这个看起来很难理解的名词，其实它并没有那么高冷，是很好理解的 首先，从使用正则化解决了一个什么问题的角度来看：正则化是为了防止过拟合， 进而增强泛化能力。用白话文转义，泛化误差（generalization error）= 测试误差（test error），其实就是使用训练数据训练的模型在测试集上的表现（或说性能 performance）好不好 如上图，红色这条“想象力”过于丰富上下横跳的曲线就是过拟合情形。结合上图和正则化的英文 Regularizaiton-Regular-Regularize，直译应该是：规则化（加个“化”字变动词，自豪一下中文还是强）。什么是规则？你妈喊你6点前回家吃饭，这就是规则，一个限制。同理，在这里，规则化就是说给需要训练的目标函数加上一些规则（限制），让他们不要自我膨胀。正则化，看起来，挺不好理解的，追其根源，还是“正则”这两字在中文中实在没有一个直观的对应，如果能翻译成规则化，更好理解。但我们一定要明白，搞学术，概念名词的准确是十分重要，对于一个重要唯一确定的概念，为它安上一个不会产生歧义的名词是必须的，正则化的名称没毛病，只是从如何理解的角度，要灵活和类比。 我的思考模式的中心有一个理念：每一个概念，被定义就是为了去解决一个实际问题（问Why&amp;What），接着寻找解决问题的方法（问How），这个“方法”在计算机领域被称为“算法”（非常多的人在研究）。我们无法真正衡量到底是提出问题重要，还是解决问题重要，但我们可以从不同的解决问题的角度来思考问题。一方面，重复以加深印象。另一方面，具有多角度的视野，能让我们获得更多的灵感，真正做到链接并健壮自己的知识图谱 How 线性模型角度对于线性模型来说，无论是Logistic Regression、SVM或是简单的线性模型，都有一个基函数 $\\phi()$，其中有很多 $\\mathbf w$ （参数）需要通过对损失函数 $E()$ 求极小值（或最大似然估计）来确定，求的过程，也就是使用训练集的训练过程：梯度下降到最小值点。最终，找到最合适的 $\\mathbf w$ 确定模型。从这个角度来看，正则化是怎么做的呢？ 二次正则项我们看一个线性的损失函数（真实值和预测值的误差） $$ E(\\mathbf w) =\\frac{1}{2} \\sum_{n=1}^{N}\\{t_n-\\mathbf w^T \\phi (\\mathbf x_n)\\}^2 \\tag{1} $$ $E(\\mathbf w)$ 是损失函数（又称误差函数），E即Evaluate，有时候写成L即Loss$t_n$ 是测试集的真实输出，又称目标变量【对应第一幅图中的蓝色点】$\\mathbf w$ 是权重（需要训练的部分，未知数）$\\phi()$ 是基函数，例如多项式函数，核函数测试样本有n个数据整个函数直观解释就是误差方差和，$\\frac{1}{2}$ 只是为了求导后消去方便计算 加正则化项，得到最终的误差函数（Error function） $$ \\frac{1}{2} \\sum_{n=1}^{N}\\{t_n-\\mathbf w^T \\phi (\\mathbf x_n)\\}^2 + \\frac{\\lambda}{2} \\mathbf w^T \\mathbf w \\tag{2} $$ (2)式被称为目标函数（评价函数）= 误差函数（损失函数） + 正则化项$\\lambda$ 被称为正则化系数，越大，这个限制越强 2式对 $\\mathbf w$ 求导，并令为0（使误差最小），可以解得 $$ \\mathbf w = (\\lambda \\mathbf I + \\Phi^T \\Phi)^{-1}\\Phi^T\\mathbf t $$ 这是最小二乘法的解形式，所以在题目中写的是从“最小二乘角度”。至于为何正则化项是 $\\frac{\\lambda}{2} \\mathbf w^T \\mathbf w$ 在之后马上解释 一般正则项直观的详解为什么要选择二次正则项。首先，需要从一般推特例，然后分析特例情况的互相优劣条件，可洞若观火。一般正则项是以下公式的形式 $$ \\frac{1}{2} \\sum_{n=1}^{N}\\{t_n-\\mathbf w^T \\phi (\\mathbf x_n)\\}^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{M} {\\vert w_j \\vert}^q \\tag{3} $$ M是模型的阶次（表现形式是数据的维度），比如M=2，就是一个平面（二维）内的点 若q=2就是二次正则项。高维度没有图像表征非常难以理解，那就使用二维作为特例来理解。这里令M=2，即 $\\mathbf x =\\{x_1,x_2\\} \\;\\mathbf w=\\{w_1,w_2\\}$ ，令q=0.5 q=1 q=2 q=4 有 横坐标是$w_1$纵坐标是$w_2$绿线是等高线的其中一条，换言之是一个俯视图，而z轴代表的是 $ \\frac{\\lambda}{2} \\sum_{j=1}^{M} {\\vert w_j \\vert}^q$ 的值 空间想象力不足无法理解的读者希望下方的三维图像能给你一个直观的领悟（与绿线图一一对应） q=2是一个圆非常好理解，考虑 $z = w_1^2 + w_2^2 $ 就是抛物面，俯视图是一个圆。其他几项同理（必须强调俯视图和等高线的概念，z轴表示的是正则项项的值） 蓝色的圆圈表示没有经过限制的损失函数在寻找最小值过程中，$\\mathbf w$的不断迭代（随最小二乘法，最终目的还是使损失函数最小）变化情况，表示的方法是等高线，z轴的值就是 $E(\\mathbf w)$$w^*$ 最小值取到的点 可以直观的理解为（帮助理解正则化），我们的目标函数（误差函数）就是求蓝圈+红圈的和的最小值（回想等高线的概念并参照3式），而这个值通在很多情况下是两个曲面相交的地方 可以看到二次正则项的优势，处处可导，方便计算，限制模型的复杂度，即 $\\mathbf w$ 中M的大小，M是模型的阶次，M越大意味着需要决定的权重越多，所以模型越复杂。在多项式模型多，直观理解是每一个不同幂次的 $x$ 前的系数，0（或很小的值）越多，模型越简单。这就从数学角度解释了，为什么正则化（规则化）可以限制模型的复杂度，进而避免过拟合 不知道有没有人发现一次正则项的优势，$w^*$ 的位置恰好是 $w_1=0$ 的位置，意味着从另一种角度来说，使用一次正则项可以降低维度（降低模型复杂度，防止过拟合）二次正则项也做到了这一点，但是一次正则项做的更加彻底，更稀疏。不幸的是，一次正则项有拐点，不是处处可微，给计算带来了难度，很多厉害的论文都是巧妙的使用了一次正则项写出来的，效果十分强大 How 神经网络模型角度我们已经知道，最简单的单层神经网，可以实现简单的线性模型。而多隐含层的神经网络模型如何来实现正则化？（毕竟神经网络模型没有目标函数） M表示单层神经网中隐含层中的神经元的数量 上图展示了神经网络模型过拟合的直观表示 我们可以通过一系列的推导得知，未来保持神经网络的一致性（即输出的值不能被尺缩变换，或平移变换），在线性模型中的加入正则项无法奏效 所以我们只能通过建立验证集（Validation Set），拉网搜索来确定M的取值（迭代停止的时间），又称为【提前停止】 这里有一个尾巴，即神经网络的不变量（invariance），我们并不希望加入正则项后出现不在掌控范围内的变化（即所谓图像还是那个图像，不能乱变）。而机器学习的其中一个核心目的也是去寻找不同事物（对象）的中包含信息的这个不变量（特征）。卷积神经网络从结构上恰恰实现了这种不变性，这也是它强大的一个原因 范数我并不是数学专业的学生，但是我发现在讲完线性模型角度后，有几个概念可以很轻松的解答，就在这里献丑把它们串联起来，并做一些总结以供查阅和对照。 我们知道，范数（norm）的概念来源于泛函分析与测度理论，wiki中的定义相当简单明了：范数是具有“长度”概念的函数，用于衡量一个矢量的大小（测量矢量的测度） 我们常说测度测度，测量长度，也就是为了表征这个长度。而如何表达“长度”这个概念也是不同的，也就对应了不同的范数，本质上说，还是观察问题的方式和角度不同，比如那个经典问题，为什么矩形的面积是长乘以宽？这背后的关键是欧式空间的平移不变性，换句话说，就是面积和长成正比，所以才有这个 没有测度论就没有（现代）概率论。而概率论也是整个机器学习学科的基石之一。测度就像尺子，由于测量对象不同，我们需要直尺量布匹、皮尺量身披、卷尺量房间、游标卡尺量工件等等。注意，“尺子”与刻度（寸、米等）是两回事，不能混淆。 范数分为向量范数（二维坐标系）和矩阵范数（多维空间，一般化表达），如果不希望太数学化的解释，那么可以直观的理解为：0-范数：向量中非零元素的数量；1-范数：向量的元素的绝对值；2-范数：是通常意义上的模（距离） 向量范数关于向量范数，先再把这个图放着，让大家体会到构建知识图谱并串联知识间的本质（根）联系的好处 p-范数 $$ \\Vert\\mathbf x \\Vert_p =(\\sum\\limits_{i=1}^{N}\\vert x_i \\vert^p)^{\\frac{1}{p}} $$ 向量元素绝对值的p次方和的 $\\frac{1}{p}$ 次幂。可以敏捷的发现，这个p和之前的q从是一个东西，随着p越大，等高线图越接近正方形（正无穷范数）；越小，曲线弯曲越接近原点（负无穷范数） 而之前已经说明，q的含义是一般化正则项的幂指数，也就是我们常说的2范数，两者在形式上是完全等同的。结合范数的定义，我们可以解释一般化正则项为一种对待求参数 $\\mathbf w$ 的测度，可以用来限制模型不至于过于复杂 $-\\infty$-范数 $$ \\Vert \\mathbf x \\Vert_{-\\infty} = arg \\operatorname*{min}_{i}{\\vert x_i \\vert} $$ 所有向量元素中绝对值的最小值 1-范数 $$ \\Vert \\mathbf x \\Vert_1 = \\sum\\limits_{i=1}^{N}\\vert x_i \\vert $$ 向量元素绝对值之和，也称街区距离（city-block） $$ \\begin{matrix} 4 & 3 & 2 & 3 & 4 \\\\ 3 & 2 & 1 & 2 & 3\\\\ 2 & 1 & 0 & 1 & 2 \\\\ 3 & 2 & 1 & 2 &3 \\\\ 4&3 & 2 &3 &4 \\\\ \\end{matrix} $$ 2-范数 $\\Vert\\mathbf x \\Vert_2 = \\sqrt{\\sum\\limits_{i=1}^{N} x_i^2}$ ：向量元素的平方和再开方。Euclid范数，也称欧几里得范数，欧氏距离 $$ \\begin{matrix} 2.8&2.2&2&2.2&2.8 \\\\ 2.2&1.4&1&1.4&2.2 \\\\ 2&1&0&1&2 \\\\ 2.2&1.4&1&1.4&2.2 \\\\ 2.8&2.2&2&2.2&2.8 \\\\ \\end{matrix} $$ $\\infty$-范数 $\\Vert \\mathbf x \\Vert_{\\infty} = arg \\operatorname*{max}_{i}{\\vert x_i \\vert}$ ：所有向量元素中绝对值的最大值，也称棋盘距离（chessboard），切比雪夫距离 $$ \\begin{matrix} 2 & 3 & 2 & 2 & 2 \\\\ 2 & 1 & 1 & 1 & 2\\\\ 2 & 1 & 0 & 1 & 2 \\\\ 2 & 1 & 1 & 1 &2 \\\\ 2&2 & 2 &2 &2 \\\\ \\end{matrix} $$ 矩阵范数1-范数 $$ \\Vert \\mathbf A \\Vert_{1} = arg \\operatorname*{max}_{1 \\leqslant j \\leqslant n}\\sum\\limits_{i=1}^m{\\vert a_{i,j} \\vert} $$ 列和范数，即所有矩阵列向量绝对值之和的最大值 $\\infty$-范数 $$ \\Vert \\mathbf A \\Vert_{\\infty} = arg \\operatorname*{max}_{1 \\leqslant i \\leqslant n}\\sum\\limits_{j=1}^m{\\vert a_{i,j} \\vert} $$ 行和范数，即所有矩阵行向量绝对值之和的最大值 2-范数 $\\Vert \\mathbf A \\Vert_{2} = \\sqrt{\\lambda_{max}(\\mathbf A^* \\mathbf A) }$ p=2且m=n方阵时，称为谱范数。矩阵 $\\mathbf A$ 的谱范数是 $\\mathbf A$ 最大的奇异值或半正定矩阵 $\\mathbf A^T \\mathbf A$ 的最大特征值的平方根 $\\mathbf A^*$ 为 $\\mathbf A$ 的共轭转置，实数域等同于 $\\mathbf A^T$ F-范数 $\\Vert \\mathbf A \\Vert_{F} = \\sqrt{ \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n \\vert a_{i,j}\\vert^2 }$ Frobenius范数（希尔伯特-施密特范数，这个称呼只在希尔伯特空间），即矩阵元素绝对值的平方和再开平方 核范数 $\\Vert \\mathbf A \\Vert_{*} = \\sum\\limits_{i=1}^n \\lambda_i$ ：$\\lambda_i$ 若 $\\mathbf A$ 矩阵是方阵，称为本征值。若不是方阵，称为奇异值，即奇异值/本征值之和 总结相信每个人在学习过程中都有过看书时，遇到0-范数正则化，或者1-范数正则化，2-范数正则化的表达时很迷惑。写到这里，希望大家能对这些看起来无法理解的晦涩名词有一个融会贯通的理解和感知！ Learning with intuitive and get Insight 以上！鞠躬！","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"}]},{"title":"Pandas-Wiki","date":"2017-09-30T17:44:25.000Z","path":"2017/09/30/Pandas-Wiki/","text":"【阅读时间】百科类【内容简介】pandas函数库相关操作手册，供查阅使用。笔记对象来自集智 必备库的导入 123import pandas as pdimport numpy as npimport matplotlib.pyplot as plt 创建对象创建一个Series对象：传递一个数值列表作为参数，令Pandas使用默认索引。 12345678910s = pd.Series([1,3,5,np.nan,6,8])print(s)&gt;0 1.01 3.02 5.03 NaN4 6.05 8.0dtype: float64 创建一个DataFrame对象：传递待datetime索引和列标签的Numpy数组作为参数。 123456789101112131415161718# 首先创建一个时间序列dates = pd.date_range('20130101', periods=6)print(dates)# 创建DataFrame对象，指定index和columns标签df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))print(df)&gt; DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D') A B C D2013-01-01 0.194508 -0.897507 0.224745 0.0902602013-01-02 2.412146 -1.191852 -1.644737 0.1909712013-01-03 -0.674645 0.395960 1.425822 -0.7182312013-01-04 0.349046 0.315026 2.058357 -0.8823452013-01-05 1.467093 0.146932 -0.680309 -0.5191552013-01-06 2.223284 -0.305247 -0.559043 1.017710 也可以传递词典来构建DataFrame，该词典的元素是形如Series的对象。 12345678910111213141516171819df2 = pd.DataFrame(&#123; 'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F' : 'foo' &#125;)print(df2)# 查看df2对象各列的数据类型print(df2.dtypes)&gt; A B C D E F0 1.0 2013-01-02 1.0 3 test foo1 1.0 2013-01-02 1.0 3 train foo2 1.0 2013-01-02 1.0 3 test foo3 1.0 2013-01-02 1.0 3 train fooA float64B datetime64[ns]C float32D int32E categoryF objectdtype: object 观察数据查看一个DataFrame对象的前几行和最后几行。 1234567891011121314print(df.head())print(df.tail(3))# 默认情况下，.head()和.tail()输出首尾的前5行，也可以手动指定输出行数。&gt; A B C D2013-01-01 0.194508 -0.897507 0.224745 0.0902602013-01-02 2.412146 -1.191852 -1.644737 0.1909712013-01-03 -0.674645 0.395960 1.425822 -0.7182312013-01-04 0.349046 0.315026 2.058357 -0.8823452013-01-05 1.467093 0.146932 -0.680309 -0.519155 A B C D2013-01-04 0.349046 0.315026 2.058357 -0.8823452013-01-05 1.467093 0.146932 -0.680309 -0.5191552013-01-06 2.223284 -0.305247 -0.559043 1.017710 查看索引(index)，列标签(columns)和Numpy数组形式的内容。 .describe()返回简约的统计信息，在工程中非常实用。 1234567891011121314151617181920212223242526272829303132# 索引print(df.index)&gt;DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04','2013-01-05', '2013-01-06'],dtype='datetime64[ns]', freq='D')# 列标签print(df.columns)&gt;Index(['A', 'B', 'C', 'D'], dtype='object')# 数值print(df.values)&gt;[[ 0.19450849 -0.89750748 0.22474509 0.09025968] [ 2.41214612 -1.19185153 -1.64473716 0.19097097] [-0.67464536 0.39595956 1.42582221 -0.71823105] [ 0.3490457 0.31502599 2.05835699 -0.88234524] [ 1.46709286 0.14693219 -0.68030862 -0.51915519] [ 2.2232837 -0.30524727 -0.55904285 1.01771006]]# 统计print(df.describe())&gt; A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.995239 -0.256115 0.137473 -0.136798std 1.231715 0.663815 1.391936 0.711615min -0.674645 -1.191852 -1.644737 -0.88234525% 0.233143 -0.749442 -0.649992 -0.66846250% 0.908069 -0.079158 -0.167149 -0.21444875% 2.034236 0.273003 1.125553 0.165793max 2.412146 0.395960 2.058357 1.017710 灵活调教你的DataFrame：转置与排序 12345678# 转置print(df.T)# 按轴排序，逐列递减print(df.sort_index(axis=1, ascending=False))# 按值排序，'B'列逐行递增print(df.sort_values(by='B')) 选中尽管标准Python/Numpy的选中表达式很直观也很适合互动，但在生产代码中还是推荐使用Pandas里的方法如.at,.iat,.loc,.iloc,.ix等。 获取DataFrame里选中的一列与Series等效。 12345678910111213141516171819202122print(df[\"A\"]) # 与df.A相同&gt;2013-01-01 0.1945082013-01-02 2.4121462013-01-03 -0.6746452013-01-04 0.3490462013-01-05 1.4670932013-01-06 2.223284Freq: D, Name: A, dtype: float64# 用[]分割DataFrameprint(df[0:3])print(df['20130102':'20130104'])&gt; A B C D2013-01-01 0.194508 -0.897507 0.224745 0.0902602013-01-02 2.412146 -1.191852 -1.644737 0.1909712013-01-03 -0.674645 0.395960 1.425822 -0.718231 A B C D2013-01-02 2.412146 -1.191852 -1.644737 0.1909712013-01-03 -0.674645 0.395960 1.425822 -0.7182312013-01-04 0.349046 0.315026 2.058357 -0.882345 按标签选择12345678910111213141516171819202122232425262728293031323334# 选中一整行print(df.loc[dates[0]])&gt;A 0.194508B -0.897507C 0.224745D 0.090260Name: 2013-01-01 00:00:00, dtype: float64 # 按标签选中复数列（所有行，输出只显示前5行）print(df.loc[:,['A','B']])&gt; A B2013-01-01 0.194508 -0.8975072013-01-02 2.412146 -1.1918522013-01-03 -0.674645 0.3959602013-01-04 0.349046 0.3150262013-01-05 1.467093 0.1469322013-01-06 2.223284 -0.305247# 行/列同时划分（包括起止点）print(df.loc['20130102':'20130104',['A','B']])&gt; A B2013-01-02 2.412146 -1.1918522013-01-03 -0.674645 0.3959602013-01-04 0.349046 0.315026# 返回一个元素（两个方法等效）print(df.loc[dates[0],'A'])print(df.at[dates[0],'A'])&gt;0.1945084944020.194508494402 按位置选择12345678910111213141516171819202122232425# 位置索引为3的行（从0开始，所以其实是第4行）print(df.iloc[3])&gt;A 0.349046B 0.315026C 2.058357D -0.882345Name: 2013-01-04 00:00:00, dtype: float64 # 按位置索引分割DataFrameprint(df.iloc[3:5,0:2])print(df.iloc[[1,2,4],[0,2]])&gt; A B2013-01-04 0.349046 0.3150262013-01-05 1.467093 0.146932# 直接定位一个特定元素df.iloc[1,1]df.iat[1,1]&gt; A C2013-01-02 2.412146 -1.6447372013-01-03 -0.674645 1.4258222013-01-05 1.467093 -0.680309 布尔值索引123456789101112131415161718192021# 用一列的值来选择数据print(df.A &gt; 0)&gt;2013-01-01 True2013-01-02 True2013-01-03 False2013-01-04 True2013-01-05 True2013-01-06 TrueFreq: D, Name: A, dtype: bool # 使用.isin()函数过滤数据df2 = df.copy()df2['E'] = ['one', 'one','two','three','four','three']# 提取df2中'E'值属于['two', 'four']的行print(df2[df2['E'].isin(['two','four'])])&gt; A B C D E2013-01-03 -0.674645 0.395960 1.425822 -0.718231 two2013-01-05 1.467093 0.146932 -0.680309 -0.519155 four 设置赋值 1234567891011121314151617# 为DataFrame创建一个新的列，其值为时间顺序（与df相同）的索引值s1 = pd.Series([1,2,3,4,5,6], index=pd.date_range('20130102', periods=6))print(s1)df['F'] = s1# 按标签赋值df.at[dates[0],'A'] = 0# 按索引赋值df.iat[0,1] = 0# 用Numpy数组赋值df.loc[:,'D'] = np.array([5] * len(df))# 最终结果print(df) 缺失数据Pandas默认使用np.nan来代表缺失数据。Reindexing允许用户对某一轴上的索引改/增/删，并返回数据的副本。 12345678910111213# 创建DataFrame对象df1，以dates[0:4]为索引，在df的基础上再加一个新的列'E'（初始均为NaN）df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E'])# 将'E'列的前两个行设为1df1.loc[dates[0]:dates[1],'E'] = 1print(df1)&gt; A B C D E2013-01-01 0.194508 -0.897507 0.224745 0.090260 1.02013-01-02 2.412146 -1.191852 -1.644737 0.190971 1.02013-01-03 -0.674645 0.395960 1.425822 -0.718231 NaN2013-01-04 0.349046 0.315026 2.058357 -0.882345 NaN 处理缺失数据12345678# 剔除df1中含NaN的行（只要任一一列为NaN就算）df1.dropna(how='any')# 用5填充df1里的缺失值df1.fillna(value=5)# 判断df1中的值是否为缺失数据，返回True/Falsepd.isnull(df1) 操作统计此类操作默认排除缺失数据 12345678910111213141516171819202122232425262728293031323334353637383940414243# 求平均值print(df.mean())&gt;A -0.190821B -0.050040C -0.203207D 5.000000F 3.000000dtype: float64 # 指定求平均值的轴print(df.mean(1))&gt;2013-01-01 1.2647492013-01-02 1.0497482013-01-03 1.5780672013-01-04 1.0356392013-01-05 1.8557542013-01-06 1.936110Freq: D, dtype: float64 # 创建Series对象s，以dates为索引并平移2个位置s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)print(s)&gt;2013-01-01 NaN2013-01-02 NaN2013-01-03 1.02013-01-04 3.02013-01-05 5.02013-01-06 NaNFreq: D, dtype: float64# 从df中逐列减去s（若有NaN则得NaN）print(df.sub(s, axis='index'))&gt; A B C D F2013-01-01 NaN NaN NaN NaN NaN2013-01-02 NaN NaN NaN NaN NaN2013-01-03 -1.572312 0.570952 -1.108305 4.0 1.02013-01-04 -3.401496 -4.304842 -4.115467 2.0 0.02013-01-05 -4.955735 -5.576715 -4.188780 0.0 -1.02013-01-06 NaN NaN NaN NaN NaN 应用对DataFrame中的数据应用函数。 1234567891011121314151617181920# 逐行累加print(df.apply(np.cumsum))&gt; A B C D F2013-01-01 0.000000 0.000000 0.058997 5 NaN2013-01-02 0.277465 -0.161767 -0.807960 10 1.02013-01-03 -0.294847 1.409186 -0.916265 15 3.02013-01-04 -0.696343 0.104344 -2.031732 20 6.02013-01-05 -0.652078 -0.472372 -1.220512 25 10.02013-01-06 -1.144929 -0.300242 -1.219241 30 15.0# 每列的最大值减最小值print(df.apply(lambda x: x.max() - x.min()))&gt;A 0.849776B 2.875794C 1.926687D 0.000000F 4.000000dtype: float64 字符Series对象的str属性具有一系列字符处理方法，可以很轻松地操作数组的每个元素。 1234567891011121314# 字符串变为小写s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])print(s.str.lower())&gt;0 a1 b2 c3 aaba4 baca5 NaN6 caba7 dog8 catdtype: object 合并连接Pandas在join/merge两中情境下提供了支持多种方式，基于逻辑/集合运算和代数运算来连接Series，DataFrame和Panel对象。 concat()方法连接数组 123456789df = pd.DataFrame(np.random.randn(10, 4))print(df)print(\"------\")# 拆分成块pieces = [df[:3], df[3:7], df[7:]]# 重新连接，可得初始数组print(pd.concat(pieces)) 增补给DataFrame增补行 1234567df = pd.DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])print(df)print(\"------\")# 将索引为3的行增补到整个DataFrame最后s = df.iloc[3]print(df.append(s, ignore_index=True)) 组合“组合”包含以下步骤： 基于一定标准将数据分组 对每个部分分别应用函数 把结果汇合到数据结构中 12345678910111213141516171819202122232425262728293031323334# 新建DataFrame对象dfdf = pd.DataFrame(&#123;'A' : ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'], 'B' : ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'], 'C' : np.random.randn(8), 'D' : np.random.randn(8)&#125;)print(df)&gt; A B C D0 foo one 0.298545 -0.1018931 bar one 1.080680 -0.7172762 foo two 1.365395 0.9394823 bar three 0.783108 -0.5759954 foo two -1.089990 -0.0338265 bar two 0.442084 1.5331466 foo one 0.041715 0.1906137 foo three 0.529231 0.380723# 对'A'列进行合并并应用.sum()函数print(df.groupby('A').sum())&gt; C DA bar 2.305871 0.239875foo 1.144897 1.375099# 对'A', 'B'两列分别合并形成层级结构，再应用.sum()函数print(df.groupby(['A','B']).sum())&gt; C DA B bar one 1.080680 -0.717276 three 0.783108 -0.575995 two 0.442084 1.533146foo one 0.340260 0.088720 three 0.529231 0.380723 two 0.275406 0.905656 重塑层次化12345678910111213141516171819202122232425262728293031tuples = list(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]))# 多重索引index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B'])df2 = df[:4]print(df2)&gt; A Bfirst second bar one -1.144920 -0.823033 two 0.250615 -1.423107baz one 0.291435 -1.580619 two -0.574831 -0.742291# .stack()方法将DataFrame的列“压缩”了一级stacked = df2.stack()print(stacked)&gt;first second bar one A -1.144920 B -0.823033 two A 0.250615 B -1.423107baz one A 0.291435 B -1.580619 two A -0.574831 B -0.742291dtype: float64 对于已经层次化的，具有多重索引的DataFrame或Series，stack()的逆操作是unstack()——默认将最后一级“去层次化”。 1234567891011121314151617181920212223242526print(stacked.unstack())&gt; A Bfirst second bar one -1.144920 -0.823033 two 0.250615 -1.423107baz one 0.291435 -1.580619 two -0.574831 -0.742291print(stacked.unstack(1))&gt;second one twofirst bar A -1.144920 0.250615 B -0.823033 -1.423107baz A 0.291435 -0.574831 B -1.580619 -0.742291 print(stacked.unstack(0))&gt;first bar bazsecond one A -1.144920 0.291435 B -0.823033 -1.580619two A 0.250615 -0.574831 B -1.423107 -0.742291 数据透视表1234567891011121314151617181920212223242526272829303132df = pd.DataFrame(&#123;'A' : ['one', 'one', 'two', 'three'] * 3, 'B' : ['A', 'B', 'C'] * 4, 'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2, 'D' : np.random.randn(12), 'E' : np.random.randn(12)&#125;)print(df)&gt; A B C D E0 one A foo -0.411674 0.2845231 one B foo -1.217944 1.5192932 two C foo 0.502824 -0.1678983 three A bar 0.565186 0.2268604 one B bar 0.626023 0.4015295 one C bar -0.437217 0.8328816 two A foo -0.825128 0.3463037 three B foo 0.069236 0.7287298 one C foo 1.647690 -0.5310919 one A bar -0.881553 0.07071810 two B bar 0.203672 1.60176111 three C bar 1.334214 -0.778639# 生成数据透视表print(pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C']))&gt;C bar fooA B one A -0.881553 -0.411674 B 0.626023 -1.217944 C -0.437217 1.647690three A 0.565186 NaN B NaN 0.069236 C 1.334214 NaNtwo A NaN -0.825128 B 0.203672 NaN C NaN 0.502824 时间序列Pandas提供了简单、强力且有效的工具，可以在频率转换中进行重采样（比如从秒级数据中提取5分钟一更新的数据）。这在金融工程中应用甚广，当然也不仅限于金融领域。 时区表示 12345678910111213141516171819202122232425262728293031rng = pd.date_range('3/6/2012 00:00', periods=5, freq='D')ts = pd.Series(np.random.randn(len(rng)), rng)print(ts)&gt;2012-03-06 -0.6051792012-03-07 0.9612522012-03-08 1.3094062012-03-09 1.1843132012-03-10 0.249745Freq: D, dtype: float64# 设定国际时区标准ts_utc = ts.tz_localize('UTC')print(ts_utc)&gt;2012-03-06 00:00:00+00:00 -0.6051792012-03-07 00:00:00+00:00 0.9612522012-03-08 00:00:00+00:00 1.3094062012-03-09 00:00:00+00:00 1.1843132012-03-10 00:00:00+00:00 0.249745Freq: D, dtype: float64# 切换时区print(ts_utc.tz_convert('US/Eastern'))&gt;2012-03-05 19:00:00-05:00 -0.6051792012-03-06 19:00:00-05:00 0.9612522012-03-07 19:00:00-05:00 1.3094062012-03-08 19:00:00-05:00 1.1843132012-03-09 19:00:00-05:00 0.249745Freq: D, dtype: float64 周期和时间戳之间的转换让我们可以方便的使用一些算术运算。比如下面的例子，我们把一个以季度为单位的时间序列转换为按日期表示。 1234567prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')ts = pd.Series(np.random.randn(len(prng)), prng)print(ts.head())ts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9print(ts.head()) 分类12345df = pd.DataFrame(&#123;\"id\":[1,2,3,4,5,6], \"raw_grade\":['a', 'b', 'b', 'a', 'a', 'e']&#125;)# 将原始记录转换为分类类型df[\"grade\"] = df[\"raw_grade\"].astype(\"category\")print(df[\"grade\"]) 将类别重命名为更有意义的字样 123456789101112df[\"grade\"].cat.categories = [\"very good\", \"good\", \"very bad\"]# 重排类别同时添加上缺失的类别名称df[\"grade\"] = df[\"grade\"].cat.set_categories([\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"])print(df[\"grade\"])# 排序在各分类中分别进行print(df.sort_values(by=\"grade\"))# 对类别列分组可以显示空类print(df.groupby(\"grade\").size()) 绘图随机累加序列 12345678# 生成一串随机序列ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))# 求累加和ts = ts.cumsum()# 输出图象ts.plot() 绘制带标签的列 12345# 生成一个4列的DataFrame，每列1000行，并逐列累加df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=['A', 'B', 'C', 'D']) df = df.cumsum()df.plot(); plt.legend(loc='best') 获取数据输入/输出CSVdf是一个DataFrame 12345# 输出至.csv文件df.to_csv('haha.csv')# 从.csv文件中读取数据pd.read_csv('haha.csv') Exceldf是一个DataFrame 123456# 输出至.xlsx文件df.to_excel('haha.xlsx', sheet_name='Sheet1')# 从.xlsx文件中读取数据pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])('haha.csv')","tags":[{"name":"Wiki","slug":"Wiki","permalink":"https://charlesliuyx.github.io/tags/Wiki/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Library","slug":"Library","permalink":"https://charlesliuyx.github.io/tags/Library/"}]},{"title":"【区块链】比特币与金融、ICO和监管","date":"2017-09-25T20:59:14.000Z","path":"2017/09/25/区块链（比特币）与金融/","text":"【阅读时间】13min - 17min - 4187 words【内容简介】其中讨论了区块链技术（比特币）对金融体系的影响，并解读了最近很火的热点问题：什么是ICO，对一些大事件的时间表进行记录和总结 三个关键问题英格兰银行副行长，布罗德本特提出了有关区块链金融的三个关键问题十分犀利，也为宋老师推崇，这里列出来大家共同思考 第一个问题 比特币它并不是创造了一种新的货币，而是创造了一种新的清算方式 清算权清算问题始终是中央银行诞生的最重要的动力，是金融体系中相当核心的问题，而货币发行权的基础在于清算权 17世纪，荷兰的阿姆斯特丹银行，进化成了中央银行，首创用金银币的账户来进行清算 19世纪，上海汇丰银行。控制了大清国的货币清算功能。担当清算中心的职责，整个清朝内部的资金流动，都要通过汇丰银行 到现在，美国对某某国家采取金融制裁，因为美国掌握着美元的清算权 支付宝和微信支付争的是网络交易的清算权 谁能控制清算权，就占据了战略的制高点，有俯瞰全局的作用，控制战局。清算权意味着游戏规则的制订权。 这种新的清算方式，成本低。英格兰银行希望主导这种潮流，也就是RSCoin的核心 英国的数字货币，依赖大账本进行清算 但是它要控制这个大账本 比特币清算的致命缺陷 互联人没有信用体系，用消耗运算力的方式来保持整个记账体系的安全 浪费巨大的资源，导致记账效率低下 会出现记账效率的瓶颈。比如，比特币，每秒只能完成7笔交易，而全世界体系要求的记账效率非常高，比如，VISA卡，每秒2000 - 7000笔 RSCoin的解决办法记账效率低下，是由于竞争记账。既然银行控制大账本，没有必要搞竞争，指定少数几家银行，负责记账。双层记账体系，树形结构优化，RSCoin使用30个授权节点，一秒可以达到2000笔 我的观点有关英国人弄得RSCoin来讲，个人认为已经脱离的了区块链技术的核心Idea：贪婪=信任。既然还是有中心化的控制，那么就等同于只使用了区块链的数据结构，本质来说，也就不是加密货币。 但是，有一些博弈和妥协的想法是非常值得借鉴的，比如，流水账本身就有透明的好处，其实是从另一方面利用区块链密码学加密技术实现了一种变种的信息透明 第二个问题 如果数字货币是一个独立的货币，未来它将会跟传统的银行，争夺储蓄 举个例子来说，共享单车想当于数字货币。出现之前，单车控制权都是银行。骑车，只能从一家银行停车场到另一家银行的停车场。自行车的控制权，由银行转向了最终客户。只要能解开密码锁，你就可以随便骑，随意停。久而久之，共享单车数量越来越大，传统的自行车就会越来越小 银行的商业模式是：吸纳储蓄➜放贷➜吃存款和贷款之间的利差。数字货币的量越大，银行体系中的储蓄减少，金融体系面临挤兑的风险，数字货币很可能成为传统银行这种商业模式的掘墓人，如何解决这个问题，布罗德本特提出了第三个问题 第三个问题 新的数字货币如何去倒逼传统银行商业模式的改革 （胖银行➜瘦银行） 胖银行的基本运行模式 货币金融学中的概念：部分准备金：允许银行用一块钱的准备金进行10倍的放大 ，即用1块钱创造10块钱 银行体系因为这个制度有了巨大的竞争优势。开银行等于印钞 其中的前提条件或基础就是一定要获得大量的储蓄然后才能进行放贷 存在问题 储蓄是别人的钱，是短期的，放贷的资产是长期的 在金融危机中，资产会迅速贬值，使得银行体系变得不稳定 危机爆发，银行倒闭，银行倒闭，社会流动性紧缩，钱荒，更多银行倒闭，工厂关门，工人下岗，消费力消失，导致金融危机和经济危机 特权化是社会动荡的罪魁祸首 瘦银行亚当·斯密，李嘉图等主张搞瘦银行（不赞同部分准备金制度） ，比如有名的芝加哥计划 周期性金融危机的爆发，就跟银行不断放大这种杠杆就直接的关系。其中的关键问题是：如果银行由胖变瘦，使银行总的资产负债表收缩，社会的流动性紧缩，发生钱荒，经济就会停滞不前 解决方式 央行给全体国民每个人开账户 传统意义上，只有金融机构可以在中央银行开账户，目的也是相互清算。以前做不到，主要原因是技术水平不够，效率太低，成本太高 但在数字货币时代，这个问题很简单。意味着央行向人民开放资产负债表，人人可以在央行有账户。应对金融危机的方式变成，中央银行给每个国民的账户上打钱 重资产商业银行现在是重资产 持有大量的贷款，长期贷款。承当巨大的风险：比如利率，汇率不稳定；经济周期；地缘政治冲突；国际油价。进而引发贷款出现违约 在日益动荡的社会中，重资产的运行模式，风险大，利差小 轻资产转型问题，就涉及到轻资产，这里可以用万达的转型之路来举例 最开始重资产：从拿地，融资，投资，建设到经营，自己搞定。造成的问题是人多事杂；押占大量资金；销售也有压力 转型成轻资产模式：出品牌，设计广场，钱由社会上投资人来投 ➜负责运营，利润分成➜赚取管理费用 银行也是类似的轻资产模式 银行出品牌，出技术团队：因为银行的人对全国各种项目比较了解，知道哪个贷款比较可靠，风险低 重点工作是：设计贷款产品；设计完，投资人来出钱，银行负责风控，运营，挣的钱也是服务费 关于三个问题的思考这么一套总结下来，的确想象中十分美好，但是我们不能忽略国家间的政治博弈，利益冲突。银行转型，也是涉及到了众多的问题和利益方面的权衡和Trade-off，但我认为，区块链比特币的出现，的确给了银行一定的压力，这样就和布罗德本特先生的真知灼见切合了，是一种倒逼，是一种时代潮流倾轧的必然产物 金融结算应用结算步骤 交易指令管理（包括交易验证） 清算（即计算交易双方的财务义务） 结算（即最终的资产转移） CSDs功能中央证券登记机构（Central Security Depositories，CSDs）在结算中发挥了关键作用，承担了3项主要功能 认证（公正并受信任地维护已发行证券的记录） 结算（将证券的所有权从卖方转给买方） 账目维护（建立并更新证券的所有权记录） 中央证券登记机构还承担证券托管、资产服务、融资、报告或证券出借等功能 结算行业的痛点在金融交易后结算中，一笔交易涉及多个中介机构。每个中介机构都使用自己的系统来处理、发送和接收交易指令、核对数据、管理差错等，并维护自己的交易记录。每个中介机构使用的数据标也准都不统一 这些都会产生大量成本。区块链技术有助于建立并维护共享的、同时化（synchronized）的账目，简化交易对账过程。 目前，结算行业讨论在金融交易后结算中引入私有的、有准入限制的区块链系统。其中，每一个节点扮演不同角色并且在读取区块链上信息上有不同权限，一组受信任的参与者承担验证职能 区块链优势 通过分布式、同时化、共享的证券所有权记录来简化和自动化交易后结算工作，降低交易对账和数据管理成本 缩短结算所需时间，减低结算风险敞口 因为与交易有关信息由交易双方共享，能促进自动清算 缩短托管链，使投资者可以直接持有证券，降低投资者承担的法律和运营风险以及中介成本 可跟踪性好，透明度高 去中心化、多备份能提高系统安全性和抗压性 应用难点 如何实现认证功能？ 尽管区块链能保证分布式账本的准确性，但还需有一个受信任的机构来确保已发行证券信息的真实性，一定的中心化必须的，只是可以利用计算机和分布式存储大数据的方式来提高效率，这部分是非常靠谱的 如何实现存管功能？ 特别是，如何将托管机构和存管机构持有的资产转移到区块链上。一个可行方案是使用电子凭证（digital token）来代表不在区块链上的资产，但需要一个受信任机构来确保电子凭证与资产之间的对应关系 如何实现券款对付（Delivery versus payment，DvP）？ 这要求区块链能同时处理现金账户 如何确保结算的最终性（settlement finality）？ 比如，比特币区块链系统因为分叉的存在，只能在概率意义上确保结算的最终性（尽管该概率随时间趋向1） 在法律上，区块链上的记录能否构成所有权证明？ 交易匹配和差错管理。区块链在比较不同维度数据、处理合同不匹配和例外情况上还面临不少障碍 如何在多方参与验证的情况下，确保交易信息的保密性？ 一个方案是一个受信任的机构和交易双方才能参加与交易有关的共识机制 另一个方案是区分交易数据和验证所需数据。零知识验证（zero-knowledge proofs, ZKPs）也是一个可能的解决工具 身份管理 可扩展性 与现有流程和基础设施的兼容性（interoperability） 区块链在产权登记、确认、交易等中的应用逻辑和有待解决的问题，与金融交易后结算非常类似。在这个领域，仔细琢磨，还有很多难关需要攻克 什么是ICO最近中国强势出手，关停ICO，手段雷厉风行，很多ICO项目都变成了一场庞氏骗局，让人唏嘘，这里针对ICO做一些思考，出自借鉴科普文章 赌场例子 有一个人开了一间赌场，每个赌徒要来赌场玩，必须先换一些筹码，才能参与赌场内的赌局。赌场内所有赌局都是实时利用筹码结算的。 这间赌场服务特别好，赌具特别好，入场费收得也非常低，总之一切都特别好，于是越来越多的赌徒都慕名而来，跑到这间赌场来玩，赌局额度也越来越大。自然，这就需要更多的筹码来确保赌场的顺利运营。 但是，这间赌场的筹码是用一种特殊的金属、特殊的工艺制造的，这些赌具只能用这种筹码玩。赌场一开始的时候造了一万个，以后再也造不出来了。怎么办？ 参与赌局的人越来越多，赌局额度越来越大，原本1个筹码1美元，同时100个人在玩，一次赢1美元，赌场运行得很顺利，除了在赌徒间流通的筹码外，赌场手里还有一些筹码，可以卖给刚进场的赌徒。现在来了1000个人，每次要赢100美元，依然只有10000个筹码。 为了玩得尽兴，于是赌徒之间互相约定，我们1个筹码100美元，这样就能玩得尽兴了。新进场的赌徒从老赌徒手里高价买回了筹码，老赌徒大赚了一笔，新赌徒玩的尽兴。赌场呢？手里剩的筹码也可以100美元的价格卖出去了。 由于赌场运营得越来越好，来的赌徒越来越多，赌局越来越大，筹码的价格就一路上涨。 对于赌场而言，由于一开始有大量的筹码以1美元的价格卖给了老赌徒，如果按照100美元的价格承兑，赌场就亏大了。于是，赌场就宣布，找我承兑筹码可以，但是只能按照1美元的价格。 但是赌场在卖出筹码的时候，依然按照市场价100美元出售，这也成为赌场盈利的一种手段。于是大家都不会把赢来的筹码拿去找赌场退钱，而是留在手里等着卖给新的赌徒。 赌场例子背后的经济学常识随着生产力越来越高，市场越来越大，市场上流通的商品（总价值）越来越多，如果货币的流通速度不变，理论上就需要更多的货币。从赌场故事中，可以非常容易地看出，由于赌场固定了筹码的总额，加上很多人持币等待升值造成筹码流通速度变化不大，于是筹码价格只能上涨。 进行一个赌场ICO 又有一个人，看到了赌场的生意这么好，眼红了，于是他也想开一间赌场。可是他没有足够的本钱，怎么办呢？于是他公开宣布：我要开一间赌场，赌场服务非常好，赌局非常好，手续费非常低。我的赌场只有10000个筹码，以后绝不增加。 现在筹码先造出来了，我把其中5000个筹码拿出来，任何人都可以来买。这5000个筹码卖完了以后，我就用卖掉的钱作为本钱开赌场。等赌场开业了，你们可以把手里的筹码卖给赌徒，或者自己来赌都行。 由于开始赌场还没开业，所以出售的筹码价格也是很低的，0.5美元1个。等赌场开业了，至少1美元1个，甚至更高。由于很多人都看好这个生意，于是5000个筹码很快就卖光了。这个人筹集到了2500美元，造了一些赌具，租了间房子，赌场就开始营业了。 由于他的服务真的很好，很快又聚集了大量的赌徒，于是筹码的价格一路飞涨，很快就到了100美元。在赌场开业前以0.5美元的价格买了筹码的人，筹码增值了200倍。 我们看到，其中加粗的环节都是预先设计或者是预想的。并且这个案例和加密货币（区块链）的特别不谋而合 ICO英文全称是Initial Coin Offering，翻译成中文是“初始货币供应”。 ICO的基本原理 公司创造了一种商业模式，在这种商业模式里，大家只能使用公司自己发行的“代币”（虚拟货币）进行交易。公司事先宣布，这种代币总额是固定，或者增发的方式是固定，也就是说，任何人都不能更改代币增发的规则（区块链技术从本质上保证了这一点）。 如果大家认为我们的商业模式非常有前途，我们的代币就会增值。现在公司拿出一定比例的代币进行发售，用筹得的经费作为本钱来运营这种商业模式。 这种依靠出售日后商业模式中的某种公司产品（如果代币可以视为公司产品的话）的方式来筹集资金的金融行为，就被称为ICO ICO的特点 1、所发行的代币必须是在未来的商业模式中有使用价值，并且不可替代。 2、ICO的商业模式中，代币的发行方式必须事先固定规则，并且不可更改。 3、ICO虽然是一种商业融资的方式，但是ICO模式并不出让股权或者负债，也就是说，运营该商业的公司，未来仍可以继续出让股权或者举债 ICO与区块链如果你之前已经对区块链最大的核心思想了有了了解，你就知道ICO里面有一个C coin，本身就是看中了区块链没有任何一个中心能够控制这个系统，数据一旦产生便不可更改，并通过工作量证明，产生的强大信任。 但是，这个加密货币的实体在未来到底能不能变现，必须要落到实际问题中，真真正正可以解决人们生活中的实际问题。 加密货币直接能解决问题是金融体系的一些问题，这在布罗德本特的三个问题中已经进行了详细的阐述。正因为金融体系的局限性，这种货币不可能同时出现很多并且都具有那么高的价值。 那就需要以太坊这种本质上的服务类落地的想法来晚上这一落地过程，这部分的一些创新和面临的技术难题，在如何评估竞争币的价值与智能合约中我进行了一切探讨 数字加密货币监管监管事件 【2017年8月】中国严打数字加密货币交易所和ICO 【2018年1月】中国政府开始叫停数字加密货币的场外交易和“出海转内销” 日本对加密货币交易非常宽容，甚至有一定大财团入场的情况发生 【2017年1月】Coinbase上线比特币现金交易功能，造成流动性溢价 【2018年1月】中国政府要求境内挖矿企业有序退出，一些“矿池”准备搬到加拿大 市场操纵现阶段，40%的比特币被1000个以下的地址持有，代币过于集中，出现市场操纵，不说做空和对冲，典型的简单手法即：若干代币持有大户一起达成共识，提高代币价格，吸引散户投资者进入，然后大户集中减持。俗称【割韭菜】 违法地带加密货币从一出现开始，就是黑市的青睐交易手段，数字加密货币被用来洗钱，绕开资本管制，逃税，赌博等 总结加密货币在现阶段不可能取代法币（货币）的地位。人与人之间就有固定的地位差距，贫富差距，天赋差距，运气差距都有固定的隔阂，本身去中心化的分布式存储系统：区块链，并不能真正的做到全世界范围脱离政府，脱离权利中心的私有财产神圣不可侵犯 我作为一个学习CS专业，学习机器学习的人来说，对于金融的理解十分浅显，希望各位前辈能多多指教，共同探讨！ 以上！鞠躬！ 【参考】 与区块链和数字货币有关的思考","tags":[{"name":"BlockChain","slug":"BlockChain","permalink":"https://charlesliuyx.github.io/tags/BlockChain/"},{"name":"FinTech","slug":"FinTech","permalink":"https://charlesliuyx.github.io/tags/FinTech/"}]},{"title":"【区块链】现代区块链与新技术","date":"2017-09-25T18:54:40.000Z","path":"2017/09/25/现代区块链与新技术/","text":"【阅读时间】不断更新的wiki博客【内容简介】总结市面上不同的币种，关注区块链技术的最新进展，整理总结各种新名词，新概念，新技术相关概念和文章 竞争币和竞争块链因为比特币的完全开源，所以基于比特币，有创新的，无创新仅仅修改一些参数的，出现非常多的竞争币和竞争块链（与比特币和区块链的关系一样，两者联系紧密） 竞争币区别与比特币主要是以下三点： 货币策略不同 基于工作量证明的一致性机制不同 一些特殊的功能，比如更强的匿名性等 如果想关注所有竞争币（机密货币）的信息，可以访问：MaoOfCoin，打开会有所有你想知道的信息 评估竞争币的价值我们可以从问几个最基本的问题开始入手 这款竞争币有没有引入重大创新？ 如果有，那么这项创新是不是足够有吸引使用比特币的用户转移 这款竞争币是不是致力于某一细分领域或引用？ 这款竞争币可以吸引到足够的矿工来抵御一致性攻击吗？ 更多的，关于财务和市场的问题： 这款竞争币的市场总值是多少？（一般最初来源于ICO） 整个系统的用户/钱包规模大概是多少？ 接受其支付的商家有多少？ 整个系统每日的交易数是多少？ 交易总量是多少？ 元币平台 Meta Coin元币和元区块链是比特币之上实现的软件层，也可以认为是覆盖在比特币系统之上的平台/协议，或者是一个币中币的实现。 这些功能扩展了核心比特币的协议，使得比特币交易和比特币地址附加信息称为可能 彩色币 Colored Coin通过仔细跟踪一些特定比特币的来龙去脉，可以将它们与其他的比特币区分开来，这些特定比特币就叫作彩色币。 它们具有一些特殊的属性，比如支持代理或聚集点 ，从而具有与比特币面值无关的价值。 彩色币可以用作替代货币、商品证书、智能财产以及其他金融工具，如股票和债券等。 彩色币本身就是比特币，存储和转移不需要第三方，可以利用已经存在的比特币的基础，因此彩色币可以为现实世界中难以通过传统方法去中心化的事物铺平道路。 万事达币 MasterCoinMasterCoin是比特币协议的应用层协议，类似HTTP协议是TCP协议的应用层一样 货币属性不同于比特币的竞争币比特币自身所具有的一些设计规则使得它成为了一个总额固定并且不通胀的虚拟货币，有一些竞争币通过对这些货币属性的微调，来达到实现不同的货币政策的目的。 莱特币 LiteCoin它是最早的一批竞争币的一员，自2011年发布至今，已经成为继比特币之后的第二成功的电子货币。它的主要创新是两点 使用了Scrypt作为工作量证明的算法（这种算法比SHA256来说，主要的特点就是内存消耗更多，难度更大，使用ASCII或者GPU矿机更加难以计算） 更快的货币参数 狗狗币 DogeCoin它是基于莱特币的一款竞争币，与2013年12月发布。之所以值得一提还是因为它飞快的出块速度和惊人的货币总量。现在已经一文不值 Freicoin于2012年7月发布。它是一种滞留性通货，可以理解为存在钱包中的货币的利率为负数，只有不断交易和消费才能保证它不变少。它的特点是它的货币政策正好和比特币的通货紧缩政策相反 货币属性总结表 货币名称 出块速度 货币总量 一致性算法 市场总值（到2017/9/25） 24小时交易量 莱特币 2分半 2014年8400万 Scrypt $2,716,920,950 $214,787,000 狗狗币 60秒 2015年达1000亿 Scrypt $123,105,955 $11,777,800 Freicoin 10分钟 2014年1亿 SHA256 $108,614 $1 一致性机制创新的竞争币在时代的发展中，除了SHA256找0的方式，还衍生出了不同的算法来实现工作量证明的一致性机制。包括Scrypt；Scrypt-N，Skein，Grosetl，SHA3，X11，Blake。这些算法都在一定程度上组织的ASIC矿机的泛滥 而在2013年，出现的一种替代方式：权益证明（或股权证明 Proof of Stake PoS），称为现代竞争币的基础 在权益证明系统中，货币的所有人可以将自己的货币做利息抵押。类似于存款证明，参与者可以保有他们货币的一部分，通过利息和矿工费的方式获取回报 Peercoin与2012年8月发布，首款工作量证明和权益证明混用的竞争币 Myriad与2014年2月发布，它同时使用了5中工作量证明算法（HA256d；Scrypt；Qubit；Skein；Myriad-Groestl），根据矿工的情况动态选择。这也是为了让系统不受集中化ASIC矿机的影响，也加强了其抵御一致性攻击的能力 黑币 BlackCoin与2014年2月发布，使用的是权益证明的一致性机制。同时，它引入的可以根据收益自动切换到不同竞争币的“多矿池”机制也值得关注 VeriCoin与2014年5月发布，它使用权益证明机制，并用市场供需关系动态调整利率。它是首款可以直接在钱包中兑换比特币支付的竞争币 NXT发音同Next，一种纯粹的权益证明竞争币，根本不采用工作量证明的挖矿机制。 它是一款完全自己实现的加密货币，并非衍生品。NXT具有很多先进的功能，包括名字注册、去中心化资产交易、集成的去中心化加密信息的权益委托。NXT也被称为加密货币2.0 货币属性总结表 货币名称 出块速度 货币总量 一致性算法 市场总值（到2017/9/25） 24小时交易量 Peercoin 10分钟 没有上限 工作量证明和权益证明混用 $31,009,448 $395,757 Myriad 30秒 2024年到20亿 多重算法 $3,758,612 $59,805 Blackcoin 1分钟 没有上限 权益证明机制 $13,817,132 $1,180,000 VeriCoin 1分钟 没有上限 权益证明机制 $10,103,100 $180,508 NXT 1分钟 1亿 权益证明机制 $63,052,381 $2,330,510 多目的挖矿创新比特币的工作量证明机制的目的是：维护比特币系统的安全，构建去中心化的信任。很多人认为挖矿这一行为是一种浪费。（这个观点个人持保留态度） 多目的挖矿算法就是为了解决工作量证明导致的“浪费”问题而出现的。多目的挖矿的在为货币系统的安全加入额外需求的同时，也为该系统供需关系加入了额外的变量 PrimeCoin与2013年7月发布，它的工作量证明算法可以搜索质数，计算孪生素数表。很有意思的，随着PrimeCoin新区块的不断产生，会不断的发现新的素数，构造一个科学成果：素数表 CureCoin与2013年5月发布。它把SHA256工作量证明算法和蛋白质褶皱结构的研究结合起来。蛋白质褶皱研究需要对蛋白质进行生化反应的模拟，用于发现治愈疾病的新药，但这一过程需要大量的计算资源 GridCoin与2013年10月发布。它结合了Scrpy为基础的工作量证明算法和参与BOINC计算项目的补贴机制。BONIC是伯克利发开的网络计算系统，算力是提供给这个平台的 货币属性总结表 货币名称 出块速度 货币总量 一致性算法 市场总值（到2017/9/25） 24小时交易量 PrimeCoin 1分钟 没有上限 计算素数 $2,750,679 $691,788 CureCoin 10分钟 没有上限 蛋白质研究 $5,078,597 $80,012 GridCoin 150秒 没有上限 BONIC $12,920,406 $91,290 致力于匿名性的竞争币其实比特币的地址和显示中真是个人的关系还是比较容易通过大数据的分析手段计算出来的，所以有一些加密货币希望能在这方面有突破 ZeroCoin/Zerocash还在开发当中 CryptoNote它提供一种以电子货币为基础的匿名性的参考实现，与2013年10月发布。它可以被克隆继而衍生出其他的实现，并内建了一个周期性的重置机制使其不能作为货币，很多竞争币基于它实现：入ByteCoin，Aeon，Boolberry，DuckNote，FantomCoin，Monero，MonetaVerde和Quazarcoin ByteCoin与2012年发布，是CryptoNote的第一个实现，之前还有一个同名的ByteCOin电子货币，BTE，这个为BCN。 ByteCoin使用了基于CryptNote的工作量证明机制，每个实例至少2MB的内存，是的GPU和ASIC矿机无法在Bytecoin中运行，它继承了CryptoNote的环签名、不可链接交易和块链抗分析匿名性等机制 Monero货币区县比ByteCoin平缓，在系统运行最开始的四年发型80%的货币 货币属性总结表 货币名称 出块速度 货币总量 一致性算法 市场总值（到2017/9/25） 24小时交易量 ByteCoin 2分钟 1840亿 基于CryptoNote $237,302,332 $1,501,490 Monero 1分钟 1840万 基于CryptoNote $1,395,218,943 $28,579,800 非货币型竞争区块链这些区块链设计模式有着自己独特的设计模式，并不主要作为货币使用。当然不少这种区块链也含有货币，但只不过它们的货币仅是一种象征，用于分配其他东西，比如一种资源或者一份合约。 域名币 NameCoin它是一种使用区块链的去中心化平台，用来注册和转让键-值对。也就是域名注册。现在，.bit的替代性域名服务（DNS）就是使用这个系统来完成。 用它注册的域名空间不受限制，人和人都可以以任意方式使用任意的命名空间 Bitmessage它是一种去中心化安全消息服务的比特币竞争区块链。其本质是一个无服务器的加密电子邮件系统。 Bitmessage可以让用户通过一个Bitmessage地址来编写和发送消息。这些消息的运作方式与比特币交易大致相同，区别在于，消息的保存是有时间时间限制的，如果两天还没被送到目的地，就会消失。 Bitmessage的好处是可以抵御全面监视，除非网络偷听者破坏了接收方的谁被，否则无法截取邮件信息 以太坊以太坊是一种图灵完备的平台，基于区块链账本，用于合约的处理和执行。它不是比特币的一个克隆，而是完全独立设计和实现的。币用来付合约执行的花费。 以太坊区块链记录的东西叫做合约，所谓合约，就是一种低级二进制码。本质上，合约是运行在以太坊系统中各个节点上的程序。这些程序可以存储数据、支付及收取、存储币以及执行无穷范围的计算行为，在系统中充当去中心化的自制软件代理 货币属性总结表 货币名称 出块速度 货币总量 一致性算法 市场总值（到2017/9/25） 24小时交易量 NameCoin 10分钟 2140刀2100万 SHA256 $20,536,647 $125,689 Ethereum 14秒 1亿 转POS中 $27,458,433,693 $459,104,000 权益证明什么是权益证明权益证明（又称股权证明），英文单词 Proof of Stake，缩写PoS，与之平级的概念是工作量证明，Proof of Work，这个我们应该很熟悉了 PoS会根据你持有货币的量和时间，给你发利息。持有货币的时间，被称之为“币龄”，每个币每天产生1币龄。 例如，你持有100个币，总共持有30天，你的币龄就是3000，此时，若你发现了一个PoS算法支持的区块（直观来说就是你拥有币的数量和时间越长，你新建区块的几率越大），你的币龄就会被置0。你每被清空365币龄，你将会从区块中获得0.05个币的利息（相当于年利率5%），那么在这个案例中，利息 = 3000 * 5% / 365 = 0.41个币。 这个利息的数量也是不同的币种自己来设定的，变为一种货币属性 也就是说，你的“挖矿”收益将正比于你的币龄，与算力无关 这种新的一致性算法不要求证明完成一定数量的计算工作，而是要求证明者对某些数量的钱展示其所有权。中本聪没有这么做的原因其实是因为当年还没有一个去中心化的能展示个人财产的方式，而现如今，有一个展示财产的方式就是比特币本身。 为什么要设计PoS设计权益证明的初衷其实是为了解决比特币本身规则的几个痛点，这也是PoS的设计者，或者很多人公认的一些原因的概括，个人观点上并不完全认同 挖矿动机衰减比特币每过240000个区块，区块奖励（Coinbase）就会减半。很多人对此表示担忧，他们认为在未来因为奖励的减少导致大家挖矿的动力越来越低，整个比特币网络就会陷入瘫痪（一种情况是大家都减少运行比特币客户端的时间，P2P可用链接节点就越来越少） 【解决方案】 在PoS体系中，只有打开钱包客户端程序，才能发现PoS区块，才能获得利息，这促使很多不想挖矿的人，也会打开钱包客户端（为了获得利息） 共识攻击按照第一条的逻辑，随着矿工的减少，比特币很有可能被一些高算力的人进行51%攻击，导致整个比特币网络攻击。个人观点是，只要比特币还有价值（甚至的对应电费的价值），世界上的人类贪婪的本性不会变，只有有利可图，或者利益足够吸引人，马上会有足够多人的去挖矿，所以会维持一种诡异的平衡。这个问题在各种PoS的说明书上被反复提到，个人观点是：不应该设想一种情况的发生，而是详细的从用户动机的角度来论证和思考，你很自然的通过现象得出一个结论，这种做法是很武断的 【解决方案】 当然，在PoS体系中，利息产生的货币是保存在PoS算法的区块中，直观结果是，这会要求攻击者还需要持有超过全球51%的货币量才可以，从侧面来说提高了攻击的难度 通货紧缩体系比特币因为货币总量和丢失等问题，会导致通货紧缩 【解决方案】 对于PoS体系来说，可以通过调整年利率的方法来调控整体的紧缩和通胀状态。但是从侧面来讲，通货紧缩和通货膨胀都是经济学问题，说白了，是一种经济体（国家）运行状态，其中涉及到十分复杂的学问，对于加密货币本身来说，还远远没有到需要讨论紧缩和通胀的时候 智能合约自2017年以来，随着IBM，微软两大巨头的加入，以及各大银行的支持，区块链+智能合约的解决方案越来越受到大家的关注和重视。 什么是智能合约合约合约的概念即合同，一个承诺，规定一个规则，大家必须遵守，并设定违反惩罚机制，这就是合约。但我们知道，合约的执行必须要有权威中心机构背书，在现代区块链实现点对点信任的基础上，合约的执行过程从某种程度来说，可以去中心化。从另一方面来说，提高了很多办事情的核心效率。 为什么这样说呢？因为现实生活中，我们对交易对象，合作伙伴，甚至婚姻都是不能说完全信任的，一方面因为人的善变特质，另一方面因为事物随时间进程推进的不确定性（比如车祸，或者不可预知的特殊情况）合约在当今社会是一个巨大的市场，没有合约，寸步难行，这么说，一点也不为过。而市场巨大的价值就是其提供的信任，而所谓智能合约，智能而字除了自动执行外，更加包含了一种由区块链本身原理带来的信任，这也是其本质价值所在 智能合约这个名词或者说概念，提出的了时间非常早，是由尼克萨博于1996年提出的，定义为： 一个智能合约是一套以数字形式定义的约定，包括合约参与方可以在上面执行这些约定的协议，智能合约的基本思想是，各种各样的合约条款，可以嵌入到我们所使用的硬件和软件中，从而使得攻击者需要很大的代价去攻击 你可以想象，最简单的智能合约是就是一台自动售货机。合约内容是，你给他钱，他出商品，非常简单的内容。 复杂的合约，可以想象，现在Uber的所有打车过程都是由Uber的服务器完成，并且使用第三方支付系统来付款，如果使用智能合约变成“去中心化的Uber”，场景就变为，每一次乘车，以一种没有破绽的逻辑来触发乘车交易这一智能合约的内容，那么就可以完全省去付款的部分，合约自动打钱（当然这里打的钱只是区块链的上加密货币）。当然这只是强行设想的一种非常复杂的智能合约形式，在现实中，基本上是完全无法实现的。 智能合约（用于实现事务的业务规则结合后的产物）实际上是一种过程调用（Procedure call），可在网络中多个节点上运行，运行后输出的结果通过合意（Consensus）过程被所有网络成员认可 个人观点是：区块链+智能合约必须对应特定场景，特定需求，才可能有应用价值，比如对信任十分需求的场景，而传统解决方案中为了信任本身需要花费很大的代价或效率十分低下。 从最近各大银行的动态，还有新闻来看，银行贷款，保险，供应链管理，合规领域都是很有潜力的 但是智能合约+区块链面临的问题也很多，截止今天，也有很多项目或技术在为了落地它而努力 比如执行速度，执行成本，可扩展性，匿名性，和现实世界的隔离性，还有政策和法律的监管等等问题。 以太坊技术栈Solidity是一门需要学习的开发语言 在应用层来说，还有对应网络部分的web3.js作为一个和网络部分交互的JavaScript API 还有一些编译器（Solc），机器层面的（EVM）的工具，社区Library（Zepplin）等 热点名词或概念总结这个版块不定期更新，主要作为新技术的一个总结窗口 RootStockRootStock 是一个建立在比特币区块链上的智能合约分布式平台。它的目标是，将复杂的智能合约实施为一个侧链，为核心比特币网络增加价值和功能。RootStock实现了以太坊虚拟机的一个改进版本，它将作为比特币的一个侧链，使用了一种可转换为比特币的代币作为智能合约的“燃料”。 侧链 Sidechain楔入式侧链技术（ pegged sidechains），它将实现比特币和其他数字资产在多个区块链间的转移，这就意味着用户们在使用他们已有资产的情况下，就可以访问新的加密货币系统。目前，侧链技术主要是由Blockstream公司负责开发。 闪电网络 Lightning Network闪电网络的目的是实现安全地进行链下交易，其本质上是使用了哈希时间锁定智能合约来安全地进行0确认交易的一种机制，通过设置巧妙的‘智能合约’，使得用户在闪电网络上进行未确认的交易和黄金一样安全（或者和比特币一样安全）。 超级账本 Hyperledger超级账本（hyperledger）是Linux基金会于2015年发起的推进区块链数字技术和交易验证的开源项目，加入成员包括：荷兰银行（ABN AMRO）、埃森哲（Accenture）等十几个不同利益体，目标是让成员共同合作，共建开放平台，满足来自多个不同行业各种用户案例，并简化业务流程。 由于点对点网络的特性，分布式账本技术是完全共享、透明和去中心化的，故非常适合于在金融行业的应用，以及其他的例如制造、银行、保险、物联网等无数个其他行业。 通过创建分布式账本的公开标准，实现虚拟和数字形式的价值交换，例如资产合约、能源交易、结婚证书、能够安全和高效低成本的进行追踪和交易。 如果想进一步具体案例，这篇文章可能可以帮到你，使用Hyperledger Composer十分钟搭建区块链概念验证环境 面向商业的区块链基础设施平台现在最常见四大区块链技术开源平台，Ethereum、来自IBM的Fabric、Corda和BCOS 在四个开源技术平台中，Ethereum代表的是公有链技术，Fabric、Corda和BCOS代表的是联盟链，即多个机构联合创建，需要身份验证的半公开“受控”系统。 在公有链、私有链还是联盟链中选型，取决于开发者和应用场景的需求。对于“安全”有特殊需求的金融机构和企业级应用来说，联盟链的低风险与高可控，最有利于说服法律部门和监管者 OpenBazzarOpenBazzar是一个结合了ebay与BitTorrentt特点的去中心化商品交易市场，使用比特币进行交易，既没有费用，也不用担心受到审查。 因此相对于易趣与亚马逊这些提供中心化服务的电子商务平台，通过OpenBazz不需要支付高额费用、不需要担心平台收集个人信息致使个人信息泄露或被转卖用作其他用途。 2015年，获得了由科技行业的两大风投公司Andreessen Horowitz和Union Square Ventures 投资。 关于如何使用OpenBazzar建立新的交易推荐文章：什么是OpenBazzar","tags":[{"name":"BlockChain","slug":"BlockChain","permalink":"https://charlesliuyx.github.io/tags/BlockChain/"},{"name":"Ethereum","slug":"Ethereum","permalink":"https://charlesliuyx.github.io/tags/Ethereum/"}]},{"title":"【区块链】一文看懂区块链：一步一步发明比特币","date":"2017-09-24T22:09:42.000Z","path":"2017/09/24/一文弄懂区块链-以比特币为例/","text":"【阅读时间】29 ~ 35 min | 11872 words【内容简介】此文潜在面向群体是对区块链或比特币的运行原理完全不了解的人。所以会从用户需求的角度出发，一步一步发明区块链（或者说比特币，因为两者互相依存），在此之后的内容有关比特币与金融，ICO，竞争币等，可选择性阅读。 如果有帮助求知乎点个赞，感谢！ 因为贪婪，所以信任 加密货币在一步一步发明发明比特币之前，解释几个直观的认知： 我们常说的比特币，是加密货币（Cryptocurrency）的一种，而加密货币实现去中心化的最关键的技术是区块链 有些地方可能把加密货币又称为数字货币（或称电子货币），但实际上，加密货币是数字货币的子集，同为子集的还有虚拟货币（如Q币），加密货币的称谓要更加专业 加密货币一定具有下列三个特点 去中心的清算 分布式的记账 离散化的支付 为了实现这些特点，需要使用到区块链技术。这里的区块链技术是一个很广义的范畴，它包含了密码学，算法等很多不同的内容，其中最精彩的点子，可算是工作量证明 = 共识了 还有另一个层次更高的角度：从解决拜占庭将军问题的角度入手来分析和科普【区块链技术】的（可参看我的另一篇博客【区块链】如何解决拜占庭将军问题），我觉得这个角度更加接近于区块链哲学的本质，毕竟，比特币具有价值属性和交易属性，和金融有紧密的联系，但是区块链是一种分布式存储系统，若仅仅从比特币（代币）的角度来解释，非常片面。毕竟，比特币，就是分布式去中心化共识记录系统中的一个字段 一步一步发明比特币第一个用户需求 - 账本和电子签名的由来 第一个用户需求描述了中心化清算系统几个关键内容的由来，只对区块链感兴趣的读者可以跳过 经济体的蓬勃发展离不开交易。在交易过程中，人们早已发现使用一般等价物（如金银）十分麻烦，发明了纸币（最早的来自中国，北宋时代四川地区的纸币交子的清算体系，是生产力发展的必然产物，最终的目标是提高生产效率），现如今，人们发现，携带现金也很麻烦 这是第一个基本用户需求：摆脱现金进行交易带来的不便 【解决办法】几个用户使用公共账本记录转账记录，月底结算，账本公开，每个人都可以修改，也就是说可以在上面添加新行（一笔交易），如小明转账给小红10块钱 产生的问题1：身份问题在这个账本条目上我们无法确认交易双方小明和小红是否是本人，可能出现伪造（逍遥法外电影中的伪造支票） 【解决办法】使用电子签名，即公钥 - 私钥对 记住，电子签名被发明的核心目的是希望在电子文档也能有一个类似与现实中个人笔迹的签名，目的一定是：确认写这个签名的人是本人，即身份确认（验证） 私钥顾名思义，也叫做密钥，是你本人需要需要妥善保管和保存的$$Sign(\\text{信息},私钥) = \\text{电子签名}$$Sign在这里是一个函数，可以理解为一连串计算（变换），这一连串计算有一个特点，就是输入值只要改变一点点，输出就会完全改变。信息和私钥一起，可以得到一个电子签名。并且这个电子签名不能被轻易的复制到其他信息里，原因是因为每一个电子签名都和这一段信息有关联。$$Verify(\\text{信息},\\text{电子签名},\\text{公钥}) = \\text{真/假}$$在进行验证的时候，Verify也是一个函数，输入值是信息，电子签名，公钥，输出是一个True or False，来判断这个电子签名是真的还是假的。 这个时候可能有人就要问了，这个电子签名我难道不能试出来吗？很不幸，这是一个有256bit的1/0字符串，可能性是2的256次方，2的10次方是1024，我只能告诉你这么多了。 解释完电子签名，我们来看看实例。小明使用自己的私钥加上小明转给小红10块钱这段话通过Sign函数生成一个签名（256位），把签名放在这条转账信息的后面，通过之前的讲解，这个签名就能保证小明已经过目了，并且说：“这真的是我小明，不用怀疑了！肯定是我” 直观结果是，我们可以利用密码学的手段，只要有对应人的数字签名，我们保证小明和小红的身份能被100%确认真实 但是这个解决方案有一个小漏洞：可以复制同一行信息来伪造交易记录，解决的办法是添加一个这笔交易独有的信息（比如时间戳） 产生的问题2：欠债跑路问题如果小明在此时账户上已经没有足够的余额进行支付，就会出现超支问题 【解决办法】添加余额记录，此时就不可避免的需要一个中间担保人（国家？信誉机构？银行？）为小明进行余额担保 一个大家都遵守的协议此时，现代金融体系的框架基本建立完毕，协议内容是 任何人都可以在账本上添加新行 固定时间间隔时用真金白银进行清算 只有有签名的交易是有效的 中间担保的人保证不可超支 此时发现一个很有趣的发现，这个比较严谨的协议有一个特点：如果所有人都按照这个协议来办事，我们可以用任何形式的东西来代替人民币了，换句话说，就是我根本不关心你在账本上添加的新行的交易内容是什么，可以是任何东西 利用这个提出需求再解决问题的过程，强化一个认知：货币 = 交易记录（账本），即货币的本质是交易记录，在这背后，有一个前提是，货币的另一个本质是一种共识，我们都信任它有价值的共识 第二个用户需求：账本放在哪里？传统的（现在的）解决方案当时是，使用中心代理-银行，来存放账本 既然是第二个用户需求，那肯定就是因为现在的解决方案大家都不满意 核心需求：去中心化中心化的痛点大致可以说几点 银行效率低下，一笔跨国转账的等待时间较长 胖银行金融体系因部分准备金制度等等方便的规则，能抬升杠杆，产生金融泡沫，进一步诱发金融危机 私有财产神圣不可侵犯是精英与平民，剥削与被剥削者几个世纪以来博弈的风暴中心 当然还有很多没有提到（比如好处，控制经济发展速度，调控供需平衡等），总之，是一种一直饱受诟病的清算方式，此时，中本聪在2009年横空出世，他提出了一种全新的清算方法，并且真正解决了陌生人间信用的问题！接下来就是真正的一步一步的发明比特币了 如何实现分布记账（去中心化）为了去中心化，我们可以反其道而行之：每个用户保存账本，分布记账。用户产生一笔交易就将这笔交易广播到到网络上所有的节点上，这样不就完美的去中心化了？ 只要是明眼人都能发现，太天真的，这个方法行不通。若行不通，那就把行不通的原因总结出来 遇到问题，总结不可行的原因，寻找解决方案。这是整个人类不断前进的核心最小单位 问题核心如何让所有人都同意这个新账本？如何保持这些账本同步？有一笔交易发生时，如何让其他人都听到并相信这一笔交易呢？ 这些问题才是真正的核心：是否能在协议（办法，规则）中添加几行，找到办法，来决定是否接受交易，并确定交易顺序，使你可以放心的相信，世界上遵守同一协议的所有人手上的账本都和你的一模一样呢？（问题描述值得品读，只有抽象出问题才能更好的去寻找解决方案） ☆解决方案解决的思路是：哪个账本的计算工作量大，就信任哪个账本。换个角度来说是【让交易欺诈和账本不一的情形的计算力成本高到不能接受甚至完全不可行】 1、密码学：哈希函数哈希函数，输入可以是任意信息或者文件，输出是固定长度的比特串。例如256bit的1/0串，这个输出叫做这个信息的“哈希值”或者“摘要”（digest）。SHA256就是一个哈希函数 密码哈希函数有几个特点 特点是输入值稍微变化后，结果就会有很大的不同，完全无法预测不同输入间的规律 逆向计算不可行，只能使用试错法（穷举法），解空间 $2^{256}$ 在每一个账本后添加一个特殊数字，对整个列表使用SHA256，我们要求这个特殊数字可以使得输出值的开头有30个零（关于如何确定0的个数问题，在后面部分有详细的说明） 根据之前说过SHA256的性质：输入变化输出不可预测，找到这个特殊数字唯一的办法就是穷举。换言之，你很容易就证明了他们进行了海量的计算。而这个特殊数字就叫做工作量证明（proof of work） 这就意味着，所有的工作量证明就对应了交易列表（账本 Ledger），如果你修改了一个交易，哪怕只是其中一个字符，就会完全改变哈希值，就得重做工作量证明，直观动图如下 修改后的重新计算 2、区块链 - 信任与共识的基石每一个小账本被称为区块，每一个不同的区块链协议（产生不同的加密货币）都会规定每一个区块的大小（最初比特币为1M） 账本组成区块，区块构成链表，区块的头包含前一块的哈希值，这就是区块链 区块链的诞生 如此一来，任何人就不能随意修改其中的内容，或者交换顺序。如果你这么做，意味着你需要重新计算所有的特殊数字 修改任何部分都以为着重新计算 规定，允许世界上的每一个人建造区块。每一个新建区块的人（找到了这个特殊数字 - SHA256值有30个零）都能获得奖励，对于新建区块的这部分人（矿工）来说 没有发送者信息，不需要签名 每一个新区块都会给整个币种增加新的虚拟（加密）货币 新建区块的过程又被称为“挖矿”：需要大量工作量并且可以向整个经济体注入新的货币 挖矿的工作是：接受交易信息，建造区块，把区块广播出去，然后得到新的钱作为奖励 对每个矿工来说，每个区块就像一个小彩票，所有人都在拼命快速猜数字，直到有一个幸运儿找到了一个特殊数字，使得整个区块的哈希值开头有许多个零，就能得到奖励。我记得有一个知乎答主给了一个形象的比喻，区块链就像一个拥有貌美如花女儿（区块）的国王，有很多的青年翘首以盼，而国王的方法是出了一道很难得题目让所有的青年计算（学习改变人生），谁算的快（在计算哈希值过程也可能是运气好）就能抱得美人归 对于想用这个系统来收付款的用户来说，他们不需要收听所有的交易，而只要收听矿工们广播出来的区块，然后更新到自己保存的区块链中就可以了 3、51%算力-共识攻击这里有一个小漏洞，因为网络的延迟或者有人在篡改区块链等因素，你作为一个收听网络广播的用户，如果同时接受到两条不同的区块链怎么办？其中的交易信息发生了冲突 注：区块链本身就是最终的大账本，发生交易的唯一方法就是把你的交易加入到大账本上。具体来说，就是让矿工把你的交易记录加入它新挖到的区块中，并把这个区块链接到区块链上。链表的纽带，当然就是工作量证明 对于上面的问题，用户的解决方案也比较简单：即，只保留最长的且难度系数最高的，也就是包含的工作量最大的那一条 用户保留最长的区块链 这里有一个Trick，即所谓信任工作量最大不仅仅是出【一道难题】，还通过等待多个区块的产生引入世界上所有矿工之间的博弈（吃瓜群众，坐看大戏，谁厉害我选谁，你们尽管斗） 个人观点：区块链的Idea最核心的创新就是从技术上把信任和贪婪画了等号。因为贪婪（希望去竞争建立区块的建立和交易费）所以信任（全网算力越大，用户越放心），这句话甚至带上了些许哲学和传奇的色彩 对于用户来说，是这样一种情景 如何更新本地区块链 其中的原因是，你可以假设Alice希望篡改一个交易信息，那么就意味着Alice需要不断的通过计算维护这个区块链了。也就是说每一次有新的区块链产生，Alice都需要不断的抢到这个彩票，理论上来说，他至少必须拥有全网51%以上的算力才能做到这一点，更多的，随着用户等待区块的增加，这个难度，幂次上升，在7-8个区块链产生后，概率上来讲，就是绝对信任 无穷大的篡改成本 此时 我们用数字签名保证了不能伪造交易记录 用区块链及工作量证明保证了不能篡改其中的信息 这两点，就完成了：证明区块链的每一条交易记录都是可信的这一终极目标 总结 - 系统可行性分析只需给出一个命题来思考：我们如何才能在这个系统下骗人呢？ 如果你想篡改一笔不存在的交易记录，那么你必须比所有人都算的快，赢得这个彩票 但所有用户会继续收听其他矿工的广播 所以为了让所有用户继续相信这个伪造的区块 你必须投入自己所有的工作量，不断给篡改后的区块链分叉增添新的区块 记住：根据协议，所有用户会一直信任他所知道的最长的链 是的，你持续的竞争过世界上所有的矿工的概率或者说代价，实在太大了，得不偿失（其实法律也是一样的道理，它强制给违法的人给予惩罚，让违法者付出他们不能承受的代价了保证公平和社会稳定运行） 注意，作为一个用户，你不能立马相信你所听到的最新区块，而是应该等待多几个区块被创建过后，再确认这的确是世界所有人都在使用的区块链（比特币的原则中，等待6个区块，才确认） 发明过程中的关键点 电子签名 Digital Signatures 公共账本就是货币 The Ledger is the currency 去中心化 Decentralize 工作量证明 Proof of work 区块链 Block Chain 比特币技术到这里，已经发明了比特币，解决了去中心化的信任这一难题。只对比特币和区块链是什么这个问题感兴趣的读者，可以停在这里了，希望大家可以在我的叙述中解决一些困惑！鞠躬！ 针对比特币的一些实现的内在细节，继续在探索和学习的道路上披荆斩棘。新技术，新点子，要拥抱它，使用它，判断它，必须先追根究底了解它 我们知道区块链中记载的核心内容，对于比特币（加密货币）来说就是转账记录。但是，一个概念真正落地成大众可以用的服务，有很多技术上，协议上的细节。接下里的部分主要探讨一些比特币具体实现方面的细节，如网络节点构成，比特币的计算难度系数，比特币总量的由来，比特币一笔交易发生的内部细节等 比特币网络节点的构成比特币网络是一种点对点的数字现金系统（P2P），网络节点中每台机器都彼此对等。P2P网络不存在任何服务端、层级关系或者中心化服务。 节点类型与分工 一个全功能节点包含上述4个模块【钱包Wallet】【矿工Miner】【完整区块链full Block-chain database】【网络路由节点Network routing】 【网络路由节点】使得节点具有参与验证并传播交易与区块信息，发现监听并维持点对点的链接的能力 【完整区块链】具有此模块的节点被称为：全节点。它能够独自自主的校验所有交易，不需要任何其他信息。 【钱包】比特币的所有权是通过数字密钥、比特币地址和数字签名来确定的，数字密钥实际上并不是存储在网络中，而是由用户生成并存储在一个文件或简单的数据库中，称为钱包。有些节点仅仅保留区块链的一部分，通过一种”简易支付验证“（SPV Simplified Payment Verification）的方法来完成交易 【矿工】挖矿节点以相互竞争的方式创造新的区块。有一些挖矿节点也是全节点，可以独立挖矿；还有一些参与矿池挖矿的节点是轻量级节点，必须依赖矿池服务器维护全节点进行工作 拥有全部四个模块被称之为核心客户端（Bitcoin Core），除了这些主要节点类型外，还有一些服务器及节点运行其他协议，如特殊矿池挖矿协议、轻量级客户端访问协议。 下表为扩展比特币网络的不同节点类型 图示 名称 说明 独立矿工 具有完整区块链副本 完整区块链节点 此种节点有时有中继作用，不断收听网络广播，维护完整区块链 轻量(SPV)钱包 移动端，或者不想太过于笨重的桌面端，只需要进行交易广播操作 矿池协议服务器 将运行其他协议的节点，连接至P2P网络的网关路由器 挖矿节点 不具有区块链，但具备Stratum协议的节点或其他矿池挖矿协议的网络节点 轻量 Stratum 钱包 不具有区块链的钱包、运行Stratum协议的网络节点 扩展比特币网络要在全世界的网络中完成整个的交易，下图描述了一个扩展比特币网络，它包含了多重类型的节点、网关服务器、边缘路由器、钱包客户端以及它们互相连接所需要的各类协议，比特币互相连接的接口一般使用8333端口 可以参看这个文章了解Stratum协议，Stratum协议详解 扩展比特币网络 如何控制区块产生速度恒定难度系数我们在发明比特币的过程已经详细说明了工作量证明寻找一个特殊数字使得SHA256函数的输出字符串的前n位是零 对于每一种不同的加密货币来说，都有一个值需要在建立货币的时候时候被定义，即每一个新区块在当前全网算力的条件被发现的【平均时间】，这也是难度系数的由来 比特币10分钟；以太坊15秒；瑞波币3.5秒；莱特币2.5分钟 抛开代码算法层面来说，实现方法就是通过找前n位是0的方法。从概率角度来说，n值越大，意味找到这个这个数的解范围越小。 随着需求0的数目一个一个增加，需要的计算时间将会程指数增长。 那么肯定会问，这个难度值如何动态调整？由谁调整？ 难度调整方式难度的调整实在每个完整节点中自动发生的。如果网络发现区块产生速率比10分钟要快时会增加难度。如果发现比10分钟慢时则降低难度。 例如比特币中的是这样定义的：每2016个区块后计算生成它们花费的时长，比上20160（14天）调整一次。有人可能会问，如果在这十四天内计算能力暴涨怎么办，其实这个10分钟的区块新建间隔的规定也只是一个估计要求，真实情况下，这个时间会偏离10分钟这个设定值很多，但是这种偏差并不会对整个区块链的运行产生影响 但是有人会问，这个过程是靠脚本（代码）来实现的，还是自己手动调整的呢？答案是，本地挖矿节点根据自己看到的链上信息自己调整。 问题又来了，为何我不自己降低难度，让自己更加容易新建区块呢？其实，因为链上所有节点确认新的区块（只有确认了你才能得到回报）是按照最长链并且计算难度最大来判断的，你如果用很小的难度新加的区块，是肯定跑不赢全网的其他矿工的 比特币总量的由来我们已经知道，矿工没新建一个区块就可以得到一定数量的比特币作为奖励，最开始，一个区块可以得到50BTC的奖励，之后每210000个区块，奖励减半，直到2140年，所有的比特币将会发放完毕，可以得到公式$$Total = 210000 \\times(50 + 25 + 12.5 + \\ldots) = 20999999980 \\approx \\text{2100万}$$而这个规则不同的竞争币种都可以自由设置。但是因为交易费的存在，挖矿的人还是会有收益，否则无法建立新的区块，那么整个比特币网络就瘫痪了 比特币的交易处理能力现在比特币区块链的区块信息我现在直接从BLOCKCHAIN上，在我写下这句话的时候，最新的区块是情况 区块高度 存在时间 交易数量 交易额 创建人(矿池) 大小(KB) 重量(kWU) 486883 3分钟 2126 25992.38 BTC.TOP 999.34 3917.54 486882 23分钟 2422 33926.89 BTCC Pool 1034.39 3996.58 486881 51分钟 358 4480.22 BTC.TOP 191.92 718.14 486880 53分钟 352 3770.26 BTC.TOP 197.27 715.72 其中的重量是指的实际存储的大小，这个值和交易协议有关，其实可以忽略。非常幸运的是，这几个区块放佛是专门为讲解这一节而出现的，这可能是天意吧 另外插一句，你会发现平均区块建立间隔时间，的确和10分钟这个设计值差距很大吧 区块容量比特币区块链从被第一次部署时，或者说源代码中已经规定，区块容量是1M。最初设计成1M的原因一方面，防止DOS攻击。另一方面，当年中本聪在创建区块链的时候的容量是32M，但是他通过一个说明为”Clear up“这样毫不起眼的Commit把区块容量改成了1M，为防止区块链体积增长过快，为区块容量这个问题添加了些神秘色彩。好吧，我承认，中本聪就已经非常具有神秘色彩了，是在神秘色彩上添上了些故事 截止2018年4月1号，完整区块链大小已经有约151GB的大小了 上表中，可以观察到，1M的容量意味着比特币最大的处理交易数量在约2400（486882区块1034.39的大小很接近了），从代码及技术文档来看，一个区块的最大处理交易数量在2700笔，意味着在一定程度上区块利用率可以超过100%。下面再给出一张时间和每秒交易数量的关系图表(交互表格点击链接) 每秒比特币交易数量 【蓝色圆圈的大小】代表的是比特币内存池（mempool）的大小（交易在等待矿工处理之前都会暂时存在这里）直观来说，就是圆越大，在等待的交易数量越多【纵坐标】是每秒交易数量（对数变换后）【横坐标】是时间 一句话总结，这是一个拥堵的网络，重负不堪。 再来看一张比特币交易费和区块使用率之间的关系图(交互表格可以点击链接) Bitcoin Fees VS BlockSize 【蓝色的圈大小】是Mempool的大小，直观来说，就是圆越大，在等待的交易数量越多【横坐标】是区块容量的使用情况【纵坐标】是每一个区块的可以得到的交易费用，单位是BTC，注意这里是一个区块被新建时候所有交易费之和（大约2400交易的所有交易费加起来） 手续费随区块使用率开始增长，甚至出现了10BTC一个区块2400笔交易的情况，意味着挖到这个区块的人通过交易费得到的回报接近了本身建立区块的回报。另，之所以可能有人看到最大交易是2700笔，应该和这个区块链大小为最大值的110%有关，从设计角度上来说，是一种缓冲 有一个结论是，扩容后，因为每一个区块的交易承载量增加，矿工的交易费收入肯定会减少。因为，通过上表可以发现，只有当区块使用程度接近95%时候，交易费才有明显的增长 再看一张用户执行交易需要等待的时长和区块使用比例间关系的图表(交互图表点击链接) Bitcoin Median Confirmation Time VS Bloacksize 【蓝色的圈大小】是Mempool的大小，直观来说，就是圆越大，在等待的交易数量越多【横坐标】是区块容量的使用情况【纵坐标】是用户平均需要等待的时间，单位是分钟。 通过上面三张表我们可以知道，矿工的计算力是整个区块链信用的基石（记住贪婪=信任），所以对矿工的激励不能少，而对于用户来说，当然希望自己的交易越快速完成越好。 对于矿工来说，区块使用率超过95%是一个很好的信号，那意味着可以拿到更多的奖励。奖励太低，在区块建立奖励越来越少的情况下，安全性（信任）就慢慢的得不到保障。这么说来，这也就变成了一个Trade-off博弈过程 分析下来，类似门罗币（menero）实现的根据网络负载来动态调整区块容量的设计似乎很合理 比特币扩容之争这是一场复杂的博弈斗争，使用隔离见证增长区块容量，并出现了比特币现金这个新的币种。 如果想要了解这里面的很多技术，英文是必须过硬的，因为比特币代码开源，可以随意fork，只要英文功底过硬，阅读白皮书，文档等，这些不同技术的处理方法都是能够学到的 比特币的一笔交易过程中到底发生了什么我们可以确认的是，每一笔都将记录在大账本中，那么我们需要研究的内容，就是区块中交易内容内的具体数据结构 交易结构每一个交易块包含的内容如下表所示 大小 字段 描述 4 bytes 版本 明确这笔交易参照的规则 1 - 9 bytes 输入数量 输入值的数量 不定 输入 一个或多个交易输入 1 - 9 bytes 输出数量 输出值的数量 不定 输出 一个或多个交易输出 4 bytes 时钟时间 UNIX时间戳或区块号 最后这个值是解锁时间，定义了能被加到区块链里的最早交易时间。大多是时候设为0，表示立马执行。 一笔比特币交易是一个含有输入值和输出值的数据结构。该数据结构包含了将一笔资金从初始点（输入值）转移至目标地址（输出值）的代码信息。比特币交易的输入值和输出值与账号或者身份信息无关。可以把它理解为一种被特定秘密信息锁定的一定数量的比特币。只有拥有者或者知道这个秘密信息的人可以解锁 交易的输入和输出比特币交易的基本单位是未经使用的一个交易输出，简称UTXO（unspent transaction outputs） 可以把UTXO类比为我们使用的人民币1，5，10，20，50，100的面值，对于UTXO来说，它的面值可以是一”聪“的任意倍数（1BTC等于一亿聪）但是这个有着任意面值的”人民币“不能随意打开，还被加上一道类似红包支付口令的密码，只有拥有这个密码的人才可以使用这个UTXO，UTXO包含，币值+一段代码（锁，只有有钥匙的人才能打开） 被交易消耗的UTXO被称为交易输入，由交易创建的UTXO被称为交易输出 交易输出不同面值的UTXO是由交易输出来提供的。你可以想象你需要购买一个3.1BTC的物品，你并不能从你的钱包中找到几个UTXO来得到3.1BTC，但是你刚好拥有一个4BTC的UTXO，你使用这个UTXO作为付款，那么你需要自己手动构建一个0.9的UTXO返还给你自己。 一个交易输出包含两个部分 一定量的比特币。被命名为“聪”（satoshis） 一个锁定脚本。给这个UTXO上锁，保证只有收款人地址的私钥才可以打开 交易输入每个交易输入是在构造的一笔交易（使用UTXO），比如你需要支付0.015BTC，钱包会寻找一个0.01BTC和0.005BTC的UTXO来组成这一笔交易。交易输入中还会包含一个解锁脚本，这是一个签名，可以类比成支付宝红包密码的口令 交易费交易费 = 求和（所有输入） - 求和（所有输出） 这里有一个比较有意思的地方，就是因为找零的输出UTXO是交易的发起这自己构建的，如果很不幸，你忘记了自己构建找零的UTXO，那么这些多余的BTC就会变成矿工的劳务费 例如，我需要和小明进行交易，需要购买一个商品，花费0.8BTC，为了确保这笔交易能被更快的处理（添加到大账本上），我要在其中添加一笔交易费，假设0.01BTC（忽略人傻钱多），那就意味着这笔交易会需要我从钱包中找到几个UTXO能组成0.81BTC。但如果我的钱包内找不出这样的UTXO，只有一个1BTC的UTXO可用，那么我就需要构建一个0.19BTC的UTXO作为找零回到自己的钱包 交易费只和交易字段使用的字节大小有关，与参与交易的比特币币值无关。UTXO是有尺寸的，比如某人想支付一笔很大的BTC交易，但是他的钱包中有很多小尺寸的UTXO，如果加入了很多个UTXO，就意味着他的交易会变复杂，存储空间需求多。当然，很多钱包会提供这方面的优化功能，保证你的交易使用的字段大小最优化 解锁和锁定脚本在实际实现的时候，这个“支付宝红包口令”被称为脚本，是一种基于逆波兰表示法的基于堆栈的执行语言。具体细节感兴趣的读者可以去比特币的Github研究代码。关于脚本有很多细节上的定义和实现方法，这里限于篇幅不展开描述了 矿工费和优先级我们知道，每一笔交易都是广播到区块链上，由矿工决定是不是加入到新区块上的。那么这里就会涉及到一个问题，谁的交易的优先级更高，是先来后到？还是谁给前多谁就能加入到新区块中？ 在区块容量一节中，有一张图表直观的展示了现在网络中一笔交易的等待时间，其中最长的，也就是30分钟，如果你不是一个超级急性子，很多时候还是可以接受的（毕竟跨国转账1-2个工作日） 优先级 = 输入值块龄 * 输出值块龄 / 交易总长度 一个交易想成为“较高优先级”，需满足条件：优先值大于57600000，等价于1个BTC，年龄为1天，交易的大小为250字节 区块中前50KB的字节是保留给“较高优先级”的，其实这一机制也保证了一笔交易不会等待时间无现长。但是我们要知道，内存池（存放待处理交易的位置）中的交易，如果在没有处理后消失，所以钱包必须拥有不断重新广播未被处理交易的功能 创币交易 - Coinbase每一个新建立的区块，都会有新的比特币作为奖励被产生，这个交易是一个特殊交易，被称为创币交易（Coinbase奖励） 创币交易中不存在解锁脚本（也叫ScriptSig字段），被Coinbase的数据取代，长度最小2字节，最大100字节，除了开始的几个自己以外，矿工可以任意使用Coinbase的其他部分。比如创世区块中，Coinbase的输入中的字段是：The Times 03/Jan/2009 Chancellor on brink of second bailout for banks，是泰晤士日报当天的头版文章标题：财政大臣将再次对银行施以援手。 Merkle树每个区块中的所有交易，都是用Merkle树来表示的。换句话说，交易的存储数据结构是，Merkle树 什么是Merkle树Merkle树是一种哈希二叉树，它可以用来进行快速查找和检验大规模数据完整性。对于比特币网络来说，使用Merkle树来存储交易信息的目的是为了高效的查找和校验某笔交易的信息是否存在 当N个数据元素经过加密（使用两次SHA256算法，也称double-SHA256），至多计算 $2log_2(N)$ 次就能检查出任意某元素是否在树中 构造Merkle树假设我们有A B C D四笔交易字段，首先需要把这四个数据Hash化。然后把这些哈细化的数据通过串联相邻叶子节点的哈希值然后哈希化。基本过程如下图所示 Merkle树的构造过程 叶子节点必须是偶数（平衡树），如果遇到奇数的情况，把最后一个节点自身复制一个，凑偶 Merkle树的效率下表显示了证明区块中存在某笔交易所需转化为Merkle路径的数据量 交易数量 区块的近似大小 路径大小（哈希数量） 路径大小（字节） 16 4KB 4个哈希 128 bytes 512 128KB 9个哈希 288 bytes 2048 512KB 11个哈希 352 bytes 65535 16MB 16个哈希 512 bytes 可以发现，即使区块容量达到16MB规模，为证明交易存在的Merkle路径长度增长也极其缓慢（幂指数增长取对数变为线性增长） Merkle应用 - 简单支付验证节点（SPV）我们知道，每当一笔新的交易产生的时候，我们必须验证这笔交易是否真的存在，在SPV节点中，不保存区块链，仅仅保存区块头。使用认证路径或者Merkle路径来验证交易是否存在于区块中 例如，一个SPV节点需要处理一笔支付，它需要验证这笔交易在某个区块中是否存在，才能决定是不是把这笔交易添加到这个区块中，那么它只需要接收少于1KB大小的，有关区块头和Merkle路径的信息，比接收完整区块（大约1MB）大小少了1千倍。简单来说，可以想象，Merkle树类似一个数组（这也是哈希表的最简单表示），下标是区块字段，下标对应数组存储的内容是这笔交易是否存在的值（True or False） 区块链（比特币）与金融BitInfoChart是我个人觉得最好的加密货币信息网站！可以添加到收藏夹 因为比特币具有价值，那就必须谈到它和金融的关系。 限于篇幅（太长了太可怕了，一个博客写2万字莫不是有病），这部分另起一篇：链接（直接点不会打开新标签），如果对ICO和金融方面感兴趣的读者欢迎移步讨论 竞争币和其他技术创新所谓竞争币当然是利用区块链技术为即使，仿照比特币的基本协议架构进行的创新后的新币种，或者是新的区块链实现模式。这篇文章围绕什么是区块链展开，这部分的内容请移步（持续更新）如何评估竞争币的价值与新技术创新（直接点不会打开新标签） 其中谈到了工作量证明的其他替代手段；到底什么是智能合约；以太坊开发技术栈等 总结感谢您看到这里，写这篇文章的目的一方面也是回答区块链（比特币）到底是什么这个困扰了自己很久的问题，另一方面，也是因为最近区块链技术非常火，需要一些接地气的科普文 比如最近最新的消息称一家保险公司，使用区块链技术来赔偿航班晚点2小时，基于以太坊智能合约第一款落地应用。 如果你已经对区块链的实现原理有了初步的认知，就明白这些应用利用了区块链的分布式特点。说到底，并没有贪婪=信任工作量证明核心，只是一种基于云的新型运用，也很有想法，但是和比特币之类就没什么关系了（当然，这是我的个人看法，最近区块链方面的有很多突破性技术，比如侧链，闪电网络等等，太多的新概念，新名词，新技术，对此，也只能不断学习）。 但你只需牢记，贪婪=信任，以太坊也是利用以太币这个媒介来实现了合约价值，中本聪用人内核的贪婪来给陌生人之间加上了信任的纽带，这个代价是永远不会变的。换句话说，如何抵抗共识攻击和安全漏洞是一个永远不会消失的议题。 一句话来说，万变不离其宗，道生一，一生二，二生三，三生万物。中本聪给了道，是个妙人，但是万物依旧有无穷可能。信任作为一个人类社会一直以来的重要问题（痛点），为了解决它，出现了权威机构进行信任背书（中心化）。建立信任，一定要付出代价，天底下没有免费的午餐，最终这些技术都会回归于一个投入产出的博弈过程（Trade-off），梳理主干，抓住要点，才能游刃有余！ 那么如何才能梳理主干，抓住要点，提升学习能力呢？见谅加一个软广告 幕布是一款笔记本软件，博主参与了部分研发工作，如果你喜爱沉浸式层次化输入，并喜欢思维导图，还喜爱记录总结整理各种书籍或文章，幕布完美切合这三类人群的需求：一键生成思维导图，极简输入界面，快捷键操作，全平台支持！Organize your brain by mubu 点我一键微信注册，一个月9块钱，良心商家，这是一篇自己写的有关幕布的介绍文章 如果感觉看完有帮助求知乎点个赞，感谢！ 以上！鞠躬！ 除了江卓尔 知乎回答等优秀的知乎答主的回答，附参考文献出处：【1】文章中引用多个Gif的出处：比特币原理-3B1B，这也是让我真正弄懂比特币的一个视频，不得不说，外国人在让门外汉入行这件事上，领先了很多【2】一本入门教材。包含代码和实现，以及很多数据结构，具体实现方式的细节，如果想成为加密货币（区块链）开发者，这本书5星推荐：精通比特币【3】宋老师的鸿观125期 （需要优酷会员）【4】只能膜拜之的创世区块作者的论文：比特币白皮书【5】ICO科普文章中的例子引用","tags":[{"name":"BlockChain","slug":"BlockChain","permalink":"https://charlesliuyx.github.io/tags/BlockChain/"},{"name":"BTCoin","slug":"BTCoin","permalink":"https://charlesliuyx.github.io/tags/BTCoin/"}]},{"title":"【直观详解】拉格朗日乘法和KKT条件","date":"2017-09-20T17:49:49.000Z","path":"2017/09/20/拉格朗日乘法和KKT条件/","text":"【阅读时间】8min - 10mun【内容简介】直观的解读了什么是拉格朗日乘子法，以及如何求解拉格朗日方程，并且给出几个直观的例子，针对不等式约束解读了KKT条件的必要条件和充分条件 What &amp; Why拉格朗日乘法（Lagrange multiplier）是一种在最优化的问题中寻找多元函数在其变量受到一个或多个条件的相等约束时的求局部极值的方法。这种方法可以将一个有 n 个变量和 k 个约束条件的最优化问题转换为一个解有 n+k 个变量的方程组的解的问题 考虑一个最优化问题 $$ \\operatorname*{max}_{x,y} f(x,y) \\qquad s.t.\\;\\; g(x,y)=c $$ 为了求 $x$ 和 $y$ ，引入一个新的变量 $\\lambda$ 称为拉格朗日乘数，再引入朗格朗日函数的极值$$\\mathcal{L}(x,y,\\lambda)=f(x,y)-\\lambda \\cdot \\bigl( g(x,y) - c\\bigl) \\tag 1$$ 红线表示 $g(x,y) = c$ ，蓝线是 $f(x,y)$ 的等高线，所有箭头表示梯度下降最快的方向。图中红线与等高线相切的位置就是待求的极大值 How那么如何求这个极值点呢？ 单约束对(1)式直接求微分，并令其为零，计算出鞍点 $$ \\nabla_{x,y,\\lambda} \\mathcal{L}(x,y,\\lambda) = 0 $$ 有三个未知数，所以需要3个方程。求 $\\lambda$ 的偏微分有 $\\nabla_{\\lambda} \\mathcal{L}(x,y,\\lambda) = 0 \\implies g(x,y)=0$，则总结得 $$ \\nabla_{x,y,\\lambda} \\mathcal{L}(x,y,\\lambda) = 0 \\iff \\begin{cases} \\nabla_{x,y} f(x,y) = \\lambda \\nabla_{x,y} g(x,y) \\\\ g(x,y)=0 \\end{cases} $$ 例子1设一个具体的例子，我们需要求下列问题 $$ \\operatorname*{max}_{x,y} f(x,y) = x^2y \\qquad s.t.\\;\\; g(x,y): x^2+y^2-3=0 $$ 只有一个约束，使用一个乘子，设为 $\\lambda$，列出拉格朗日函数 $$ \\mathcal{L}(x,y,\\lambda)=f(x,y)-\\lambda \\cdot \\bigl( g(x,y) - c\\bigl) = x^2y + \\lambda(x^2+y^2-3) $$ 接下来求解上式，分别对三个待求量偏微分 $$ \\begin{align} \\nabla_{x,y,\\lambda} \\mathcal{L}(x,y,\\lambda) & = \\left( \\frac{\\partial \\mathcal{L}}{\\partial x},\\frac{\\partial \\mathcal{L}}{\\partial y},\\frac{\\partial \\mathcal{L}}{\\partial \\lambda}\\right)\\\\ & = (2xy + 2\\lambda x, x^2 + 2\\lambda y, x^2 + y^2 - 3) \\end{align} $$ 令偏微分分别等于0，得到 $$ \\nabla_{x,y,\\lambda} \\mathcal{L}(x,y,\\lambda) = 0 \\iff \\begin{cases} 2xy+2\\lambda x = 0 \\\\ x^2 + 2\\lambda y = 0 \\\\ x^2 + y^2 - 3 = 0 \\end{cases} \\iff \\begin{cases} x(y + \\lambda) = 0 & (i)\\\\ x^2 = -2\\lambda y & (ii)\\\\ x^2 +y^2 = 3 & (iii) \\end{cases} $$ 根据上式，我们可以解得 $\\mathcal{L}$: $$ (\\pm \\sqrt{2},1,-1 ); (\\pm \\sqrt{2},-1,1 );(0,\\pm \\sqrt{3},0) $$ 根据几个不同的解带入 $f(x,y)$ 得到，2，-2，0，也就是我们需要的最大值，最小值，对应的直观图像解释如下图所示（非常直观的展现约束和等高线的含义） 例子2关于拉格朗日乘子法的应用，有一个十分著名的：求离散概率分布 $p_1,p_2,\\cdots,p_n$ 的最大信息熵 $$ f(p1,p2,\\cdots,p_n) = - \\sum_{j=1}^n p_j log_2{p_j} \\\\ s.t. \\quad g(p1,p2,\\cdots,p_n) = \\sum_{k=1}^n p_k = 1 \\text{（概率和为1）} $$ 单约束问题，引入一个乘子 $\\lambda$ ，对于 $k \\in [1,n]$ ，要求 $$ \\frac{\\partial}{\\partial p_k} (f + \\lambda(g - 1)) = 0 $$ 将 $f$ 和 $g$ 带入有 $$ \\frac{\\partial}{\\partial p_k} \\left( -\\sum_{k=1}^np_klog_2{p_k} + \\lambda (\\sum_{k=1}^n p_k - 1)\\right) = 0 $$ 计算这 n 个等式的偏微分，我们可以得到： $$ -\\left( \\frac{1}{\\ln(2)} + log_2p_k \\right) + \\lambda = 0 $$ 这说明所有的 $p_i$ 都相等，所以得到 $p_k = \\frac{1}{n}$ 我们可以得到一个结论是：均匀分布的信息熵是最大的 多约束既然可以解决单约束，继续思考一下多约束情况的直观表现，假设我们的约束是两条线，如下图所示 和单约束的解决方法类似，我们画出等高线图，目的就是在约束线上找到一个点可以和等高线相切，所得的值实在约束范围内的最大值或者最小值，直观表示如下图 解算方法是讲单约束的扩展到多约束的情况，较为类似，可举一反三 KKT条件已经解决的在等式约束条件下的求函数极值的问题，那不等式约束条件下，应该如何解决呢？ 这就需要引出KKT条件（Karush-Kuhn-Tucker Conditions），它是在满足一些有规则的条件下，一个非线性规划问题能有最优化解法的一个必要和充分条件 考虑以下非线性最优化问题，含有 $m$ 个不等式约束，$l$ 个等式约束$$\\operatorname*{min}_{x}f(x) \\qquad s.t. \\; g_i(x) \\leqslant 0,\\; h_j(x) =0$$ 必要条件假设 $f,g_i,h_j$ 三个函数为实数集映射，再者，他们都在 $x^$ 这点连续可微，如果 $x^$ 是一个局部极值，那么将会存在一组称为乘子的常数 $\\lambda \\geqslant 0,\\mu_i \\geqslant0, \\nu_j$ 令 $$ \\lambda + \\sum_{i=1}^m \\mu_i + \\sum_{j=1}^l |\\nu_i| \\gt 0, \\\\ \\lambda \\nabla f(x^*) + \\sum_{i=1}^m \\mu_i \\nabla g_i(x^*) + \\sum_{j=1}^l \\nu_i \\nabla h_j(x^*) = 0, \\\\ \\mu_i g_i(x^*) =0 \\; \\text{for all} \\; i=1,\\ldots,m $$ 这里有一些正则性条件或约束规范能保证解法不是退化的（比如$\\lambda$为0），详见 充分条件假设 $f,g_i$ 为凸函数，$h_j$ 函数是仿射函数（平移变换），假设有一个可行点 $x^*$，如果有常数 $\\mu_i \\geqslant 0$ 及 $\\nu_j$ 满足 $$ \\nabla f(x^*) + \\sum_{i=1}^m \\mu_i \\nabla g_i(x^*) + \\sum_{j=1}^l \\nu_i \\nabla h_j(x^*) = 0 \\\\ \\mu_i g_i(x^*) =0 \\; \\text{for all} \\; i=1,\\ldots,m $$ 那么 $x^*$ 就是全局极小值 总结总的来说，拉格朗日乘子法是一个工具（手段或方法），来解决在有约束情况的求函数极值的问题","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"}]},{"title":"【直观详解】支持向量机SVM","date":"2017-09-20T01:23:39.000Z","path":"2017/09/19/支持向量机SVM学习笔记/","text":"【阅读时间】13min - 19min【内容简介】详解解读什么是支持向量机，如何解支持向量以及涉及的拉普拉斯乘子法，还有核方法的解读 什么是支持向量机-SVM支持向量机-SVM(Support Vector Machine)从本质来说是一种：用一条线（方程）分类两种事物 SVM的任务是找到这条分界线使得它到两边的margin都最大，注意，这里的横坐标是 $x_1$ 纵坐标为 $x_2$，如下图所示 margin 有了直观的感知，在定义这一节在做一些深入的思考，分解名词（Support Vector Machine）并尝试解释： Machine - Classification Machine 说明它的本质是一个分类器 Support Vector - 如上图所示，在Maximum Margin上的这些点就是支持向量，具体说即最终分类器表达式中只含有这些支持向量的信息，而与其他数据点无关。在下面的公式中，只有支持向量的系数 $\\alpha_i$ 不等于0。说人话，上图中两个红色的点，一个蓝色的点，合起来就是支持向量 $$ \\mathbf w \\cdot \\varphi (\\mathbf x) = \\sum_i \\lambda_i y_i k(\\mathbf x_i,\\mathbf x) $$ 公式中每一个符号的含义在后文有说明 如何求解支持向量机对于我们需要求解的这个超平面（直线）来说，我们知道 它离两边一样远（待分类的两个部分的样本点） 最近的距离就是到支持向量中的点的距离 根据这两点，抽象SVM的直接表达（Directly Representation） 注：$arg \\operatorname*{max}_{x} f(x)$ 表示当 $f(x)$ 取最大值时，x的取值 $$ arg \\operatorname*{max}_{boundary} margin(boundary) \\\\ \\text{所有正确归类的两类到boundary的距离} \\ge margin \\tag{1} $$ 其实这个公式是一点也不抽象，需要更进一步的用符号来表达。 我们知道在准确描述世界运行的规律这件事上，数学比文字要准确并且无歧义的多，文字（例子）直观啰嗦，数学（公式）准确简介 硬间隔 SVM支持向量机 注：公式中加粗或者带有向量箭头的都表达一个向量 假设这些数据线性可分，也可称为硬间隔（Hard Margin） 首先定义超平面：$\\mathbf w^T \\vec x_i + b = 0$，接下来为了方便，设 $\\vec x = (x_1,x_2)$ 即一条直线 任意点 $\\vec x_i$ 到该直线的距离为 $\\frac{1}{\\lVert \\mathbf w \\lVert} (\\mathbf w^T \\vec x_i + b)$ 对于空间内所有训练点的坐标记为 $(\\vec x_i,y_i)$，其中 $y_i$ = 1 or -1， 表示点 $\\vec x_i$ 所属的类 如果这些训练数据是线性可分的，选出两条直线（上图中的虚线），使得他们的距离尽可能的大，这两条直线的中央就是待求的超平面（直线） 为了表达直观，我们定义这两个超平面（直线）分别为 $\\mathbf w^T \\vec x_i + b = 1$ 和 $\\mathbf w^T \\vec x_i + b = -1$，两个超平面（直线）之间的距离为 $\\gamma = \\frac{2}{\\lVert \\mathbf w \\lVert}$ 注：选择1的好处是，w 和b进行尺缩变换（kw和kb）不改变距离，方便计算 为了使得所有样本数据都在间隔区（两条虚线）以外，我们需要保证对于所有的 $i$ 满足下列的条件 $\\mathbf w^T \\vec x_i + b \\geqslant 1$ 若 $y_i = 1$ $\\mathbf w^T \\vec x_i + b \\leqslant -1$ 若 $y_i = -1$ 上述两个条件可以写作 $y_i(\\mathbf w^T \\vec x_i + b) \\geqslant 1, \\;\\text{for all 1}\\; 1\\leqslant i \\leqslant n$ 这里的n指样本点的数量 上面的表达（Directly Representation）可以被写成 $$ arg \\operatorname*{max}_{\\mathbf w,b} \\left\\{ {\\frac{1}{\\lVert \\mathbf w \\lVert} \\operatorname*{min}_{n} [y_i(\\mathbf w^T\\vec x_i}+b)]\\right\\} \\tag{2} $$ 最终目的是找到具有“最大间隔”（Maximum Margin）的划分超平面（直线），找到参数 $\\mathbf w$ 和 $b$ 使得 $\\gamma$ 最大 则可以对(2)式进行形式变换，得到 canonical representation $$ arg \\operatorname*{max}_{\\mathbf w,b} \\frac{2}{\\lVert \\mathbf w \\lVert} \\implies arg \\operatorname*{min}_{\\mathbf w,b} \\frac{1}{2}\\lVert \\mathbf w \\lVert ^2 \\\\ s.t.\\; y_i(\\mathbf w^T\\vec x_i+b) \\geqslant1,\\;i = 1,2,\\ldots,m \\tag{3} $$ 注：s.t. ：subject to 表示约束条件，表达的意思等价于：为了使得所有样本数据都在间隔区（两条虚线）以外 为了解(3)式，需要用到拉格朗日乘子法（Method of lagrange multiplier），它是用来求解在约束条件目标函数的极值的，详细直观详解 注：以下解算过程希望完全看懂强烈建议理解阅读详细直观详解，很多地方推导过程只写必要过程及结论 根据约束的形式，我们引入m个拉格朗日乗法子，记为 $\\boldsymbol \\lambda = (\\lambda_1,\\ldots,\\lambda_m)^T$ ，原因是，有m个约束，所以需要m个拉格朗日乗法子。可以得出拉格朗日方程如下： $$ \\mathcal{L}(\\mathbf w,b,\\boldsymbol \\lambda) = \\frac{1}{2}\\lVert \\mathbf w \\lVert ^2 - \\sum_{i=1}^m \\lambda_i \\{ y_i(\\mathbf w^T\\vec x_i+b) -1 \\} \\tag{4} $$ 解这个拉格朗日方程，对 $\\mathbf w$ 和 $b$ 求偏导数，可以得到以下两个条件 $$ \\mathbf w = \\sum_{i=1}^m \\lambda_i y_i \\vec x_i \\\\ 0 = \\sum_{i=1}^m \\lambda_i y_i $$ 将这两个条件带回公式(4)，可以得到对偶形式（dual representaiton），我们的目的也变为最大化 $\\mathcal{L}(\\boldsymbol \\lambda)$，表达式如下 $$ arg \\operatorname*{max}_{\\boldsymbol \\lambda}\\mathcal{L}(\\boldsymbol \\lambda) = \\sum_{i=1}^m \\lambda_i - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j \\vec x_i \\vec x_j \\mathbf x_i^T \\mathbf x_j \\\\ s.t. \\qquad \\lambda_i \\geqslant 0, \\forall i\\;;\\quad \\sum_{i=1}^m \\lambda_i y_i = 0 \\tag{5} $$ 以上表达式可以通过二次规划算法解出 $\\boldsymbol \\lambda$ 后，带回，求出$\\mathbf w$ 和 $b$，即可得到模型 $$ f(\\mathbf x) = \\mathbf w^T\\mathbf x + b = \\sum_{i=1}^m \\lambda_i y_i \\mathbf x_i^T \\mathbf x + b \\tag{6} $$ 补充一些关于二次规划算法的相关，(3)式的约束是一个不等式约束，所以我们可以使用KKT条件得到三个条件： $$ \\lambda_i \\geqslant0 ;\\quad y_i f(\\mathbf x_i)-1 \\geqslant0; \\quad \\lambda_i\\{ y_i f(\\mathbf x_i)-1 \\}=0 $$ 使用这些条件，可以构建高效算法来解这个方程，比如SMO（Sequential Minimal Optimization）就是其中一个比较著名的。至于SMO是如何做的，考虑到现代很多SVM的Pakage都是直接拿来用，秉承着前人付出了努力造了轮子就不重复造的核心精神，直接调用就好 软间隔已经说明了如何求得方程，以上的推导形式都是建立在样本数据线性可分的基础上，如果样本数据你中有我我中有你（线性不可分），应该如何处理呢？这里就需要引入软间隔（Soft Margin），意味着，允许支持向量机在一定程度上出错 由上一节我们得知，约束为： $y_i(\\mathbf w^T\\vec x_i+b) \\geqslant1,\\;i = 1,2,\\ldots,m$ ，目标是使目标函数可以在一定程度不满足这个约束条件，我们引入常数 $C$ 和 损失函数 $\\ell_{0/1}(z)$ 为0/1损失函数，当z小于0函数值为1，否则函数值为0 $$ \\operatorname*{min}_{\\mathbf w,b} \\frac{1}{2}\\lVert w \\lVert^2 + C \\sum_{i=1}^m \\ell_{0/1}(y_i(\\mathbf w^T\\vec x_i+b) -1) \\tag {7} $$ 对于(7)式来说 $C \\geqslant 0$ 是个常数，当C无穷大时，迫使所有样本均满足约束；当C取有限值时，允许一些样本不满足约束 但 $\\ell_{0/1}(z)$ 损失函数非凸、非连续，数学性质不好，不易直接求解，我们用其他一些函数来代替它，叫做替代损失函数（surrogate loss） $$ \\begin{align} & \\text{hinge损失:} \\ell_{hinge}(z) = max(0,1-z)\\\\ & \\text{指数损失:} \\ell_{exp}(z) = e^{-z}\\\\ & \\text{对数损失:} \\ell_{log}(z) = log(1+e^{-z})\\\\ \\end{align} $$ 三种常见损失函数如下图 为了书写方便，我们引入松弛变量（slack variables）: $\\xi_i \\geqslant 0$，可将(7)式重写为 $$ \\operatorname*{min}_{\\mathbf w,b,\\xi_i} \\frac{1}{2}\\lVert w \\lVert^2 + C \\sum_{i=1}^m \\xi_i \\\\ s.t. \\quad y_i(\\mathbf w^T\\vec x_i+b) \\geqslant 1 - \\xi_i ;\\; \\xi_i \\geqslant 0,\\; i = 1,2,\\ldots,m \\tag{8} $$ (8)式就是常见的软间隔支持向量机，其中，每一个样本都有一个对应的松弛变量，用以表征该样本不满足约束的程度，求解的方法同理硬间隔支持向量机 支持向量机扩展核方法以上我们求解的支持向量机都是在线性情况下的，那么非线性情况下如何处理？这里就引入：核方法 对于这样的问题，可以将样本从原始空间映射到一个更高为的特征空间，使得样本在这个特征空间内线性可分，直观可视化解释 为了完成这个目的，令 $\\phi(\\mathbf x)$ 表示将 $\\mathbf x$ 映射后的特征向量，于是，在特征空间划分超平面所对应的模型可表示为： $$ f(\\mathbf x) = \\mathbf w^T \\phi(\\mathbf x) + b $$ 同理上文中引入拉格朗日乘子，求解整个方程后可得 $$ \\begin{align} f(\\mathbf x) &= \\mathbf w^T \\phi(\\mathbf x) + b \\\\ &= \\sum_{i=1}^m \\lambda_i y_i \\phi(\\mathbf x_i)^T \\phi(\\mathbf x) + b \\\\ &= \\sum_{i=1}^m \\lambda_i y_i k(\\mathbf x,\\mathbf x_i)+ b \\end{align} $$ 这里的函数 $k(\\cdot,\\cdot)$ 就是核函数（kernel function），常见的核函数见下表 名称 表达式 参数 线性核 $\\boldsymbol x_i^T \\boldsymbol x_j$ 无 多项式核 $(\\boldsymbol x_i^T \\boldsymbol x_j)^d$ $d \\geqslant 1$ 多项式次数 高斯核 $exp(-\\frac{\\lVert\\boldsymbol x_i - \\boldsymbol x_j \\lVert^2}{2\\sigma^2})$ $\\sigma&gt;0$ 高斯核带宽 拉普拉斯核 $exp(-\\frac{\\lVert\\boldsymbol x_i - \\boldsymbol x_j \\lVert^2}{\\sigma})$ $\\sigma&gt;0$ Sigmoid核 $tanh(\\beta \\boldsymbol x_i^T\\boldsymbol x_j + \\theta)$ $\\beta&gt;0$ $\\theta&gt;0$ 也可以通过函数组合得到这些值 多类问题多类问题可以使用两两做支持向量机，再由所有的支持向量机投票选出这个类别的归属，被称为one-versus-one approace。 Reference知乎各类回答Wiki百科PRML周志华-机器学习","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"}]},{"title":"Dota2伤害类型详解","date":"2017-09-18T21:31:12.000Z","path":"2017/09/18/Dota2伤害类型详解/","text":"【阅读时间】5min 百科类【内容简介】有关Dota2所有伤害来源的总结和互相作用总结，方便查阅 伤害来源攻击伤害主要来自于普通物理攻击 - 平A 具体计算方式 技能伤害详情参见 包含了所有来自技能的伤害，三种类型：魔法，物理和纯粹 伤害类型互相作用机制表格 游戏机制 物理攻击 物理技能 魔法伤害 纯粹伤害 护甲 降低 降低 正常 正常 伤害格挡 降低 正常^1 正常 正常 魔法抗性 正常 正常 降低 正常 虚无 没有效果 美国效果 降低 正常 闪避 可能落空 正常 正常 降低 致盲 可能落空 正常 正常 正常 伤害加深^2 加深 加深 加深 加深 伤害减免^2 降低 降低 降低 降低 伤害无效化^2 没有效果 没有效果 没有效果 没有效果 魔法伤害护盾 正常 正常 没有效果 正常 无敌 没有效果 没有效果 没有效果 没有效果 技能免疫 正常 不定 不定 不定 物理与护甲和伤害格挡有关，虚无和一些技能可以造成物理免疫，召唤单位和守卫的平A也是物理攻击 物理免疫技能炼金术士：酸性喷雾 炼金术士：不稳定化合物 敌法师：法力损毁 兽王：野性飞斧 赏金猎人：暗影步 钢背兽：针刺扫射 人马：反击 克林克次：灼热之箭 戴泽：剧毒之触 戴泽：暗影波 死亡先知：驱使恶灵 龙骑士：古龙形态溅射 上古巨神：裂地沟壑 上古巨神：回音重踏 灰烬之灵：无影拳 主宰：无敌斩 昆卡：潮汐使者 拉席克：恶魔赦令 噬魂鬼：盛宴 露娜：月刃 马格纳斯：加强力量溅射 司夜刺客：复仇 剃刀：风暴之眼 力丸：背刺 斯拉达：鱼人碎击 斯拉达：重击 狙击手：爆头 熊灵：缠绕之爪 斯温：巨力挥舞 圣堂刺客：隐匿 潮汐猎人：锚机 熊战士：怒意狂击 冥界亚龙：幽冥剧毒 编织者：虫群 魔法大多数技能都是魔法伤害，虚无状态会承受更多伤害 在纯粹和物理技能中未提及的都是魔法伤害 纯粹纯粹伤害能作用与技能免疫单位，不能作用于无敌单位 斧王-反击螺旋 祸乱之源：蚀脑 祸乱之源：噩梦 嗜血狂魔：血之祭祀 嗜血狂魔：割裂 陈：忠诚考验 末日使者：末日 魅惑魔女：推进 谜团：午夜凋零 谜团：黑洞 祈求者：炎阳冲击 杰奇洛：A烈焰焚身 莉娜：A神灭斩 美杜莎：（石化）秘术异蛇 司夜刺客：尖刺外壳 全能骑士：洗礼 殁境神蚀者：奥术天球 帕吉：肉钩 痛苦女王：超声冲击波 沉默术士：智慧之刃 幽鬼：荒芜 圣堂刺客：灵能之刃 伐木机：锯齿飞轮 伐木机：伐木锯链 伐木机：死亡旋风 修补匠：激光 骨灰 标记是一种特殊标记，为的是与其他技能区分开来 生命移除标记某些生命移除标记的技能可以立即杀死幻想 干扰者：恶念瞥视 莱恩：妖术 莱恩：法力抽取 美杜莎：石化凝视 帕格纳：生命吸取 羊刀：变羊 暗影萨满：变羊 有些技能利用生命移除来制造生命消耗效果，通常都是非致命伤害，伤害类型也是纯粹，被标记为生命移除 臂章：扣血 哈斯卡：沸血之矛对自身 艾欧：过载 凤凰：凤凰冲击 凤凰：烈火精灵 凤凰：烈日炙烤 魂戒：献身 工程师：自爆起飞 不朽尸王：噬魂 不反弹标记不反弹标记会使得一些受到伤害事件不会与带有不反弹标记的伤害相互作用，这防止了无限伤害循环( 刃甲：反弹伤害 司夜刺客：尖刺外壳 冥界亚龙：腐蚀皮肤 术士：致命链接","tags":[{"name":"Dota2","slug":"Dota2","permalink":"https://charlesliuyx.github.io/tags/Dota2/"},{"name":"Wiki","slug":"Wiki","permalink":"https://charlesliuyx.github.io/tags/Wiki/"}]},{"title":"【直观详解】机器学习分类器性能指标详解","date":"2017-09-12T20:05:32.000Z","path":"2017/09/12/机器学习分类器性能指标详解/","text":"【阅读时间】16 - 26 min【内容简介】系统详解分类器性能指标，什么是准确率 - Accuracy、精确率 - Precision、召回率 - Recall、F1值、ROC曲线、AUC曲线、误差 - Error、偏差 - Bias、方差 - Variance及Bias-Variance Tradeoff 在任何领域，评估（Evaluation）都是一项很重要的工作。在Machine Learning领域，定义了许多概念并有很多手段进行评估工作 混淆矩阵 - Confusion Matrix准确率定义：对于给定的测试数据集，分类器正确分类的样本数与总样本数的之比 通过准确率，的确可以在一些场合，从某种意义上得到一个分类器是否有效，但它并不总是能有效的评价一个分类器的工作。一个例子，Google抓取了100个特殊页面，它的索引中有10000000页面。随机抽取一个页面，这是不是特殊页面呢？如果我们的分类器确定一个分类规则：“只要来一个页面就判断为【不是特殊页面】”，这么做的效率非常高，如果计算按照准确率的定义来计算的话，是(9,999,900/10,000,000) = 99.999%。虽然高，但是这不是我们并不是我们真正需要的值，就需要新的定义标准了 对于一个二分类问题来说，将实例分为正类（Positive/+）或负类（Negative/-），但在使用分类器进行分类时会有四种情况 一个实例是正类，并被预测为正类，记为真正类（True Positive TP/T+） 一个实例是正类，但被预测为负类，记为假负类（False Negative FN/F-） 一个实例是负类，但被预测为正类，记为假正类（False Positive FP/F+） 一个实例是负类，但被预测为负类，记为真负类（True Negative TN/F-） TP和TN中的真表示分类正确，同理FN和FP表示分类错误的 为了全面的表达所有二分问题中的指标参数，下列矩阵叫做混淆矩阵 - Confusion Matrix，目的就是看懂它，搞清楚它，所有模型评价参数就很清晰了 DiagnosticTesting Diagram Relationships between various measures of diagnostic testing by CMG Lee. In the SVG image, hover over a block or relation to highlight it. #mainsvg { font-family:Helvetica,Arial,sans-serif; font-size:6px; text-anchor:middle; stroke-linejoin:round; stroke-linecap:round; stroke-width:0.7; fill:none; stroke-opacity:1; fill-opacity:1; } #mainsvg:hover { stroke-opacity:0.25; fill-opacity:0.25; } .active:hover { stroke-opacity:1; fill-opacity:1; } text { fill:#000000; cursor:default; } .label { stroke:none; fill:#000000 } .op { stroke-width:0.15; font-size:5px; font-weight:bold; } + &#247; &#247; &#247; + &#247; &#247; +,&#247; &#247; F1 s.F1 score = 2 / (1 / Recall + 1 / Precision) ACCAccuracy = (&#931; True positive + &#931; True negative) / &#931; Total population DORDiagnostic odds ratio = Positive likelihood ratio / Negative likelihood ratio LR+Positive likelihood ratio = True positive rate / False positive rate LR&#8722;Negative likelihood ratio = False negative rate / True negative rate FDRFalse discovery rate = &#931; False positive / &#931; Predicted condition positive PPVPositive predictive value, Precision = &#931; True positive / &#931; Predicted condition positive NPVNegative predictive value = &#931; True negative / &#931; Predicted condition negative FORFalse omission rate = &#931; False negative / &#931; Predicted condition negative TPRTrue positive rate, Recall, Sensitivity, probability of detection = &#931; True positive / &#931; Condition positive FNRFalse negative rate, Miss rate = &#931; False negative / &#931; Condition positive FPRFalse positive rate, Fall-out, probability of false alarm = &#931; False positive / &#931; Condition negative TNRTrue negative rate, Specificity = &#931; True negative / &#931; Condition negative prev.Prevalence = &#931; Condition positive / &#931; Total population pop.Total population = Condition positive + Condition negative = Predicted condition positive + Predicted condition negative 样本空间 = 正类 + 负类 = 预测结果正类 + 预测结果负类 Pc&#8722;Predicted condition negative = False negative + True negative Pc+Predicted condition positive = True positive + False positive C+Condition positive = True positive + False negative C&#8722;Condition negative = False positive + True negative T&#8722;负类中预测正确的部分 F&#8722;负类中预测错误的部分 F+正类中预测错误的部分 T+正类中预测正确的部分 通过上面的的讨论已经有T+:TP F+:FP T-:TN F-:FN C+:样本正类 C-:样本负类 Pc+:预测正类 Pc-:预测负类 用样本中的正类和负类进行计算的定义 缩写 全称 等价称呼 计算公式 TPR True Positive Rate 真正类率 Recall Sensitivity $ \\frac {\\sum T+}{\\sum C+}$ FNR False Negative Rate 假负类率Miss rate Type rs error $ \\frac {\\sum F-}{\\sum C+}$ FPR False Positive Rate 假正类率fall-out Type 1 error $ \\frac {\\sum F+}{\\sum C-}$ TNR Tre Negative Rate 真负类率Specificity $ \\frac {\\sum T-}{\\sum C-}$ 用预测结果的正类和负类进行计算的定义 缩写 全称 等价称呼 计算公式 PPV Positive Predictive Value 正类预测率Precision $ \\frac {\\sum T+}{\\sum Pc+}$ FOR False Omission Rata 假错误率 $ \\frac {\\sum F-}{\\sum Pc-}$ FDR False Discovery Rate 假发现率 $ \\frac {\\sum F+}{\\sum Pc+}$ NPV Negative Predictive Value 负类预测率 $ \\frac {\\sum T-}{\\sum Pc-}$ 其他定义概念 缩写 全称 等价称呼 计算公式 ACC Accuracy 准确率 $ \\frac {\\sum (T+) + \\sum {T-}}{样本空间}$ LR+ Positive Likelihood Ratio 正类似然比 $ \\frac {TPR}{FPR}$ LR- Negative likelihood ratio 负类似然比 $ \\frac {FNR}{TNR}$ DOR Diagnostic odds ratio 诊断胜算比 $ \\frac {LR+}{LR-}$ F1 score $F_1$ test measure F1值 $\\frac{2}{\\frac{1}{recall}+\\frac{1}{precision}}$ MCC Matthews Correlation coefficient 马修斯相关性系数 $\\frac{TP \\times TN - FP \\times FN}{\\sqrt {(TP + FP)(TP + FN)(TN + FP)(TN +FN)}}$ LR+/-指的是似然比，LR+ 越大表示模型对正类的分类越好，LR-越大表示模型对负类的分类效果越好 F1值是精确值和召回率的调和均值，其实原公式是 $F_\\beta = (1 + \\beta^2)\\frac{precision \\times recall}{(\\beta^2recall)+recall}$，这里的β表示：召回率的权重是准确率的β倍。即F值是一种精确率和召回率的综合指标，权重由β决定 MCC值在[-1,1]之间，靠近1表示完全预测正确，靠近-1表示完全悖论，0表示随机预测 最终为了不那么麻烦，说人话，还是一图胜千言 Precision - Recall 图片详解： 左边暗一些部分的点都是真正的正类，右边亮一些部分的点都是真正的负类 中间的一个圆圈就是我们的正类分类器：注意，这个圈是的预测结果都是正类，也就是说在这个分类器看来，它选择的这些元素都是它所认为的正类，对应的，当然是圈以外的部分，也就是预测结果是负类的部分 底下的Precision和Recall示意图也相当的直观，看一下就能明白 ROC CurveROC - Receiver Operating Characteristic Curve，接受者操作特征曲线，ROC曲线 这个曲线乍看下为啥名称那么奇怪呢，原来这个曲线最早是由二战中的电子工程师和雷达工程师发明的，用来侦测战场上的敌军飞机，舰艇等，是一种信号检测理论，还被应用到心理学领域做知觉检测。 什么是ROC曲线ROC曲线和混淆矩阵息息相关，上一部分已经详细解释了相关内容，这里直接说明ROC曲线的横坐标和纵坐标分别是什么 横坐标：FPR假正类率，纵坐标：TPR真正类率 初看之下你不懂一个曲线表示的什么意思，那么看几个特征点或特殊曲线是一个非常好的方法。按照这种方法来分析ROC曲线： 第一个点：(0,1)，FPR=0 TPR=1 ，这意味着所有的正类全部分类正确，或者说这是一个完美的分类器，将所有的样本都分类正确了 第二个点：(1,0)， FPR=1 TPR=0 ，和第一个点比较，这是第一个点的完全反面，意味着是个最糟糕的分类器，将所有的样本都分类错误了（但其实可以直接取反，就是最好的模型，因为是二分类问题） 第三个点：(0,0)，FPR=0 TPR=0 也就是原点，这个点表示的意思是，分类器预测所有的样本都为负类 第四个点：(1,1)，FPR=1 TPR=1，和第三个点对应，表示分类器预测所有的样本都为正类 一条线：y=x。这条对角线上的点实际上就是一个采用随机猜测策略的分类器的结果 总结来说，ROC曲线的面积越大，模型的效果越好；ROC曲线光滑以为着Overfitting越少 还是一图胜千言 ROC曲线解释 $TPR = \\frac{TP}{TP+FN}$ $FPR = \\frac{FP}{FP+TN}$ 蓝色图像是正类分类器的概率分布，红色图像负类分类器的概率分布，竖直的黑线是阈值（Threshold），二分类分类器的输出就是一个取值在[0,1]间的值（概率），我们将黑线从0移动到1，就能得出一条曲线，这条线就是ROC曲线 如果问这个分类器画成的图像为何是一个类似帽子的形状，例子是最佳的说明方法，我们就来算一个ROC曲线看看，下图是20个测试样本的结果，“Class”一栏表示每个测试样本真正的标签（p表示正类，n表示负类），“Score”表示每个测试样本属于正样本的概率，Inst#是序号数 example-data 接下来，我们从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值（和你的测试样本的数量有关），将它们画在ROC曲线的结果如下图： example-roc-curve 当然我们也可以曲很多个阈值画曲线，不一定非要从测试样本的结果中取20个 为什么使用ROC曲线ROC曲线有一个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线的对比： ROC-PrecisionRecall 在上图中，(a)和(c)为ROC曲线，(b)和(d)为Precision-Recall曲线。 (a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大，记住这个结论即可 PRC Curve在上面提到了一个指标，PRC - Precision-Recall 曲线，画法和ROC很相似，但是使用值是Precision和Recall AUC ValueAUC - Area Under Curve被定义为ROC曲线下的面积 AUC在[0.5,1]之间，这是因为ROC曲线一般都处于y=x这条直线的上方（否则这个做分类器的人连简单的取非都不会真可以去死了） AUC值越大，证明这个模型越好 Bias-Variance Tradeoff三个名词，Error误差 Bisa偏差 Variance方差 三个名词表示了什么再来一次，一图胜千言 准：Bias 描述的是根据样本训练的模型的输出预测结果的期望与样本真实结果的差距，说人话，这个模型对样本拟合的好不好。想在Bias上表现好，降低Bias，就是复杂化模型，增加模型的参数，但这样容易过拟合（Overfitting）Low Bias对应的就是点都在靶心附近，所以瞄的都是准的，但手不一定稳 确：Variance 描述的是根据样本训练的模型在测试集上的表现（泛化能力） ，想在Variance上表现好，降低Variance，需要简化模型，减少模型的参数，这样做容易欠拟合，对应上图的High Bias，点偏离的中心。Low Variance对应的是点打的都很集中，但不一定在靶心附近，手很稳，但是瞄的不准 要准确表达这两个定义的含义必须要使用公式化的语言，不得不感叹，在准确描述世界运行的规律这件事上，数学比文字要准确并且无歧义的多，文字（例子）直观啰嗦，数学（公式）准确简介 我们假设有这样的一个函数，$y=f(x) + \\epsilon$ ，其中噪声 $\\epsilon$ 均值为0，方差为 $\\sigma^2$ 我们的目的是去找到一个函数 $\\hat {f}(x)$ 尽可能接近 $f(x)$ ，我们可以用均方误差（MSE）或者交叉熵，或者DL散度来表示这个接近程度，我们希望 $(y - \\hat f(x) )^2$ 对样本空间内的所有样本和测试集中的所有样本都最小 机器学习核心就是用各种不同的算法去找这个 $\\hat f$，希望最小，那就使用一个公式来表征这个值得大小，即期望，也称Total Error（误差），在机器学习的训练中，这个值是评判模型好坏最重要：$$E[(y - \\hat f(x))^2] = Bias[\\hat f(x)]^2 + Var[\\hat f(x)] + \\sigma^2$$ 其中 $Bias[\\hat f(x)] = E[\\hat f(x) - f(x)]$，且 $Var[\\hat f(x)] = E[\\hat f(x)^2] - E[\\hat f(x)]^2$ Bias-Variance Tradeoff作为机器学习一个核心训练的观点或者说概念，推导觉得还是十分重要，整理如下 推导过程为了公式简介，把 $f(x)$ 与 $\\hat f(x)$ 简写为 $f$ 与 $\\hat f$ ，记随机变量为 $X$，有$$Var[X] = E[X^2] - E[X]^2 \\implies E[X^2] = Var[X] + E[X]^2$$ 因为 $f$ 是一个已经确定的函数，所以 $E[f] = f$ 成立 根据 $y = f + \\epsilon$ 和 $E[\\epsilon] = 0$ 有$$E[y] = E[f + \\epsilon] = E[f] = f$$噪声的方差 $ Var[\\epsilon] = \\sigma^2$ $$ Var[y] = E[(y-E[y])^2] = E[(y - f)^2] = E[(f + \\epsilon - f)^2] = E[\\epsilon^2] = Var[\\epsilon] + E[\\epsilon]^2 = \\sigma^2 $$ 由于 $\\epsilon$ 和 $\\hat f$ 互相独立 $$ \\begin{align} E[(y - \\hat f)^2] & = E[y^2 + \\hat f^2 - 2y\\hat f] \\\\ & = E[y^2] + E[\\hat f^2] - E[2y\\hat f] \\\\ & = Var[y] + E[y]^2 + Var[\\hat f] + E[\\hat f]^2 - 2fE[\\hat f] \\\\ & = Var[y] + Var[\\hat f] + (f^2 - 2fE[\\hat f] + E[\\hat f]^2) \\\\ & = Var[y] + Var[\\hat f] + (f - E[\\hat f])^2 \\\\ & = \\sigma^2 + Var[\\hat f] + Bias[\\hat f]^2 \\end{align} $$ 总结感觉在实际使用中，你不需要去自己写代码来画这些曲线，只要是框架是一定整合了这些值得结果，但是知其然知其所以然，越了解它是如何画的，越能处理奇怪的特殊情况 常见的处理方式是记下来所有指标的结果，即这些指标怎么变，表示了模型的那些方面好或者坏的结论，但是如果在特殊的问题出现了不在你看的结果中的情况可能还是会捉襟见肘，还是脚踏实地，能看见更大的世界！","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"}]},{"title":"【直观详解】信息熵、交叉熵和相对熵","date":"2017-09-11T07:25:49.000Z","path":"2017/09/11/什么是信息熵、交叉熵和相对熵/","text":"【阅读时间】10min - 13min【内容简介】使用一个现实中直观的例子详解信息熵、交叉熵及相对熵的核心概念，读完后，希望能帮助你建立起这三个概念的固有直觉，不再疑惑 要完成题目的最终解释，必须从熵这个神奇的概念开始讲起 什么是熵 - Entropy词源 - 最初来源于热力学Entropy来源于希腊语，原意：内向，即：一个系统不受外部干扰时往内部稳定状态发展的特性。定义的其实是一个热力学的系统变化的趋势 $$\\Delta S = \\frac{Q}{T} = \\frac{热量}{温度} \\tag{1-1}$$1923年，德国科学家普朗克来中国讲学用到entropy这个词，胡刚复教授看到这个公式，创造了“熵”字，因为“火”和热量有关，定义式又是热量比温度，相当自洽 信息论信息论中，熵是接受的每条消息中包含的信息的平均值。又被称为信息熵、信源熵、平均自信息量。可以被理解为不确定性的度量，熵越大，信源的分布越随机 1948年，由克劳德·爱尔伍德·香农将热力学中的熵引入信息论，所以也叫做：香农熵 生态学在生态学中，熵表示生物多样性的指标 广义的定义熵是描述一个系统的无序程度的变量；同样的表述还有，熵是系统混乱度的度量，一切自发的不可逆过程都是从有序到无序的变化过程，向熵增的方向进行 我们接下来要讨论的信息熵 交叉熵 相对熵 更多的着眼于信息论的角度，换句话说，更加关注概率和不确定性 什么是信息熵、交叉熵、相对熵可以将对熵的理解从简单到复杂依次分解成三个层次来理解 如何衡量不确定事物的发生？数学是一种工具，使用数学来描述现实中的各种事物是一个数学家本质的工作目标。而现实中不确定性，或者说不太确定是否会发生的事件必须要找到一种抽象的、符号化和公式化的手段去表示。 比如天气情况，假设可能有【阴、晴、雨、雪】四种情况，使用概率符号表示 $\\mathbf P = [p_1,p_2,p_3,p_4]$，接下来自然而然的思考：那么，什么条件（情况）会影响这些值呢？ 假设有一下三种描述，或者说条件 今天是晴天，所以明天可能也是晴天 天气预报说明天下雨 9月12日苹果公司举行发布会 那么这三个描述中，很明显，第二条的信息量更大，因为它可以使得不确定事件发生在 $p_3$ 的概率更大。类似的，第三条对判断毫无帮助，信息量为0。注意，信息量不等于信息熵，如果是这样，那么直接用概率来衡量就可以了，不需要在重新定义一个概念 其实信息熵是信息量的期望（均值），它不是针对每条信息，而是针对整个不确定性结果集而言，信息熵越大，事件不确定性就越大。单条信息只能从某种程度上影响结果集概率的分布 考虑到信息冗余，信息量存储下来究竟需要多大空间？我们已经有了 $\\mathbf P = [p_1,p_2,p_3,p_4]$ 来表示天气情况，那么用计算机来存储每天的天气，那该如何编码呢？ 常见的做法是，4个不同的信息，只需要2bit就能做到，00 01 11 10，假设我们在南方城市，肯定要把00编码成雨天，这样可以节省存储空间，至于为什么能节省存储空间，这就要讨论编码方式。可以简单的理解为，如果一串信息一串0很多，可以通过编码压缩这一群0来节省空间 使用一个公式来计算记录n天数据需要的存储空间：Sn $$ S_n = n \\times \\sum_{i = 1}^4{\\left(P_i \\times F(P_i) \\right) } \\tag{2-1} $$ $P_i$ 表示第i个事件发生的概率；$F(P_i)$ 表示存储空间的存储因子 如何确定这个函数 $F(P_i)$ 的形式？考虑这个函数需要满足条件：概率大的事件对应小的存储空间，说人话，就是成反比，你的数学功底不错的话，脑海中第一反应出来满足这个条件最直观是反比例函数，说人话， $\\frac{1}{P_i}$ 。 之后我们发现这个公式中有个除法非常讨厌，我们想着去掉它，脑海中第一反应出来的满足这个条件的一定是取对数，至于为什么取对数，那说道就很多，取对数是指数的逆操作， 对数操作可以让原本不符合正态分布的模型符合正态分布，比如随着模型自变量的增加，因变量的方差也增大的模型取对数后会更加稳定 取对数操作可以rescale（原谅我，这里思前想后还是感觉一个英文单词更加生动）其实本质来说都是因为第一点。说人话版本，人不喜欢乘法，对数可以把乘法变加法 那么我们结束清楚之后，就很容易就可以定义出$$F(P_i) = \\log_a ({\\frac{1}{P_i}}) \\tag{2-2}$$ a作为底数，可以取2（处理2bit数据），10（万金油），e（处理正态分布相关的数据） 结合对信息熵的定义（第一节最后的粗体字）然后把（2-2）带入（2-1），就会发现，哦！看着有点眼熟啊$$H(P) = \\sum_i {P(i)log_a {\\frac{1}{P(i)}}} = - \\sum_i {P(i)log_a {P(i)}} \\tag{2-3}$$这这这，就是信息熵的定义式吧？总结就发现，信息熵其实从某种意义上反映了信息量存储下来需要多少存储空间 总结为：根据真实分布，我们能够找到一个最优策略，以最小的代价消除系统的不确定性（比如编码），而这个代价的大小就是信息熵 理解基于信息熵的交叉熵和相对熵因为是我们用2bit模式存储，为了计算方便，这里取a = 2 先计算刚刚有关天气问题 $\\mathbf P = [p_1,p_2,p_3,p_4]$ ：【阴、晴、雨、雪】的信息熵，假设我们对天气的概率一无所知，那么四种天气的发生概率为等概率（服从平均分布），即 $\\mathbf P = [\\frac {1}{4},\\frac {1}{4},\\frac {1}{4},\\frac {1}{4}]$ ，带入公式2-3，得到 $H(P) = 2$ ，存储信息需要的空间 $S_n = 2n$ 继续思考，假设我们考虑天气的城市是一个地处中国南方雨季的城市，那么阴天和雨天的概率从经验角度（先验概率）来看大于晴天雪天，把这种分布记为 $\\mathbf Q = [\\frac{1}{4},\\frac{1}{8},\\frac{1}{2},\\frac{1}{8}]$，带入公式2-3，信息熵 $H(Q) = 1.75$，存储信息需要的空间 $S_n = 1.75n$ 直观的来考虑上面不同的两种情况，明显当事件的不确定性变小时候，我们可以改变存储策略（00 雨天 01 阴天），再通过编码，节省存储空间。信息熵的大小就是用来度量这个不确定大小的 关于编码的方式，这里提一下，哈夫曼树与哈夫曼编码 ，有兴趣的读者可以去研究一下 交叉熵的由来我们把这个问题再扩展一下 天气【阴、晴、雨、雪】 信息熵 $\\mathbf P = [\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4}]$ $H(P) = 2$ $\\mathbf Q = [\\frac{1}{4},\\frac{1}{8},\\frac{1}{2},\\frac{1}{8}]$ $H(Q) = 1.75$ $\\mathbf Z = [\\frac{1}{8},\\frac{1}{16},\\frac{3}{4},\\frac{1}{16}]$ $H(Z) = \\frac{7}{8}+\\log_2 {\\frac{4}{3}} = 1.29$ $\\mathbf W = [0,0,1,0]$ $H(W) = 0$ 接下来，假定在确定性更大的概率分布情况下，用更不确定的存储策略来计算，比如使用 $\\mathbf P$ 的概率乘上 $\\mathbf Q$ 的存储因子，套用公式2-3$$H(\\mathbf P,\\mathbf Q) = \\sum_i {P(i) \\log_a {\\frac{1}{Q(i)}}} \\tag{3-1}$$顾名思义，看公式3-1的形式，就不难发现，这就是所谓的交叉熵，计算可得 交叉熵 P Q Z W P $H(P,P) = 2$ $H(P,Q) = 2.25$ $H(P,Z) = \\frac{11}{4}+\\frac{1}{4}\\log_2 {\\frac{4}{3}} = 2.85$ +inf Q $H(Q,P) = 2$ $H(Q,Q) = 1.75$ $H(Q,Z) = \\frac{7}{4}+\\frac{1}{2}\\log_2 {\\frac{4}{3}} = 1.96$ +inf Z $H(Z,P) = 2$ $H(Z,Q) = 1.375$ $H(Z,Z) = \\frac{7}{8}+\\log_2 {\\frac{4}{3}} = 1.29$ +inf W $H(W,P) = 2$ $H(W,Q) = 1$ $H(W,Z) = \\log_2 {\\frac{4}{3}} = 0.415$ $H(W,W) = 0$ 上表直观的展现的交叉熵的数值表现，PQZW依次不确定性越来越低，极端情况的W不确定性为0，即是确定的 交叉熵，用来高衡量在给定的真实分布下，使用非真实分布指定的策略消除系统的不确定性所需要付出努力的大小 总的来说，我们的目的是：让熵尽可能小，即存储空间小（消除系统的不确定的努力小）。（不要问为什么想要存储空间小，这都是钱更是效率和时间） 通过上表我们发现一个规律，为了让熵小，解决方案是：是用确定性更大的概率乘以确定性更小的存储因子，比如不确定性越大的概率分布，如P概率分布，其信息熵越大；基于同一真实（确定性）分布的情况下，套用不确定性更大的存储因子，如P的存储因子，得出的交叉熵越大 在机器学习中，即用测试结果集（样本结果集）的概率乘以训练出来的结果集存储因子，而在不断的训练过程中，我们要做的就是通过不断调整参数，降低这个值，使得模型更加的稳定，不确定性越来越小，即突出需要表征的数值的特点（白话文也就是分类的效果更好） 相对熵的由来有了信息熵和交叉熵后，相对熵是用来衡量两个概率分布之间的差异，记为 $D(P||Q) = H(P,Q) - H(P)$，也称之为KL散度$$D_{KL}(P||Q) = \\sum_i{P(i) \\log_a {\\frac{P(i)}{Q(i)}}}$$当 $P(i) = Q(i)$ 的时候，该值为0，深度学习过程也是一个降低该值的过程，该值越低，训练出来的概率Q越接近样本集概率P，即越准确，或者可以理解为相对熵一把标尺，用来衡量两个函数是否相似，一样就是0，当然，这种解释十分牵强，但是更直观 关于底数 $a$ 的选择问题，其实和概率分布的情况是分不开的。比如使用2进制编码，那么所能表示的不同情况的数量，$\\sum_{i=0}^N 2^i$，我们知道，指数函数变化率变化很大，不好分析，稳定性差。对数操作可以乘法变加法，指数放下来，是十分好用的数学工具（其实是一种变换域的思想，这种思想在整个信息论，统计学中处处可见） 比如使用 $ln()$ 的时候，对应的分布，其实是正态分布，很好理解，正太分布的底数是 $e$","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"}]},{"title":"Dota2机制总结","date":"2017-09-06T06:50:56.000Z","path":"2017/09/05/Dota2机制总结/","text":"【阅读时间】百科类型文章【内容简介】这是一份有关Dota2游戏机制的总结，核心目的是为了方便查阅，计算公式。针对人群是对数据和游戏机制有很大兴趣的高玩，从中你可能能了解如何通过击杀或得更多的经济，哪些操作可以躲避技能等等 版本信息：更新到7.09 金钱击杀英雄奖励获得金钱 = 110 + 连杀奖励 + （被击杀者等级 * 8） 连杀奖励 = 60 * （连杀数-2）[大于0] 助攻奖励 助攻英雄数 获得金钱 1 财产总和贫穷系数 × 财产总和排名系数 × ( 126 + 4.5 × 阵亡英雄等级 + 财产总和前期系数 × 90 + 财产总和系数 × 0.03375 ) 2 财产总和贫穷系数 × 财产总和排名系数 × ( 63 + 3.6 × 阵亡英雄等级 + 财产总和前期系数 × 67.5 + 财产总和系数 × 0.03375 ) 3 财产总和贫穷系数 × 财产总和排名系数 × ( 31.5 + 2.7 × 阵亡英雄等级 + 财产总和前期系数 × 45 + 财产总和系数 * 0.03375 ) 4 财产总和贫穷系数 × 财产总和排名系数 × ( 22.5 + 1.8 × 阵亡英雄等级 + 财产总和前期系数 × 31.5 + 财产总和系数 × 0.027 ) 5 财产总和贫穷系数 × 财产总和排名系数 × ( 18 + 0.9 × 阵亡英雄等级 + 财产总和前期系数 × 22.5 + 财产总和系数 × 0.02025 ) 7.11更新： 【新】（阵亡英雄财产综合 × 0.026 + 70） / 击杀涉及英雄的数量 【旧】财产总和前期系数 × {} + 财产总和系数 × {} 财产总和差异 = （ 敌方总经济/己方总经济 ） - 1 【最大值1，最小值0】 【财产总和系数】 = 财产总和差异 * 阵亡英雄财产总和 【财产总和前期系数】 = （敌方总经济 - 己方总经济）/4000 【最大值为1】 【财产总和贫穷系数】 = 1.3 - 0.1 * 财产总和排名（阵亡英雄经济在队伍中的排名） 【财产总和排名系数】 1/2/3/4/5个英雄中，最富的到最穷的分别为{1} / {0.7; 1.3} / {0.7; 1; 1.3} / {0.7; 0.7; 1.3; 1.3} / {0.7; 0.7; 1; 1.3; 1.3} Roshan200 团队奖励 150 - 400 击杀奖励 死亡掉钱损失不可靠金钱 = 50 + 财产总和 ÷ 40 5千经济 ➜ 175 1万经济 ➜ 290 2万经济 ➜ 550 复活复活时间 每级增加2秒 每到6级的倍数增加10秒 18级后每级增加4秒 买活花费买活金钱 = 100 + (英雄等级 英雄等级 1.5) + (游戏时间(s) * 0.25 ) 放弃比赛掉线超过5分钟后，所有金钱被队友平分 物理攻击伤害最终攻击伤害 $$ 最终攻击伤害 = \\\\ \\{ [\\text {MD} × (1 + \\sum \\text {PBD}) + \\text {FBD}] \\times \\text {CSM} - \\text {BD} \\}\\\\ \\times \\text {AVM} \\times \\text {ATM} \\times \\text {GDM} $$ wiki链接 MD(Main Damage) 主要攻击力【白字攻击力】主要攻击力 = 基础攻击力 + 主属性 除此之外，所有加成的攻击力都是【绿字攻击力】 PBD(Percentage bonus damage) 百分比攻击力加成这个加成是加法叠加 Tips: 圣者遗物可以增加60攻击力，一个出支配带头狼的VS，36%+30% = 66%，60 / 0.66 = 90，也就是说，白字攻击力达到90就等于这个英雄出了一个圣者遗物，相当的可怕。 技能 加成数值 头狼光环 30% 强化图腾 100%/200%/300%/400% 野性驱使 15%/26%/37%/48%（只影响狼人控制的单位） 授予力量 20%/30%/40%/50% (天赋 30%/40%/50%/60%) 神之力量 80%/120%/160% (友军A帐: 75%/100%/125%) 复仇光环 12%/20%/28%/36% (天赋 32%/40%/48%/56%) 祭品光环 15% FBD(Flat Bonus Damage) 固定值百分比加成 技能 加成数值 其他数值 臂章-邪恶之力 31 嗜血渴望 最高攻击力加成/英雄: 16/24/32/40 按照敌方英雄生命值百分比线性变化 战意 最高叠加层数: 5/7/9 每层攻击力加成: 18/24/30 持续时间：14s 极度饥渴 60/100/140 持续时间：14s 死亡契约 基于目标最大生命值的攻击力加成: 5%/7%/9% 持续时间：65s 精准光环 敏捷 20%/26%/32%/38% (天赋26%/32%/38%/44%) 范围：全地图 星体游魂 小兵6/9/12/15 英雄 12/24/36/48 (天赋92/104/116/128) 持续时间：9s 灵动迅捷 10/25/40/55/70/85/100 A帐115 持续时间：9s 卡尔-火 4/8/12/16/20/24/28 * 3 决斗 10/14/18 (天赋50/54/58) 永久存在buff 战斗嚎叫 70/100/130 持续时间：6s 月之祝福 14/22/30/38 光环范围：900 狼人 - 嚎叫 英雄 10/15/20/25 非英雄 4/6/8/10 夜晚翻倍 持续时间：13s 静电连接 每秒偷取 7/14/21/28 (天赋21/28/35/42) 链接时间：8s 持续时间：18s 支配死灵 最大灵魂数: 18/24/30/36 (A帐 22/30/38/46) 每个灵魂攻击力：2（天赋4） 折光 攻击次数: 3/4/5/6 (天赋6/7/8/9)攻击力加成: 20/40/60/80 持续时间：17s 魔化 20/40/60/80 持续时间：40/44/48/52s 长大 30/45/60 加主要攻击力 衰退光环 英雄死亡 30/35/40/45 小兵死亡 5 持续时间：30/40/50/60s CSM(Critical Strike Multiplier) 致命一击倍数 致命一击来源 几率% 伤害% DPS期望% 头狼 - 致命一击 20 200 20 血棘 - 致命一击 20 175 15 酒仙 - 醉拳 10/15/20/25 230 13/19.5/26/32.5 混沌一击 12 125/175/225/275 3/9/15/21 水晶剑 - 致命一击 20 175 15 大炮 - 致命一击 30 235 40.5 剑舞 20/25/30/35 200% 20/25/30/35 狼人 - 变身 40 160/180/200 24/32/40 恩赐解脱 15 230/340/450 19.5/36/52.5 殊死一搏 15 英雄 150/200/250/300 7.5/15/22.5/30 血棘 - 灵魂撕裂 100 140 140 忍术 100 150/175/200/225 12/10/8/6 天赋 -5 棒击大地 100 150/175/200/225 天赋+100 触发条件 暗杀 100 A帐 280 距离内所有敌人 海象神拳 100 350/A帐500 冷却 36/24/12 游戏中出现的红字代表的是减少前的物理伤害 BD(Blocked Damage) 被格挡伤害 伤害格挡来源 几率% 格挡伤害 圆盾 50 近战16 远程8 穷鬼盾 英雄100 非英雄50 近战20 远程10 先锋盾 50 近战70 远程35 赤红甲 - 坚盾 100 60 海妖外壳 100 12/24/36/48 伤害格挡不格挡物理伤害技能，守卫的攻击也不格挡 AVM(Armor Value Multiplier) 护甲值倍数见护甲 ATM(Armor Type Multiplier) 护甲类型倍数见攻击类型，英雄打英雄100% GDM(General Damage Multipliers) 一般伤害倍数见伤害调整 攻击类型基础打英雄护甲75%伤害 穿刺打英雄护甲50%伤害，基础护甲（小兵护甲）150%伤害 伤害调整伤害减免和加深除了回光返照，幽灵船，魔法护盾之外，伤害减免和加深的叠加为加法叠加 技能 来源 数值% 血之狂暴 加深接受输出 25/30/35/40 远处 减半 赎罪 加深接受 18/24/30/36 灵魂猎手 加深接受 20/30/40/50 守卫冲刺 加深接受 15 血肉傀儡 加深接受 20/25/30 200范围内最高 回光返照 减免接受 0 全免 4/5/6 A帐+1 刚毛后背 减免接受 背后 16/24/32/40 侧面 减半 奔袭冲撞 减免接受 A帐 40 持续4秒 过载 减免接受 5/10/15/20 幽灵船 减免接受 40/45/50 持续10秒 决斗 减免接受 A帐 100 持续6/7/8秒 魔法护盾 减免接受 60 1.6/1.9/2.2/2.5 钻地 减免接受 40 折射 减免接受 10/14/18/22 激怒 减免接受 80 持续4秒 陵卫斗篷 减免接受 4层 8/12/16/20 冷却6/5/4/3 寒冬诅咒 减免接受 100 3.25/4/4.75 战斗饥渴 降低输出 A帐 30 持续10秒 白银之锋 降低输出 50 持续5秒 伤害无效化伤害实例仍然存在，如果一些与伤害触发相关的事件并且没有低于伤害阈值的伤害，仍然会触发伤害事件 技能或物品名称 说明 无天光盾 110/140/170/200 天赋 +300 15s持续时间 回光返照 3/4/5(A 5/6/7) 伤害转化为治疗 凝魂之类 5次 大于50点的伤害抵挡120点 尖刺外壳 2.25s持续时间 无效化每个玩家的第一次伤害 守护天使 6/7/8 (A 8/9/10) 物理伤害无效化 虚妄芝诺 7/8/9(天赋 +2) 持续时间结束受到被无效化的伤害 折光 次数 3/4/5/6(天赋 +3) 忽略低于5点的伤害 活体护甲 所有类型伤害无效化 20/40/60/80 次数 4/5/6/7(天赋 +4) 持续15s 低于5伤害忽略 极寒之拥 持续时间4s 无效物理伤害 技能攻击伤害技能伤害计算魔法伤害受到魔法抗性影响，技能伤害可以由智力获得增强 $$ 增强数值 = [初始智力 + (当前等级 - 1) \\times 智力成长] / 14 / 100 + 技能增强天赋 $$ $$ 技能最终伤害 = 技能伤害数值 \\times (1 + 增强数值) \\times \\\\ \\prod_{i=1}^n{(1 - 魔法抗性增加_i)} \\times\\prod_{i=1}^n{(1 + 魔法抗性降低_i)} $$ 技能增强天赋远古冰魄10：8% 蝙蝠骑士15：5% 人马20：10% 死亡先知10：5% 干扰者20：10% 大地之灵20：15% 灰烬之灵10：8% 矮人直升机10：6% 杰奇洛10：8% 拉西克20：5% 莉娜20：6% 莱恩20：8% 马格纳斯10：15% 米拉娜15：5% 食人魔魔法师25：15% 殁境神蚀者25：8% 凤凰20：8% 帕克20：10% 拉比克20：8% 暗影恶魔15：8% 影魔15：6% 风暴之灵25：10% 伐木机20：5% 修补匠15：4% 孽主15：12% 维萨吉25：20% 风行者20：15% 技能伤害类型分为：魔法伤害，物理伤害，纯粹伤害 大部分伤害为魔法伤害 物理伤害技能炼金术士：酸性喷雾 炼金术士：不稳定化合物 敌法师：法力损毁 斧王：反击螺旋 兽王：野性飞斧 赏金猎人：暗影步 钢背兽：针刺扫射 人马：反击 克林克兹：灼热之箭 戴泽：剧毒之触 戴泽：暗影波 死亡先知：驱使恶灵 主宰：无敌斩 昆卡：潮汐使者 拉西克：恶魔的赦令 噬魂鬼：盛宴 剃刀：风暴之眼 斯拉达：鱼人碎击 斯拉达：深海重击 狙击手：爆头 工程师：感应地雷 工程师：爆破起飞 潮汐猎人：锚机 熊战士：怒意狂击 冥界亚龙：幽冥剧毒 编织者：虫群 纯粹伤害技能祸乱之源：蚀脑 祸乱之源：噩梦 刃甲：反弹伤害 嗜血狂魔：血之祭祀 嗜血狂魔：割裂 陈：忠诚考验 死亡先知：吸魂巫术 末日使者：末日 魅惑魔女：推进 谜团：午夜凋零 祈求者：电磁脉冲 祈求者：阳炎冲击 莉娜：神灭斩A帐 美杜莎：石化后秘术异蛇 司夜刺客：尖刺外壳 全能骑士：洗礼 殁境神蚀者：奥术天球 帕吉：肉狗 痛苦女王：超声波冲击 沉默术士：智慧之刃 幽鬼：荒芜 圣堂刺客：灵能之刃 伐木机：锯齿飞轮 伐木机：伐木锯链 伐木机：带树木死亡旋风 修补匠：激光 骨灰 攻击速度基础攻击间隔 BAT英雄在没有额外攻速加成的情况下每两次攻击间的时间间隔 攻击速度 ISA 面板中英雄增加的攻击速度 由装备获得的攻击速度加成 每个英雄基础100点基础攻速 由Debuff造成的攻速减低 攻击速度计算公式 $$ 每秒攻击的次数 = \\frac{(100 + IAS) × 0.01} {BAT} $$ $$ 每次攻击的时间 = \\frac{1}{每秒攻击的次数} $$ 攻击速度 效果 -80 五分之一BAT时间来攻击 -75 四分之一BAT时间来攻击 -66 三分之一BAT时间来攻击 -50 二分之一BAT时间来攻击 +00 正常状态 +100 * n （1+n）倍攻击速度 根据表格我们可以知道减攻速的技能在基础攻速很高的情况下基本没有什么效果，但是越接近0速度，减速效果越明显 增加攻击速度技能列表 技能 增加数值 持续时间s 魔霭诅咒 10/20/30/40 4.5 雷肤兽 - 暴怒 75 8 雷肤兽 - 战鼓光环 15 光环范围 900 天穹守望者 - 磁场 50/60/70/80 3.5/4.5/5.5/6.5 淘汰之刃 30 6 A帐10 成功淘汰 野性之心 15/25/35/45 光环范围 900 扫射 130 天赋 +70 4/6/8/10 熊怪 - 迅捷光环 15 光环范围 900 飓风之力 100 5 狂战士之血 220/260/300/340 剩下10%生命值最高 灵动迅捷 10/25/40/55/70/85/100/A115 9 卡尔 - 雷 2/4/6/8/10/12/14 * 3 开关 过载 40/50/60/70 开关 强攻 65/90/115/140 5 狂暴 50/60/70/80 3/4/5/6 炽魂 每层40/55/70/85(天赋 75/90/105/120) 10 最高3层 德鲁伊 - 狂猛 10/20/30/40 18/22/26/30 跳跃 16/32/48/64 (天赋 +100) 5 死灵射手光环 5/7/9 光环范围 900 暗夜猎影 45/60/75/90 夜晚 嗜血术 30/40/50/60 (天赋 +40) 30 幻影突袭 130 4s or 4次攻击 战斗专注 60/120/180 5 热血战魂 15/20/25/30 (105/140/175/210) 每次攻击同个目标 超强力量 400 15 or 3/4/5/6次攻击 黄泉颤抖 64 3/4/5/6 集中火力 500 20 寒冬诅咒 70 3.25/4/4.75 降低攻击速度技能比较有效果的降低攻速的技能 烈火精灵：80/100/120/140 不可侵犯：40/70/100/130，蝮蛇突袭：40/60/80 重生：75 黄泉颤抖：64 小狼-致残：60 冰封魔印：30/40/50/60 雷霆一击：25/35/45/55 原始咆哮：50 冰霜新星：20/30/40/50 液态火：20/30/40/50 石化凝视：50 夜魔虚空：50 冰眼：45 豪猪：10/20/30/40 冰火交加：28/32/36/40 毒龙法球：10/20/30/40 全能光环：10/18/26/34 护甲白字护甲$$敏捷 = 基础敏捷 + (等级 - 1) * 敏捷成长$$ $$白字护甲 = 基础护甲 + ( \\frac{敏捷}{7})$$ 护甲值倍数$$护甲值倍数 = 1 - \\frac{0.06 \\times 护甲值}{1 + 0.06 \\times |护甲值| }$$ 护甲值倍数倍数和护甲值的相关曲线 相关曲线 纵坐标是护甲值倍数，横坐标是现在英雄的护甲，不同颜色的线是此时减少的护甲（越上面的线减的越多） 有效生命值 （EHP）有效生命值 = 总生命值 ÷ 护甲值倍数 $$ 实时有效物理生命值 = 当前生命值 \\div (1 - \\frac{0.06 \\times 当前总护甲值}{1 + 0.06 \\times |当前总护甲值| }) $$ $$ 实时有效魔法生命值 = 当前生命值 \\div (0.75 \\times (1 - 装备提供抗性_1) \\times \\ldots \\times (1 - 装备提供抗性_n)) $$ 护甲调整增加护甲的技能 技能 加成数值 持续时间s 黑龙 - 龙肤光环 3 光环范围 900 狂战士怒吼 40 2/2.4/2.8/3.2 编织 0.75/1.0/1.25 每秒 18/24/30 24 龙族血统 3/6/9/12(天赋 翻倍) 永久 霜冻护甲 3/5/7/9 40 战斗嚎叫 10/15/20 6 变形术 4/6/8 变形状态 寒冰盔甲 8 45 战吼 5/10/15/20 8 活性护甲 5/10/15/20 每层 1/1.2/1.4/1.6 10/13/16/19 崎岖外表 3/4/5/6 永久 巨魔 - 狂战士之怒 6 切换 减低护甲的技能 技能 降低数值 持续时间s 酸性喷雾 4/5/6/7 (天赋 +4) 16 远古 - 亵渎 50% 6 粘稠鼻液 1/1.4/1.8/2.2 最高层数4(8) 英雄5 小兵10 实相裂隙 3/4/5/6 8 编织 0.75/1/1.25每秒 (18/24/30) 24 自然秩序 基础护甲：40%/60%/80%/100% 光环范围 275 火人 - 攻击 每次1点 上限10 5 击中刷新时间 激流 2/3/4/5 8 范围 320 风暴之眼 0.7/0.6/0.5 (天赋 -0.1) 打击1次1点 30 魔王降临 3/4/5/6 光环范围 900 侵蚀雾霭 10/15/20 18 隐匿 2/4/6/8 10 巨浪 3/4/5/6 (天赋 +5) 4 死亡旋风 敏捷损失 * 0.14 14 恐怖波动 3/4/5/6 1400距离 300范围 15 虫群 1.4/1.25/1.1/0.95 攻击一次1点 16 护甲相关装备强袭 +5 玄冥盾牌系列 +2 勋章 +7 天鹰 +2 炎阳纹章 +10 祭品 +4 黯灭 -7 勋章 -7 炎阳纹章 -10 强袭 -5 枯萎之石 -2 疯脸 -5 闪避机制闪避与致盲都会在攻击完成（弹道击中）时有一定几率触发 叠加与计算多个闪避来源乘法叠加 上下坡落空几率如果攻击者处于比目标更低的位置时，远程攻击会有25%的几率落空。 攻击者和目标之间的地形的高低差异实在击中目标时决定的，中路对线过程中，可以使用弹道飞行过程位移来保持和目标的同样地形高度保证必中 飞行单位无上下坡落空几率 计算公式$\\prod_{i=0}^n$ 的含义是把i=0到n所有的项相乘 $$ 落空几率 = \\prod_{i=0}^n (1 - 闪避来源_i) \\times \\prod_{j=0}^n (1 - 致盲来源_j) \\times 上下坡落空几率 $$ $$ 命中几率 = 1 - \\prod_{i = 0}^n{(1 - 必中/克敌先机来源_i)} $$ $$ 最终命中几率 = 1 - 落空几率 \\times (1 - 命中几率) $$ $$ 最终落空几率 = 落空几率 \\times (1 - 命中几率) $$ 公式只是为了程序数值计算使用，是需要记住：每一次攻击要绕过所有的闪避成功命中，只有当所有的闪避都失败了，这次攻击才可以造成伤害。所以说，出很多个闪避装备，在一定程度上对物理核心非常克制，这时候物理核心必须出金箍棒 闪避来源 技能或物品名称 闪避几率% 敌法师 - 20级右天赋 15 磁场 100 3.5/4.5/5.5/6.5s 醉拳 10/15/20/25 一段时间的100%闪避 赏金猎人 - 25级右天赋 25 蝴蝶 35 人马 - 15级右天赋 10 克林克兹 - 20级右天赋 20 虚空 - 25级右天赋 20 黑暗贤者 - 10级右天赋 12 天堂之戟 25 噬魂鬼 - 20级右天赋 15 狼人 - 20级右天赋 15 美杜莎 - 15级左天赋 15 米波 - 20级右天赋 10 大圣 - 10级左天赋 12 模糊 20/30/40/50 猴子 - 20级左天赋 15 炎阳纹章 20 炎阳纹章- 队友使用 - 日耀 20 7s 斯温 - 20级左天赋 20 闪避护肤 20 圣堂刺客 - 15级左天赋 12 风行 100 3/4/5/6a 致盲来源 技能或物品名称 落空几率% 醉酒云雾 70 4s 麻痹之咬 30/40/50/60 2s 致盲之光 80 3/4/5s 伤残恐惧 白天10 3s 夜晚50 5/6/7/8s 辉耀 - 辉耀灼烧 17 烟雾 40/50/60/70 6s 激光 100 3/3.5/4/4.5s 小兵 6s 近战旋风飞斧 60 4/5/6/7s 克敌机先来源为一种攻击特效，防止该次攻击落空，用来反制闪避，致盲，以及远程单位上下坡的25%几率落空，也能够防止近战攻击由于目标在攻击之前超过了350距离而落空 但是攻击弹道依旧可以躲避 对建筑物无效 技能或物品名称 备注 不会落空为100% 强化图腾 带有Buff的一次攻击不会落空 棒击大地 不会落空的即时攻击 金箍棒 每次攻击带有克敌先机 复仇 破影一击不会落空 窒息之刃 不会落空的即时攻击 白银之锋 - 暗影步 破影一击不会落空 暗杀 需要A帐 自然庇护 破影一击不会落空 海象神拳！ 不会落空 死亡守卫 需要A帐 不会落空 必中来源必中防止一个单位受到的任何攻击落空 血棘的灵魂撕裂，岗哨守卫，炎阳纹章给敌方使用提供35%的必中效果 移动速度英雄移动速度表 叠加相似的装备提供的移动速度不叠加，除了风帐 多个鞋类物品不叠加 夜叉 散夜对剑 幻影斧不叠加 多个战鼓或风灵之纹不叠加 风灵之纹和战鼓鞋类物品叠加 公式移动速度 = （基础移动速度 + 具体移动速度加成） * （1 + 百分比移动速度加成和减速的和） 转身速度转身速率表 英雄 基础转身速率 转180°时间 凤凰, 噬魂鬼, 影魔, 石鳞剑士, 虚空假面, 蝙蝠骑士, 钢背兽 1 0.094 撼地者 0.9 0.105 风暴之灵, 风行者, 马格纳斯 0.8 0.118 卓尔游侠, 圣堂刺客, 巨牙海民, 帕吉, 拉比克, 狙击手, 艾欧, 邪影芳灵 0.7 0.135 米波 0.65 0.145 不朽尸王, 主宰, 伐木机, 修补匠, 先知, 全能骑士, 力丸, 发条技师, 变体精灵, 复仇之魂, 大地之灵, 天穹守望者, 孽主, 宙斯, 幻影刺客, 幻影长矛手, 戴泽, 斧王, 斯温, 昆卡, 暗影恶魔, 沉默术士, 炼金术士, 矮人直升机, 祸乱之源, 赏金猎人, 远古冰魄, 酒仙, 陈, 露娜, 食人魔魔法师, 黑暗贤者, 齐天大圣, 龙骑士 0.6 0.157 上古巨神, 亚巴顿, 光之守卫, 克林克兹, 兽王, 军团指挥官, 冥界亚龙, 冥魂大帝, 剃刀, 剧毒术士, 半人马战行者, 司夜刺客, 哈斯卡, 嗜血狂魔, 天怒法师, 娜迦海妖, 寒冬飞龙, 小小, 工程师, 巨魔战将, 巫医, 巫妖, 帕克, 帕格纳, 干扰者, 幽鬼, 德鲁伊, 恐怖利刃, 拉席克, 敌法师, 斯拉克, 斯拉达, 暗夜魔王, 暗影萨满, 末日使者, 术士, 杰奇洛, 树精卫士, 死亡先知, 殁境神蚀者, 水晶室女, 沙王, 混沌骑士, 潮汐猎人, 灰烬之灵, 熊战士, 狼人, 痛苦女王, 瘟疫法师, 祈求者, 神谕者, 米拉娜, 维萨吉, 编织者, 美杜莎, 育母蜘蛛, 莉娜, 莱恩, 裂魂人, 谜团, 魅惑魔女 0.5 0.188 大部分英雄的转身速度都比较慢，第一梯队1-0.7速率几个英雄在这方面有明显的优势 特殊说明 艾欧和石鳞剑士，执行命令不需要转身，如果是技能需要转身，但使用物品不需要转身 无敌斩 无影拳 凤凰冲击 烈日炙烤 期间，不需要转身执行 使用 洪流 暗影护符 微光披风 魔瓶 净化药水 魔法芒果 治疗药膏 都不需要转身面向目标 影响转身速率的技能 技能名称 效果 蝙蝠骑士 - 粘性燃油（叠油） 转身速率减缓：70% 持续时间：8s 美杜莎 - 石化凝视 转身速率减缓：35% 持续时间：5/6/7 石鳞剑士 - 地雷滚滚（对自身） 转身速率：0.063初始/跳跃/反弹后转身速率加成：0.086转身速率加成持续时间：0.25 凤凰 - 烈日炙烤（对自身） 转身速率：0.013 = 每秒转25°（龟速） 魔法抗性魔法抗性除了米波35%，维萨吉10%魔法抗性外，其他英雄都为25%基础魔法抗性 魔法抗性乘法叠加，不同的提高魔法抗性的装备可以叠加 魔法抗性加成来源 技能或物品名称 加成数值%及备注 法术护盾 26/34/42/50 小马or小熊怪光环 英雄5 非英雄20 可叠加 魔抗斗篷 15 微光披风 15 被动 微光披风 - 微光 45 5s 0.6s渐隐时间 挑战头巾 25 狂战士之血 20/30/40/50 最大10%生命值 洞察烟斗 30 被动 洞察烟斗光环 10 腐肉堆积 6/8/10/12 失效力场 10/14/18/22 腐蚀皮肤 10/15/20/25 魔法抗性减少来源 技能或物品名称 减少数值% 备注 冰霜漩涡 15/20/25/30 16s 0.5s粘滞时间 自然秩序 40/60/80/100 光环范围350 1s粘滞时间 虚化冲击 40 敌方3s 友方4s 幽灵形态 40 4s 幽魂护罩 20 3/3.5/4/4.5 衰老 30/40/50/60 3.5 上古封印 30/35/40/45 3/4/5/6 纷争面纱 25 16 魔法抗性100%来源 技能或物品名称 备注 黑皇杖 10/9/8/7/6/5 牺牲 跳跃时间or持续5s 剑刃风暴 5s 狂暴 3/4/5/6 (天赋+1s) 石化凝视 3s 天赋5s 驱逐 4/5/6/7 命运赦令 3/3.5/4/4.5 魔法吸收护盾魔法吸收护盾计算是计算魔抗后的吸收数值，魔抗越高，护盾效果越好 任何类型魔法护盾无法叠加，同时吸收伤害 技能或物品名称 吸收数值 烈火罩 50/200/350/500 （天赋 +500） 挑战头巾 - 绝缘 325 持续12s 洞察烟斗 - 法术护盾 400 持续12s 施法距离幕布笔记链接 物品被动效果叠加独立叠加 攻击力 属性加成 魔法值/生命值 生命恢复速率/魔法恢复速率（基础速率 * 加成倍数） 攻击速度加成 护甲加成 分裂区域 移动速度加成 乘法叠加出现边缘递减效应$$加成 = 1 - (1-x) \\times (1-y) \\times (1-z) \\times \\ldots$$其中 $x y z$ 都表示一个百分比 魔法抗性乘法叠加 一个100点魔法伤害的技能 英雄本身25%魔法抗性，伤害变为 100 * （1 - 25%） = 75 再装备挑战头巾，再降低30%，伤害变为 75 * （1 - 30%） = 52.5 躲避躲避是一种躲避弹道的行为，更确切的说，是使弹道完全失去跟踪目标能力的行为。白话文就是：秀操作，骚 躲避技能的方式技能以下技能在施法时能躲避弹道 炼金术士：化学狂暴 酒仙：元素分离 混沌骑士：混沌之军 噬魂鬼：感染``幻影斧：镜像 变体精灵：波浪形态 娜迦海妖：镜像 幻影长矛手：神行百变``凤凰：超新星 帕克：相位转移 力丸：绝杀秘技 风暴之灵：球状闪电 传送所有的真闪烁都能躲避弹道，躲避发生在使用技能移动时 敌法师：闪烁 闪烁匕首：闪烁 远行鞋：传送 艾欧：传送(只有艾欧传送过去时可以躲避) 先知：传送 帕克：灵动之翼 痛苦女王：闪烁 熊灵：回归 回城卷轴：传送 孽主：黑暗之门 编织者：时光倒流 陈：忠诚考验 光之守卫：召回 变体精灵：替换复制品 隐身所有能获得隐身状态的技能技能都能躲避弹道，除非敌人的在弹道到达之前使用了反隐，但是必须要注意不同技能的渐隐时间 隐藏变为临时性的隐藏不能躲避弹道。 躲避与变为隐藏无关，而是与技能本身有关。这意味着隐藏技能不一定都能躲避弹道， 但是，利用合适的时机，可阻止弹道或一般技能，击中施法者或目标。 隐藏来源有一下技能 酒仙：元素分离 混沌骑士：混沌之军 大地之灵：残炎魔咒 噬魂鬼：吸收 幻影斧：镜像 娜迦海妖：镜像 殁境神蚀者：星体禁锢 幻影长矛手：神行百变 凤凰：超新星 帕克：相位转移 力丸：绝杀秘技 暗影恶魔：崩裂禁锢 巨牙海民：雪球 无敌变为无敌不能躲避弹道，但是可以在击中时减轻或使其效果无效。 攻击伤害和技能伤害会被忽略。有一些技能可以影响无敌单位。 无敌来源 祸乱之源：噩梦 酒仙：元素分离 混沌骑士：混沌之军 大地之灵：残岩魔咒 灰烬之灵：无影拳 灰烬之灵：激活残焰 风帐：龙卷风 虚空假面：时间漫游 佣兽：石像形态 祈求者：强袭飓风 主宰：无敌斩 噬魂鬼：吸收 噬魂鬼：感染 幻影斧：镜像 变体精灵：波浪形态 娜迦海妖：镜像 娜迦海妖：海妖之歌 殁境神蚀者：星体禁锢 幻影长矛手：神行百变 凤凰：超新星 帕克：相位转移 力丸：绝杀秘技 暗影恶魔：崩裂禁锢 狂风：龙卷风 风暴之灵：球状闪电 巨牙海民：雪球 可以被躲避的弹道任何单位和英雄的所有物理攻击的弹道都可以躲避 可以被躲避的技能亚巴顿：迷雾缠绕 赏金猎人：投掷飞镖 酒仙：醉酒云雾 钢背兽：粘稠鼻涕 育母蜘蛛：孵化蜘蛛 混沌骑士：混乱之箭 陈：赎罪 戴泽：剧毒之触 龙骑士：神龙摆尾 大地：投掷巨石 撼地者：回音击 虚灵之刃：虚化冲击 变体精灵：变体攻击 泥土傀儡：投石 娜迦海妖：诱捕 食人魔魔法师：引燃 神谕者：气运之末 幻影刺客：窒息之刃 幻影长矛手：灵魂之矛 痛苦女王：暗影突袭 阿托斯：致残 天怒法师：震荡光弹 狙击手：暗杀 斯温：风暴之拳 潮汐猎人：巨浪 修补匠：导热飞弹 复仇之魂：魔法箭 冥界亚龙：蝮蛇突袭 维萨吉：灵魂超度 风行者：束缚击 寒冬飞龙：碎裂冲击 冥魂大帝：冥火暴击 不可以被躲避的技能炼金术士：不稳定化合物 天穹守望者：闪光幽魂 爱人直升机：追踪导弹 哈斯卡：牺牲 拉西克：闪电风暴 巫妖：连环霜冻 莉娜：神灭斩 莱恩：死亡一指 美杜莎：秘术异蛇 米拉娜：流星风暴 瘟疫法师：死亡脉冲 痛苦女王：痛苦尖叫 拉比克：技能窃取 天怒法师：奥法鹰隼 幽鬼：幽鬼之刃 小小：投掷 树精卫士：寄生种子 巨牙海民：雪球 寒冬飞龙：碎裂冲击弹射 巫医：麻痹药剂 视野Dota2中，掌握视野掌握主动权，通常来说，白天视野1800，夜晚视野800，装备银月之晶获得300额外夜间视野，吞噬获得150夜间视野 视野例外英雄模型视野不同列表 英雄名称 白天 夜晚 斯拉克 1800 1800 暗夜魔王 800 1800 狙击手 1800 1100 蝙蝠骑士 1200 800 赏金猎人 1800 1000 增加视野技能 英雄名称 白天视野 夜间视野 露娜 - 月之祝福 1800 800/1050/1300/1550/1800 狼人 - 变身 1800 800/1800 寒冬飞龙 - 严寒灼烧 1800 800/1200 裂魂人【10级】天赋夜晚视野 +400 寒冬飞龙【15级】天赋夜晚视野 +500 斯拉达【20级】天赋夜晚视野 +1000 野怪也召唤单位视野 单位名称 白天 夜晚 尸王 - 不朽僵尸 1400 1400 丘陵巨魔牧师 1400 1400 死灵龙 - 佣兽（高空视野） 390 390 冥魂大帝 - 骷髅兵 800 600 术士 - 地狱火 1800 1800 酒仙 - 大地 1800 800 先知 - 大树人 500 500 兽王 - 战鹰 1000 1000 上古巨神 - 星体游魂（高空视野） 400 400 先知 - 树人 500 500 死灵射手/死灵战士 1300/1400/1500 800 酒仙 - 烈火 1800 800 德鲁伊 - 熊灵 1400 800 祈求者 - 熔炉精灵 1200 800 酒仙 - 狂风 1800 800 狗头人 1400 800 甲虫 321 321 狼人 - 精灵狼 1200 800 谜团 - 精神体 1200 800 蜘蛛 - 小蜘蛛 700 700 兽王 - 豪猪 1400 800 豺狼人刺客 400 400 远古岚肤兽 1400 800 远古雷肤兽 1400 800 鹰身女妖侦察者 1800 1800 鹰身女妖风暴巫师 1800 1800 视野类型幕布笔记链接","tags":[{"name":"Dota2","slug":"Dota2","permalink":"https://charlesliuyx.github.io/tags/Dota2/"},{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://charlesliuyx.github.io/tags/Data-Analysis/"},{"name":"Wiki","slug":"Wiki","permalink":"https://charlesliuyx.github.io/tags/Wiki/"}]},{"title":"【直观详解】Logistic Regression","date":"2017-09-04T07:21:50.000Z","path":"2017/09/04/LogisticRegression学习笔记/","text":"【阅读时间】17min - 22min【内容简介】从不同角度解释为何使用Logistic回归模型，解读模型的现实意义，详细解读为何使用以及什么是交叉熵损失函数。并详细梳理符号表达，对公式不再恐惧 什么是【回归（Regression）】回归（Regression）是一项模拟技术，用来从一个或多个解释变量中预测输出变量的值 什么是及为什么【Logistic Regression】回归（Regression）是用来预测的，比如给你一组虫子的腿长和翅膀长数据，让你判断虫子是A类虫还是B类虫。 逻辑回归则是用来预测二进制输出变量取值（如：是/不是）的预测技术 即输出变量只有两个值得预测技术 下文中将会从不同的角度 概率论角度首先，需要回忆一下几个概念 【大数定理】 $$ \\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{i=1}^n {X_i} = \\mu $$ 不断的采样一个随机变量，得到n个值，当n趋向于正无穷的时候，这个平均值就收敛于随机变量的期望 【中心极限定理】 大量相互独立{条件1}的随机变量，其均值的分布以正态分布{结论}为极限{条件2} 【贝叶斯公式】 默认你已经对条件概率了若指掌（在某件事情已经发生的情况下另一件事发生的概率），关于贝叶斯方法的前世今生，这个链接或许可以帮到你。 那贝叶斯公式是如何推出来的？ 问题描述我们需要求的问题是：你在校园里面随机游走，遇到了N个穿长裤的人（但是可能因为你高度近视你无法看出他们的性别），问，这N个人里面有多少个女生，多少个男生，即，穿裤子的人里面有多少个女生 解决过程 $$ 穿裤子的人中的女生比例 = \\frac{穿长裤的女生人数}{穿长裤的总人数} =\\\\ \\frac {U\\times P(Girl)\\times P(Paints|Girl)}{U\\times P(Boy)\\times P(Paints|Boy) + U\\times P(Girl)\\times P(Paints|Girl)}\\tag{1-1} $$ 化简上式，可以发现其实分母合起来就是 $P(Paints)$ ，分子其实就是既穿裤子又是女孩，整理得 $$ P(Girl|Paints) = \\frac{P(Girl) \\times P(Paints|Girl)}{P(Paints)} $$ 再一般化，用A表示穿裤子的，B表示女生$$P(B|A) = \\frac{P(B)\\times P(A|B)}{P(A)} = \\frac{P(AB)}{P(A)}\\tag{1-2}$$上式就是贝叶斯公式的一般形式，我们在推导中发现，正常人类对频率的感知和理解速度要高于对概率的。 比如“穿长裤的女生人数”这个概念，用总人数乘以女人比例，得出女生人数，再用女生人数乘以女生中穿裤子人数的比例得到穿裤子的女生人数。这一串推导感觉毫无困难。但如果读成：在A发生条件下，发成B的概率，会让人乍看下，感到有一定的理解困难。 我们常说Sense，我觉得这就是一种敏感，对条件概率表达方式的敏感，在你看到的时候，抓住那个最关键的点，不存在任何的迷惑 那Logistic Function和贝叶斯公式有什么联系呢？ 如果我们把公式（1-1）也符号化，$B_1$ 表示女生，$B_2$表示男生，$A$ 表示穿裤子$$P(B_1|A) = \\frac {P(B_1)P(A|B_1)}{P(B_2)P(A|B_2) + P(B_1)P(A|B_1)}\\tag{1-3}$$右边同时除以 $P(B_1)\\times P(A|B_1)$ ，并定义 $a = \\ln{\\left( \\frac{P(B_1)P(A|B_1)}{P(B_2)P(A|B_2)}\\right)}$ 直接由公式(1-3)可得到$$f(a) = \\frac{1}{1 + e^{-a}} \\tag{1-4}$$很熟悉的形式，其实就是logistic函数的一般形式（对数几率函数），而这个函数的值就是 $f(a)$ ，很明显，是一个概率 另一个很重要超级重要的常识就是：正态分布的的累计分布函数（就是从负无穷到x积分）和概率分布函数长得样子很像Logistic累计分布函数和概率密度函数，可能看到这句话很多人就已经真相大白了，应给无论从中心极限定理出发，还是从统计学概率论角度来看，概率分布存在的价值是为了描述自然界（现实）中的随机事件，构造函数本身就十分重要，不同的规律需要不同的函数去拟合 正太分布概率密度函数（左）累计密度函数（右） Logistic函数概率密度函数（左）累计密度函数（右） 统计学角度动机 - 需要解决什么问题在现实生活中，有时候需要探究某一事件 $A$ 发生的概率 $P$ （0 - 1 之间的一个数）与某些因素 $\\mathbf X = (X_1, X_2, \\ldots, X_p)’$ 之间的关系。（其中1到p是各种不同的因素） ☆ 【核心问题】考虑到很多情况下，$P$ 对 $\\mathbf X$ 的变化并不敏感，即 $\\mathbf X$ 需要发生很大的变化才能引起 $P$ 的微弱改变 比如，农药的用量和杀死害虫的概率之间，在农药用量在很小的范围内增长的时候，因为药效不够，杀死害虫的概率增长很慢。 因此，我们要构造一个关于 $P$ 的函数 $\\theta(P)$ ，使得它在 $P = 0$ 或 $P = 1$ 附近，$P$ 的微小变化对应 $\\theta(P)$ 的较大改变，同时，$\\theta(P)$ 要尽可能的简单。于是，我们可以构造一个函数（注意：构造函数是数学中很有效的手段，我们需要什么特性就用什么方法来构造一个满足我们需求的函数）c$$\\frac {\\partial \\theta(P)}{\\partial P} =\\frac{1}{P} +\\frac{1}{1-P}$$根据上述公式可以解得$$\\theta(P) =\\ln\\left(\\frac{P}{1-P}\\right)$$ 可视化 这个 $\\theta(P)​$ 就是Logit变换，可以看到，这个函数很符合我们的要求： $P = 0​$ 或 $P = 1​$ 附近，$P​$ 的微小变化对应 $\\theta(P)​$ 的较大改变 方案 - 如何解决这个问题为了建立因变量 $P$ 与自变量 $\\mathbf X$ 之间的合理变动关系，一个很自然的假设就是线性关系，也就是：$$P = \\mathbf X’ \\boldsymbol{\\beta}$$其中 $\\boldsymbol \\beta = (\\beta_1,\\beta_1,\\ldots,\\beta_p)$ 表示每一个不同因素对最终概率 $P$ 产生的影响（这个也可以写作，权重weight） 由需求可知，在某些情况下，$P = 0$ 或 $P = 1$ 附近，$P$ 对 $\\mathbf X$ 的变化并不敏感，简单的线性关系不能反映这一特征。此时，构造的 $\\theta(P)$ 就派上用场了$$\\ln\\left(\\frac{P}{1-P}\\right) = \\mathbf X’ \\boldsymbol{\\beta}$$进行一系列的公式推导有$$\\ln\\left(\\frac{P}{1-P}\\right) = \\mathbf X^\\mathrm T \\boldsymbol{\\beta} \\implies \\frac{P}{1-P} = e^{\\mathbf X^\\mathrm T \\boldsymbol{\\beta}} \\implies P = \\frac{e^{\\mathbf X^\\mathrm T \\boldsymbol{\\beta}}}{1 + e^{\\mathbf X^\\mathrm T \\boldsymbol{\\beta}}}$$则上述最后推出的就是Logistic回归模型 机器学习角度周志华《机器学习》，3.3 对数几率回归笔记 和统计学角度相同，我们的目的是依旧是完成一个二分类任务，输出标记 $y \\in {0,1}$ ，而线性回归模型产生的预测值 $z = \\boldsymbol w^{T}\\boldsymbol x + b$ 是实值，于是，我们需要把 z 转换为0/1值，最理想的是单位阶跃函数（unit-step function z &gt; 0➜y=1，z&lt;0➜y=1） 单单位阶跃函数不连续，不能微分，积分，求逆，于是我们希望找到能在一定程度上近似单位阶跃函数的替代函数（surrogate function），并希望它单调可微，答案很明显，就是对数几率函数（logistic function）$$y = \\frac{1}{1+e^{-z}}$$ z 为预测值，y 为输出，对数几率函数是一种Sigmoid函数【一种形状类似S的函数】，将$z = \\boldsymbol w^{T}\\boldsymbol x + b$ 带入上面的公式 $$y = \\frac{1}{1+e^{-(\\boldsymbol w^{T}\\boldsymbol x + b)}} \\implies \\ln(\\frac{y}{1-y}) = \\boldsymbol w^{T}\\boldsymbol x + b$$如果将 $y$ 作为 $\\mathbf x$ 作为正例的可能性，$1-y$ 为其反例的可能性$$\\frac {y}{1-y}$$上面的式子成为“几率”(odds)：表示 $\\mathbf x$ 是正例的相对可能性，对odds取对数得到“几率对数”(log odds，也就做logit) 生态学角度可以换一个角度来解读这个问题的前世今生 1798年的时候一个叫Malthus的英国牧师发现人口的变化率和人口的数目成正比，需要用数学的手法建立一个公式来表征这个现象，则，使用 $N(t)$ 这个函数来表示t时刻某个地区的总人口数（根据成正比）$$\\frac{dN(t)}{dt} = {rN(t)}$$ 其中，r是常数，表示 $N(t)$ 的变化率 直接解出这个方程$$N(t) = N_0e^{rt}$$这很明显是一个指数增长函数，其实也是种群增长的函数表示 但是问题也是很明显的：种群因为环境容量的限制一定是不能无限增长的，即，这个模型非常不靠谱，需要重新设计模型来复合现实中的情况。Pierre-François Verhulst 在1838年提出，构造一个函数$$\\frac{dN(t)}{dt} = {rN(t)}\\left(1 - \\frac{N(t)}{K}\\right)$$ K是一个常数，表示系统的容量（capacity） 令 $f(t) = \\frac{N(t)}{K}$ ，在方程两边同时除以 $K$ ，上述方程变为：$$\\frac{df(t)}{dt} = rf(1 - f)$$这也是Logistic方程的一般形式 总结从不同的角度来研究问题就会发现，其实很多时候我们解决一个问题具有一个相似的模式，包括大数定律，贝叶斯全概率公式是一切的基石和解决问题的主要工具 一个模型的建立规则依据数据的分布特征，而这里依托的一个关键信息就是：在靠近输入0，1两点的时候，y随x的变化不明显，线性模型没法很好的反应这个特征，所以就构造了一个逻辑回归模型来表示这个特征 并且Logistic回归模型的本质是一个概率模型，因为在描述该分类时，我们其实是以概率来衡量的 重要概念均方误差 Mean Squre Error MSE指参数估计值与参数真值之差平方的期望值，是一种目标函数（Objective Function），常用于线性回归$$MSE = \\frac{1}{n} \\sum_{t = 1}^n{(observed_t - predicted_t)}^2$$ 交叉熵 Cross Entropy又称为logloss，是Objective function的一种，也称Loss function or Coss Function 什么是熵我觉得这个问题必须搞明白一件事就是：什么是熵 Entropy 广义的定义是：熵是描述一个系统的无序程度的变量；同样的表述还有，熵是系统混乱度的度量，一切自发的不可逆过程都是从有序到无序的变化过程，向熵增的方向进行 有一个很神奇的解释是：熵字为火字旁加商。当时有位姓胡的学者作为普朗克的防疫。S(entropy)定义为热量Q与温度的比值，所以造字：熵 至于信息论上熵的概念更有意思，有兴趣可以转到 要理解这个Cross Entropy，必须了解它是用来干啥的？ 延伸：信息熵 交叉熵 相对熵的理解，需要跳转到另一篇笔记：什么是信息熵、交叉熵和相对熵 简单来说Cross Entropy可以表示可以度量最终训练结果于测试集的差异程度，MSE也是同样的作用。 换种更具体的说法：我们用p表示真实标记（训练样本标记）的分布，q是训练后的模型的预测标记（输出值标记）的分布，而交叉熵损失函数可以衡量p与q的相似性。 似然函数定义：给定联合样本值 $x$ 关于（未知 - 因为也是一边的自变量）参数 $\\theta$ 的函数$$L(\\theta|x) = f(x;\\theta)$$ $x$ 指联合样本随机变量 $X$ 取到的值，比如天气取值 $X$ =【晴，阴，雨，雪】$x$ = 晴 $\\theta$ 指未知参数，属于参数空间，比如正态分布的均值，方差等 $f(x;\\theta)$ 是密度函数，表示 $\\theta$ 参数下联合样本值 $x$ 的联合密度函数（所以这里不用|符号，|符号表达的意思是条件概率或条件分布） 从定义上，似然函数和密度函数是完全不同的两个数学对象：前者是关于 $\\theta$ 的函数，后者是关于 $x$ 的函数。中间的等号理解成函数值形式相等 这个等式表示的是对于事件发生的两种角度的看法。左边表示概率，右边表示可能性。要表达的含义都是：给定一个样本 $x$ 后，我们去测度这个样本出现的可能性到底有多大。说人话，比如样本空间是 $X =【晴，阴，雨，雪】$，函数表达的就是样本 $x$ = 晴在这个样本空间下发生的概率或可能性 从统计学的角度来说，这个样本的出现一定是基于一个分布的（比如二项分布，只正态分布等等），那么我们假设这个分布为 $f(x;\\theta)$ ，对于不同的 $\\theta$ 样本的分布不一样。 $f(x;\\theta)$ 函数表示的就是在参数 $\\theta$ 下 $x$ 出现的概率有多大（可以带入天气例子思考） $L(\\theta|x)$ 表示在给定样本 $x$ ，哪个参数 $\\theta$ 使得 $x$ 出现的可能性有多大。说人话，我们已经知道天气是晴天，哪个参数（可能是 $\\theta_1$ $\\theta_2$）使得这个函数值最大 对于Logistic Regression 为什么要用LogLoss - Cross Entropy了解了熵，和似然函数，我们可以开始看看在Logistic Regression的条件下为什么要用LogLoss，换句话也就是说，它一定有它的优势，我们采用，那么它有什么优势？ Logistic Regression的本质还是一个二分类问题，即Y = 0，or Y = 1 令 $P(Y=0|x) = \\pi(x)$ $P(Y=1|x) = 1 - \\pi(x)$ $y_i$ 表示i次试验，取值就是0 or 1（二分类问题） $\\pi(x) = \\frac{1}{1 + e^{-wx}}$ 是Logistic Function的表现形式，其中w相当于似然函数一节提到的 $\\theta$ 是需要求的参数（加深理解，其实在二分类问题中，Logistic函数就是一种形式上的概率分布的表现形式） 所以使用基本概率方法可以求解二分类的问题的似然函数 $$ \\ell(w) = \\prod_{i = 1}^{N} [\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i} $$ 注解：说白就和算扔N次硬币，一个连续正反事件串的概率是多少一个含义 看到乘法和指数，第一反应取对数，得到对数似然函数$$L(w) = \\sum_{i=1}^N{[y_ilog_a\\pi(x_i) + (1-y_i)log_a(1-\\pi(x_i))]}$$ 如果跟随我的步伐走到这一步，你会发现，这个形式，前半部分是“正例成立”的交叉熵，后半部是“反例成立”的交叉熵，说实话，叫做交叉熵和二项分布，伯努利过程分不开联系。在上面不远的地方已经详细定义了这几个符号代表的意思 我们发现，$-\\frac{L(w)}{N}$ 就是我们一直使用的Objective function or Loss Function or Cost Function（加负号才是最终的形式）。总之，训练的目的就是要求能够使得这个函数达到最小的参数，最终的目的还是计算出模型参数，就是 $w$ ，这个参数在上方的统计学角度，和机器学习角度都进行的讨论，重复阅读可以链接这些知识点 至于LogLoss的好处，一是取对数之后，乘法边加法，指数放下来，是凸函数，方便可以寻找最优解。二是加快了收敛速度，这里有个形象的步长比喻，可以想象成去了对数后，缩小了尺度，可以让最快梯度下降法要走的距离变短","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"}]},{"title":"Xpath-Wiki","date":"2017-08-28T19:00:33.000Z","path":"2017/08/28/Xpath使用指南/","text":"【阅读时间】查阅类文档【内容简介】Xpath相关使用法法和例子文档，以供查阅（➜ 后是对应语句的输出output） XPath 相关例子Note例子1123456789101112131415from lxml import etreesample1 = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;My page&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;Welcome to my &lt;a href=\"#\" src=\"x\"&gt;page&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;This is the first paragraph.&lt;/p&gt; &lt;!-- this is the end --&gt; &lt;/body&gt;&lt;/html&gt;\"\"\"def getxpath(html): return etree.HTML(html)s1 = getxpath(sample1) //绝对路径 text() 获取内容中的文字信息1s1.xpath('//title/text()') ➜ ['My page'] / 相对路径1s1.xpath('/html/head/title/text()') ➜ ['My page'] 获取属性src的值1s1.xpath('//h2/a/@src') ➜ ['x'] 获取所有属性href的值1s1.xpath('//@href') ➜ ['#'] 获取网页中的所有文本123456789101112131415s1.xpath('//text()')➜['\\n ', '\\n ', 'My page', '\\n ', '\\n ', '\\n ', 'Welcome to my ', 'page', '\\n ', 'This is the first paragraph.', '\\n ', '\\n ', '\\n'] 获取网页中的所有注释1s1.xpath('//comment()') ➜ [&lt;!-- this is the end --&gt;] 例子212345678910111213sample2 = \"\"\"&lt;html&gt; &lt;body&gt; &lt;ul&gt; &lt;li&gt;Quote 1&lt;/li&gt; &lt;li&gt;Quote 2 with &lt;a href=\"...\"&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Quote 3 with &lt;a href=\"...\"&gt;another link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;h2&gt;Quote 4 title&lt;/h2&gt;Something here.&lt;/li&gt; &lt;/ul&gt; &lt;/body&gt;&lt;/html&gt;\"\"\"s2 = getxpath(sample2) 获取所有li中的文本1s2.xpath('//li/text()') ➜ ['Quote 1', 'Quote 2 with ', 'Quote 3 with ', 'Something here.'] 获取第一个 第二个li中的文本，两种写法均可1s2.xpath('//li[position() = 1]/text()') ➜ ['Quote 1'] 1s2.xpath('//li[1]/text()') ➜ ['Quote 1'] 1s2.xpath('//li[position() = 2]/text()') ➜ ['Quote 2 with '] 1s2.xpath('//li[2]/text()') ➜ ['Quote 2 with '] 奇数 偶数 最后一个1s2.xpath('//li[position() mod2 = 1]/text()') ➜ ['Quote 1', 'Quote 3 with '] 1s2.xpath('//li[position() mod2 = 0]/text()') ➜ ['Quote 2 with ', 'Something here.'] 1s2.xpath('//li[last()]/text()') ➜ ['Something here.'] li下面a中的文本1s2.xpath('//li[a]/text()') ➜ ['Quote 2 with ', 'Quote 3 with '] li下a或者h2的文本1s2.xpath('//li[a or h2]/text()') ➜ ['Quote 2 with ', 'Quote 3 with ', 'Something here.'] 使用 | 同时获取 a 和 h2 中的内容1s2.xpath('//a/text()|//h2/text()') ➜ ['link', 'another link', 'Quote 4 title'] 例子312345678910111213sample3 = \"\"\"&lt;html&gt; &lt;body&gt; &lt;ul&gt; &lt;li id=\"begin\"&gt;&lt;a href=\"https://scrapy.org\"&gt;Scrapy&lt;/a&gt;begin&lt;/li&gt; &lt;li&gt;&lt;a href=\"https://scrapinghub.com\"&gt;Scrapinghub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\"https://blog.scrapinghub.com\"&gt;Scrapinghub Blog&lt;/a&gt;&lt;/li&gt; &lt;li id=\"end\"&gt;&lt;a href=\"http://quotes.toscrape.com\"&gt;Quotes To Scrape&lt;/a&gt;end&lt;/li&gt; &lt;li data-xxxx=\"end\" abc=\"abc\"&gt;&lt;a href=\"http://quotes.toscrape.com\"&gt;Quotes To Scrape&lt;/a&gt;end&lt;/li&gt; &lt;/ul&gt; &lt;/body&gt;&lt;/html&gt;\"\"\"s3 = getxpath(sample3) 获取 a 标签下 href 以https开始的1s3.xpath('//a[starts-with(@href, \"https\")]/text()') ➜ ['Scrapy', 'Scrapinghub', 'Scrapinghub Blog'] 获取 href=https://scrapy.org1s3.xpath('//li/a[@href=\"https://scrapy.org\"]/text()') ➜ ['Scrapy'] 获取 id = begin1s3.xpath('//li[@id=\"begin\"]/text()') ➜ ['begin'] 获取text = Scrapinghub1s3.xpath('//li/a[text()=\"Scrapinghub\"]/text()') ➜ ['Scrapinghub'] 获取某个标签下 某个参数 = xx1s3.xpath('//li[@data-xxxx=\"end\"]/text()') ➜ ['end'] 1s3.xpath('//li[@abc=\"abc\"]/text()') ➜ ['end'] 例子41234567891011121314151617181920212223sample4 = u\"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;My page&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;Welcome to my &lt;a href=\"#\" src=\"x\"&gt;page&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;This is the first paragraph.&lt;/p&gt; &lt;p class=\"test\"&gt; 编程语言&lt;a href=\"#\"&gt;python&lt;/a&gt; &lt;img src=\"#\" alt=\"test\"/&gt;javascript &lt;a href=\"#\"&gt;&lt;strong&gt;C#&lt;/strong&gt;JAVA&lt;/a&gt; &lt;/p&gt; &lt;p class=\"content-a\"&gt;a&lt;/p&gt; &lt;p class=\"content-b\"&gt;b&lt;/p&gt; &lt;p class=\"content-c\"&gt;c&lt;/p&gt; &lt;p class=\"content-d\"&gt;d&lt;/p&gt; &lt;p class=\"econtent-e\"&gt;e&lt;/p&gt; &lt;!-- this is the end --&gt; &lt;/body&gt;&lt;/html&gt;\"\"\"s4 = etree.HTML(sample4) 获取 class = test 标签中的所有文字12s4.xpath('//p[@class=\"test\"]/text()')➜ ['\\n 编程语言', '\\n ', 'javascript\\n ', '\\n '] 使用String来获得文字段； strip() 移除字符串收尾字符，默认为空格12345print (s4.xpath('string(//p[@class=\"test\"])').strip())➜编程语言python javascript C#JAVA 获取所有class属性中以content开始的1s4.xpath('//p[starts-with(@class,\"content\")]/text()') ➜ ['a', 'b', 'c', 'd'] 获取所有class属性中包含content的1s4.xpath(('//*[contains(@class,\"content\")]/text()')) ➜ ['a', 'b', 'c', 'd', 'e']","tags":[{"name":"Wiki","slug":"Wiki","permalink":"https://charlesliuyx.github.io/tags/Wiki/"},{"name":"crawl","slug":"crawl","permalink":"https://charlesliuyx.github.io/tags/crawl/"}]},{"title":"PDF复制粘贴去除多余的回车符","date":"2017-07-30T06:03:08.000Z","path":"2017/07/29/PDF复制粘贴去除多余的回车符/","text":"直接上解决步骤，但是只能适用于Windows平台，Mac这边可以尝试用Alfred + workflow来对剪切板操作来解决，或者用BetterTouchTool的自带个性化功能来尝试。只是一个思路，没有在Mac系统尝试 下载 Autohotkey ，安装（这一步都卡住那估计救不了了） 桌面右键 ➜ 新建 ➜ 创建新的AutoHotkey Script 右键创建的文件 ➜ 选择 Edit Script 出来一个记事本 编辑记事本文件，在已经有的内容下直接加上 1234567891011121314151617#IfWinActive ahk_class classFoxitReader^c:: old := ClipboardAll clipboard := &quot;&quot; send ^c clipwait 0.1 if clipboard = clipboard := old else &#123; tmp := RegExReplace(clipboard, &quot;(\\S.*?)\\R(.*?\\S)&quot;, &quot;$1 $2&quot;) clipboard := tmp StringReplace clipboard, clipboard, % &quot; &quot;, % &quot; &quot;, A clipwait 0.1 &#125; old := &quot;&quot; tmp := &quot;&quot;return 这里有个问题 IfWinActive ahk_class classFoxitReader 第一行的classFoxitReader 是指的你用什么程序打开PDF 如果是FoxitReader就是classFoxitReader 如果是Acrobat Adobe就是AcrobatSDIWindow 可以用Autohotkey中的 WinGetClass 来获得某一个窗口的ahk_class 保存退出 桌面上双击你刚刚编辑的文件，可以看到右下角出现了一个H形状的图标 大功告成，这时候你再试试去PDF文档里面ctrl + c就没有回车符了（当然，段落还是无法区分的），也不一定，这一段既然是脚本语言，那就有无限的可能性，就看你的算法实现能力了对吧！","tags":[{"name":"Tools","slug":"Tools","permalink":"https://charlesliuyx.github.io/tags/Tools/"},{"name":"Autohotkey","slug":"Autohotkey","permalink":"https://charlesliuyx.github.io/tags/Autohotkey/"}]},{"title":"LeetcodeNote","date":"2017-07-01T07:18:41.000Z","path":"2017/07/01/LeetcodeNote/","text":"算法培训课程基本模型汇总笔记 线基本模型数学归纳法树基本模板 Draw/Equation -&gt; Tree shape Define TreeNode 本点信息必然是辅助变量，计入TreeNode 孩子信息决定TreeNode的形状 任何第一次走的节点，如果不能走，一定要画出来打一把叉 Binary Search123456789101112131415Public int func(T[] array, V tartget )&#123; int pos = -1; int start = 0; int end array.length - 1; while ( start &lt;= end )&#123; int mid = start + (end - start)/2; if ( f(a[mid]) &lt;= target )&#123; pos = mid; start = mid + 1; &#125; else &#123; end = mid - 1; &#125; &#125; return pos;&#125; Bottom up - Recursion123456789101112131415public &lt;T_P&gt; func(T_v_1, v1 …)&#123; checkhastreeNode(); return helper(root(T_v_1, v_1, …))&#125;private &lt;T_P&gt; helper(T_v_1, v1, …)&#123; resultchildfirst = helper(childFirst); … resultchildlast = helper(childLast); -&gt; result by childs //generate cur node's result; return result;&#125; DFS12345678910111213141516171819202122232425262728public class DFSTree &#123; public Type_R func(T_1, e1, T_2, e2)&#123; checkrootexists(); TreeNode[] array = new TreeNode[TREE_HEIGHT]; Stack&lt;TreeNode&gt; stack = Stack&lt;&gt;(); stack.push(root); while (!stack.Empty())&#123; TreeNode curNode = stack.pop(); Operation at node; stack.push(childLast); … stack.push(childFirst); &#125; return result; &#125; private class TreeNode&#123; T_V_1 field_1; … T_V_q field_q; int _height; &#125; BFS1234567891011121314151617181920212223242526272829303132public class BFS &#123; public TypeR func(T_1 v_1, T_p, v_p) &#123; checkexistroot(); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while ( !queue.isEmpty() )&#123; int size = queue.size(); for ( int I = 0; I &lt; size; i++ )&#123; TreeNode node = queue.remove(); op at node; queue.add(childFirst); … queue.add(childLast); &#125; update var_l,…,var_k for next level &#125; return result; &#125; private class TreeNode&#123; T_1 field_1; … &#125;&#125; 图基本模板","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://charlesliuyx.github.io/tags/Algorithm/"},{"name":"Leetcode","slug":"Leetcode","permalink":"https://charlesliuyx.github.io/tags/Leetcode/"}]},{"title":"幕布-全平台笔记思维导图工具","date":"2017-06-17T17:30:47.000Z","path":"2017/06/17/幕布-全平台笔记思维导图工具/","text":"利益相关：幕布深度使用用户 向大家强烈自来水一款从知乎上了解到的效率神器：幕布，简直相见恨晚，自从它4月正式上线后就一直使用，对我的日常生活、学习和计划帮助巨大（个人情况：硕士CE在读，ML方向，效率至上主义者，简约UI风格拥护者） 幕布是一款思维管理工具，可以用来做笔记，梳理思路，做待办事项等等等等。 人类的记忆是有缺陷的，计算机能帮助人进行记忆。我们可以记住大方向的条目，再借助一个笔记软件来唤起我们的记忆。树形结构是一种极为高效的模式及手段。 笔记软件很多，思维导图软件很多，但是能同时满足以下几点的我找了很久都没有找到，直到遇到幕布。如果你和我也有同样的需求，真心的希望这款优秀的软件能帮助到你，提高的你的日常效率，让每一次阅读，每一个计划都高效落地 UI简约，专注于层次输入本身幕布的官网是这样的： 幕布官网 幕布作为一款笔记软件编辑界面是这样的，幕布专注于层次化输入，每一个输入对于幕布来说都是一个条目，条目就是我们进行知识梳理的主干 幕布编辑界面 每一个操作都提倡使用快捷键，拒绝鼠标 + 键盘混用带来的输入思路打断的低效率 幕布快捷键页面 全平台，云存储和同步，客户端离线编辑答主因为同时使用各种设备：12.9寸iPad，Mac，Windows + Linux 台式机和iPhone手机几个工作平台，平常使用电脑进行笔记记录，碎片化时间使用个手机进行背诵记忆等，需要一款全平台的笔记软件。 而幕布，只要有浏览器，有网络就能流畅使用，有离线需求的用户，也可以通过客户端的形式满足日常编辑的需求。 幕布全平台 一键生成思维导图选择幕布的重要理由之一，废话不多说，上Gif！ 幕布快捷键页面 用过各种软件，Coggle是UI最漂亮的，但是基本的演示需求幕布完全可以满足，清楚明了 那么话说回来了，幕布可以用来干什么呢？下面就展示几个主要应用场景（有我自己的，也有来自于幕布使用趣味案例） 首先，官网给出了一份幕布产品引导，其中详细介绍了幕布的使用场景 读书笔记 方案计划 流程说明 等等 下面案例一些答主自己日常的一些特殊使用场景，基本应用比如做笔记，待办事项，做日程规划等不一一列出来了，就是基本的笔记需求。 TED演讲笔记没有遇到幕布之前，我经常看TED的各种演讲，用于开拓视野，进行英语表达的积累，做笔记的速度太慢，太不方便，遇到幕布之后，我将TED的视频中很关键的内容记录成幕布的条目层次，之后利用碎片化的时间使用手机客户端进行记忆和背诵，极大的丰富了我的谈资（记住的东西才能侃，有条理有依据的说辞才有说服力） 埃里克 哈世延：下一个科学界大突破是什么 对于这个TED笔记例子 演讲人大体思路 经典的单句和例子（中英文） 另一方面，因为幕布的全平台特性，我会用碎片化的时间利用电脑端的幕布来进行背诵（TED的演讲内容对于积累对应领域的英文表达方式有很大的帮助） 程序设计和Presentation编程前先走流程和功能设计是我平时的习惯，这里有一个很简单的Server-Client模式的练习设计用法：设计程序功能，直接一键思维导图展示，PPT完全不用做了，非常愉悦 程序Feature Dota2 Wiki在国外我发现Dota2维基十分的给力，作为Dota2玩家有一些施法距离，施法机制等有时候需要查看（进阶），但是使用网站一方面内容太多，国内访问实在太慢了，而且搜索功能也做的不好，至于我做了什么事情，各位看gif自行感受 Dota2 【利益相关：正在制作，预计Ti7前可以上线，希望也能通过数据帮助到中国军团吧，作为一个做计算机的程序员也希望贡献自己的一份力量】 期末考试复习幕布可以帮助我们把书读薄，我们知道所有的书的特点就是具有层次化，每一本非常优秀的教材都有一套自己对于本学科的知识体系的理解和层次化抽象，之前我进行期末复习需要的时间大概是7天左右，有了幕布可以把时间缩减为3天或者更短 期末复习笔记总览 期末复习笔记具体内容 幕布精选在幕布里，学习知乎模式，你也可以分享自己中意的作品，获得点赞，在后面讨论，甚至有打赏功能，因为软件本身还很年轻，一切还在发展阶段，对于我本人来说，幕布精选的内容只是锦上添花，我个人不太需求这个功能，但是其中还有一份驾考总结挺有用的，哈哈 幕布精选 幕布精选打赏功能 总结和杂七杂八我现在的习惯是，只要是读微信公众号的文章，做笔记，读书等，都会用幕布进行记录和整理，感觉提升效率十分明显（节省了我30-40%左右的时间，每天） 幕布提高我的三个能力 整理和总结的能力【如何把书读薄】 层次化思维能力【有组织的整理自己的知识体系和思路模式，加强效率，节省时间】 背诵能力【全平台（手机），我对碎片化时间能有效利用，我可以多次重复背诵需要背诵的内容】 最后，谢谢你阅读本答案到这个位置，对于我来说，幕布这种层次化的思维模式解决了我当年考高考时候的问题：什么学习方法是最好的？我觉得幕布的层次化整理知识的能力就是答案，幕布提供的是一张纸，一支笔，最后使用幕布能把你的学习生活提升到什么程度，完全取决于你的能力本身，幕布只是工具，帮助你整理你的大脑，帮你进行背诵，方便查阅。 工具永远是工作，创造效益的永远是你，未来也是人创造的，不是工具。 【利益相关，使用我的幕布分享链接可以获得15天的免费高级版试用机会，跪求点击注册！hohohohoho】 我的分享链接 幕布，绝对是一个神器，希望能帮助到各位，提升效率，创造更大的价值！","tags":[{"name":"Tools","slug":"Tools","permalink":"https://charlesliuyx.github.io/tags/Tools/"},{"name":"Mubu","slug":"Mubu","permalink":"https://charlesliuyx.github.io/tags/Mubu/"}]},{"title":"深入浅出看懂AlphaGo如何下棋","date":"2017-05-27T18:51:22.000Z","path":"2017/05/27/AlphaGo运行原理解析/","text":"【阅读时间】15min 8506 words【阅读内容】针对论文AlphaGo第一版本，进行了详细的说明和分析，力求用通俗移动的语言让读者明白：AlphaGo是如何下棋的 问题分析围棋问题，棋盘 19 * 19 = 361 个交叉点可供落子，每个点三种状态，白（用1表示），黑（用-1表示），无子（用0表示），用 $\\vec s$ 描述此时棋盘的状态，即棋盘的状态向量记为 $ \\vec s$ （state首字母）。 $$\\vec s = (\\underbrace{1,0,-1,\\ldots}_{\\text{361}})\\tag {1-1}$$假设状态 $\\vec s$ 下，暂不考虑不能落子的情况， 那么下一步可走的位置空间也是361个。将下一步的落子行动也用一个361维的向量来表示，记为 $\\vec a$ （action首字母）。$$\\vec a = (0,\\ldots,0,1,0,\\ldots)\\tag {1-2}$$公式1.2 假设其中1在向量中位置为39，则 $\\vec a$ 表示在棋盘(3,1)位置落白子，3为横坐标，1为列坐标 有以上定义，我们就把围棋问题转化为。 任意给定一个状态 $\\vec s$ ，寻找最优的应对策略 $\\vec a$ ，最终可以获得棋盘上的最大地盘 总之 看到 $\\vec s$ ，脑海中就是一个棋盘，上面有很多黑白子 看到 $\\vec a$ ，脑海中就想象一个人潇洒的落子 接下来的问题是，如何解决这样一个问题呢？ 先上论文！干货第一 Mastering the game of Go with deep neural networks and tree search 问题解决首先想到，棋盘也是一幅图像，那么在当时最好用的图像处理算法就是深度卷积神经网络（Deep Convolutional Neural Network）。 深度卷积神经网络——策略函数（Policy Network）关于什么是CNN，这篇文章十分靠谱，深入浅出的讲解了什么是CNN An Intuitive Explanation of Convolutional Neural Networks （好像原地址挂了）（5.29更新，原地址已经恢复，原地址的排版更好，估计之前那个博主在进行博客的整理） 大致可以理解为： CNN例子 对一副图像进行处理，给定很多样本进行训练，使得最后的神经网络可以获得指定（具有分类效果）的输出。 比如，根据上图可以观察到（这是一个已经训练好的神经网络），最右侧的输出是[0.01 , 0.04 , 0.94 , 0.02]，其中第三个值0.94代表的是boat，接近1，所以我们判断这幅图片中有船这个物体（类似的，如果使用这幅图像进行训练，那么指定输出应该是[0, 0, 1, 0]，因为图中只有船这个物体） 在Deep Learning中，卷积层的中的Filter也需要训练，也就是说我们使用已有数据来学习图像的关键特征，这样，就可以把网络的规模大幅度的降低 总而言之，CNN可以帮助我们提取出图像中有实际含义的特征，那么这和围棋又有什么关系呢？我们来看看Deepmind团队是怎么运用CNN来解决围棋问题。 深度卷积神经网络解决围棋问题2015年，Aja Huang在ICLR的论文Move Evaluation in Go Using Deep Convolutional Neural Networks中就提出了如何使用CNN来解决围棋问题。 他从围棋对战平台KGS上获得了人类选手的围棋对弈棋谱，对于每一个状态 $ \\vec s$，都会有一个人类进行 $ \\vec a$ 的落子，这也就是一个天然训练样本 $ \\langle \\vec s,\\vec a\\rangle $，如此可以得到3000万个训练样本。 之后，将 $ \\vec s$ 看做一个19*19的二维图像（具体实现依据论文输入数据是19*19*48（48是这个位置的其他信息，比如气等信息，激励函数用的 tanh）使用CNN进行训练，目标函数就是人类落子向量 ${\\vec a}’$，通过使用海量的数据，不断让计算机接近人类落子的位置。就可以得到一个模拟人类棋手下棋的神经网络。 使用训练的结果，我们可以得到一个神经网络用来计算对于每一个当前棋盘状态 $ \\vec s$ ，所对应的落子向量 $ \\vec a$ 的概率分布（之所以是概率分布，是因为，计算好的神经网络，输出一般是一个0-1之间的浮点数，越接近1的点表示在这个位置越接近人类的风格，也可以等同于作为人类概率最大的落点。$$\\vec a=f(\\vec s) \\tag{2-1}$$根据公式2.1，我们记 $f()$ 为$P_{human}(\\vec s)$ ，论文中也叫做Policy Network，也称策略函数。表示的含义是 在状态 $\\vec s$ 下，进行哪一个落子 $\\vec a$ 是最接近人类风格的 计算出来的直观结果，对应到棋盘上如下图，可以看到，红色的区域的值有60%，次大值位于右方，是35%（此图来自于AlphaGo论文） Policy Network 还记得刚刚举得船图的例子嘛？可以类比一下，机器发现现在的状态 $ \\vec s$ 和之前的某一种类型有些类似，输出是一个1*361的向量，其中有几个值比较大（接近1就是100%），那么就用这个值当做下一个 $ \\vec a$ 的位置。不幸的，这种训练方法有很大的局限的，可以直观想到的是，如果对战平台上数据本身就都是俗手，那不是训练出来一个很蠢的神经网络嘛？棋力如何呢？ 深度卷积网络策略的棋力很不幸，据Aja Huang本人说，这个网络的棋力大概相当于业余6段所有的的人类选手。远远未能超过当时最强的围棋电脑程序CrazyStone。 既然比不过，那么就学习它，Aja Huang打算把 $P_{human}(\\vec s)$ 和CrazyStone结合一下，那么问题就来了， CrazyStone是怎么来解决围棋问题的呢？ 这是Aja Huang的老师Remi Colulum在2006年对围棋AI做出的另一大重要突破 干货论文送上 MCTS Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search MCTS 蒙特卡洛搜索树——走子演算（Rollout）蒙特卡洛搜索树（Monte-Carlo Tree Search）是一种大智若愚的方法，它的基本思想是： 首先模拟一盘对决，使用的思路很简单，随机 面对一个空白棋盘 $\\vec s_0$，最初我们对棋盘一无所知，假设所有落子的方法分值都相等，设为1 之后，【随机】从361种方法中选一种走法 $\\vec a_0$，在这一步后，棋盘状态变为 $\\vec s_1$。之后假设对方也和自己一样，【随机】走了一步，此时棋盘状态变为 $\\vec s_2$ 重复以上步骤直到 $\\vec s_n$并且双方分出胜负，此时便完整的模拟完了一盘棋，我们假设一个变量r，胜利记为1，失败则为0 那么问题就来了，如果这一盘赢了，那意味着这一连串的下法至少比对面那个二逼要明智一些，毕竟我最后赢了，那么我把这次落子方法 $(\\vec s_0, \\vec a_0)$ 记下来，并把它的分值变化：$$\\text{新分数} = \\text{初始分数} + r \\tag{2-2}$$同理，可以把之后所有随机出来的落子方法 $(\\vec s_i, \\vec a_i)$ 都应用2-2公式，即都加1分。之后开始第二次模拟，这一次，我们对棋盘不是一无所知了，至少在 $\\vec s_0$ 状态我们知道落子方法 $\\vec a_0$ 的分值是2，其他都是1，我们使用这个数据的方法是：在这次随机中，我们随机到 $\\vec a_0$ 状态的概率要比其他方法高一点。 之后，我们不断重复以上步骤，这样，那些看起来不错（以最后的胜负来作为判断依据）的落子方案的分数就会越来越高，并且这些落子方案也是比较有前途的，会被更多的选择。 $$ score(\\vec s) = \\begin{pmatrix} r_{11} & r_{12} & \\cdots & r_{1n} \\\\ r_{21} & r_{22} & \\cdots & r_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ r_{n1} & r_{n2} & \\cdots & r_{nn} \\end{pmatrix} $$ 如上述公式所述，n=19，每一个状态 $\\vec s$ 都有一个对应的每个落子点的分数，只要模拟量足够多，那么可以覆盖到的 $\\vec s$ 状态就越多，漏洞就越来越小（可以思考李世石的神之一手，是否触及到了AlphaGo1.0的软肋呢？即没有考虑到的状态 $\\vec s$ ） 最后，当进行了10万盘棋后，在此状态选择那个分数最高的方案落子，此时，才真正下了这步棋。这种过程在论文里被称为Rollout 蒙特卡洛搜索树的方法十分的深刻精巧，充满的创造力，它有一些很有意思的特点： 没有任何人工决策的if else逻辑，完全依照规则本身，通过不断的想象（随机）来进行自我对弈，最后提升这一步的质量。有意思的是，其实这也是遵照了人类下棋的思维模式（模仿，只是这一次模仿的不是下棋风格，而是人类思考的方式。十分奇妙，人从飞鸟中受到启发发明了飞机，从鱼身上受到启发发明了潜艇，现在，机器学习的程序，通过学习人类使自身发生进化），人类中，水平越高的棋手，算的棋越多，只是人类对于每一个落子的判断能力更加强大，思考中的棋路，也比随机方式有效的多，但是机器胜在量大，暴力的覆盖到了很多情况。注意，这一个特点也为之后的提高提供了思路。 MCTS可是持续运行。这种算法在对手思考对策的时候自己也可以思考对策。在对方思考落子的过程中，MCTS也可以继续进行演算，在对面落子后，在用现在棋盘的情况进行演算，并且之前计算的结果一定可以用在现在情况中，因为对手的下的这步棋，很可能也在之前演算的高分落子选择内。这一点十分像人类 MCTS是完全可并行的算法 Aja Huang很快意识到这种方法的缺陷在哪里：初始策略（或者说随机的落子方式）太过简单。就如同上面第一条特点所说，人类对每种 $\\vec s$ （棋型）都要更强的判断能力，那么我们是否可以用 $P_{human}(\\vec s)$ 来代替随机呢？ Aja Huang改进了MCTS，每一步不使用随机，而是现根据 $P_{human}(\\vec s)​$ 计算得到 $\\vec a​$ 可能的概率分布，以这儿概率为准来挑选下一个 $\\vec a​$。一次棋局下完之后，新分数按照下面的方式来更新$$\\text{新分数} = \\text{调整后的初始分} + \\text{通过模拟的赢棋概率} \\tag{2-3}$$如果某一步被随机到很多次，就应该主要依据模拟得到的概率而非 $P_{human}(\\vec s)$ ，就是说当盘数不断的增加，模拟得到的结果可能会好于 $P_{human}(\\vec s)$ 得到的结果。所以 $P_{human}(\\vec s)$ 的初始分会被打个折扣，这也是公式2-3中的调整后的初始分的由来 $$ \\text{调整后的初始分} = \\frac{P_{human}(\\vec s)}{(\\text{被随机到的次数} + 1)} \\tag{2-4} $$ 如此一来，就在整张地图上利用 $P_{human}(\\vec s)$ 快速定位了比较好的落子方案，也增加了其他位置的概率。实际操作中发现，此方案不可行，因为计算这个 $P_{human}(\\vec s)$ 太慢了太慢了 一次 $P_{human}(\\vec s)$ 的计算需要3ms，随机算法1us，慢了3000倍，所以，Aja huang训练了一个简化版本的 $P_{human-fast}(\\vec s)$ ，把神经网络层数、输入特征减少，耗时下降到2us，基本满足了要求。 更多的，策略是，先以 $P_{human}(\\vec s)$ 开局，走前面大概20步，之后再使用 $P_{human-fast}(\\vec s)$ 走完剩下的到最后。兼顾速度和准确性。 综合了深度卷积神经网络和MCTS两种方案，此时的围棋程序已经可以战胜所有其他电脑，虽然和其他人类职业选手还有一定的差距。 2015年2月，Aja Huang在Deepmind的同事在顶级学术期刊nature上发表的文章 Human-level control through deep reinforcement learning 用神经网络打游戏。这篇文章，给AlphaGo提供的了新的方向 强化学习——局面函数（Value Network）强化学习（Reinforcement learning）用来实现左右互搏和自我进化，首先说说这篇论文干了一件什么事情，Deepmind团队的大牛们使用强化学习的方法在红白机上打通了200多个游戏，大多数得分都要比人好。 什么是强化学习那什么是强化学习呢？这里推荐莫烦大神的 什么是强化学习 系列教程的知乎专栏，以及另一篇强化学习指南 后者对强化学习的基本概念，实现方法进行全面的讲解，含有公式推导。还有两篇我自己做的笔记，什么是强化学习，强化学习算法介绍 对于强化学习（Reinforcement learning），它是机器学习的一个分支，特别善於控制一只能够在某个环境下自主行动的个体 (autonomous agent)，透过和环境之间的互动，例如 sensory perception 和 rewards，而不断改进它的 行为。 比如，吃豆人游戏，自主行动的个体就是控制的吃豆人，环境就是迷宫，奖励就是吃到的豆子，行为就是上下左右的操作。 强化学习的输入是 状态 (States) = 环境，例如迷宫的每一格是一个 state 动作 (Actions) = 在每个状态下，有什么行动是容许的 奖励 (Rewards) = 进入每个状态时，能带来正面或负面的 价值 (utility) 输出是 方案 (Policy) = 在每个状态下，你会选择哪个行动？也是一个函数 所以，我们需要根据S，A，R，来确定什么样的P是比较好的，通过不断的进行游戏，获得大量的交互数据，我们可以确定在每一个状态下，进行什么动作能获得最好的分数，而强化学习也就是利用神经网络来拟合这个过程。 例如，打砖块游戏有一个秘诀是把求打到墙后，这样球能自己反弹得分，强化学习程序在玩了600盘后，学到了这个秘诀。也就是说程序会在每一个状态下选择那个更容易把球打到墙后面去的操作。如下图，球快要把墙打穿的时候，评价函数 $v$ 的值会大幅度上升 打墙游戏的评价函数图 我们可以发现，强化学习的基本思路和MCTS后异曲同工之妙，也是在对游戏完全没有了解的情况，通过不断的训练（进行多盘对弈，和获得进行行动后的分数反馈）来进行训练，自我提升。 利用强化学习增强棋力参考这种思路，Aja Huang给围棋也设计了一个评价函数 $v(\\vec s)$ 。此函数的功能是：量化评估围棋局面。使用$v(\\vec s)$可以让我们在MCTS的过程中不用走完全局（走完全盘耗时耗力，效率不高）就发现已经必败。 在利用 $P_{human}(\\vec s)$ 走了开局的20步后，如果有一个 $v(\\vec s_i)$ （i为当前状态）可以直接判断是否能赢，得到最后的结果r，不需要搜索到底，可以从效率（剪枝，优化算法时间复杂度）上进一步增加MCTS的威力。 很可惜的，现有的人类棋谱不足以得出这个评价函数。所以Aja Huang决定用机器和机器对弈的方法来创造新的对局，也就是AlphaGo的左右互搏。 自对弈 神经网络的训练过程和结构 先用 $P_{human}(\\vec s)$ 和 $P_{human}(\\vec s)$ 对弈，比如1万盘，得到1万个新棋谱，加入到训练集中，训练出 $P_{human-1}(\\vec s)$ 。 使用$P_{human-1}(\\vec s)$和$P_{human-1}(\\vec s)$对弈，得到另1万个新棋谱，加入训练集，训练出$P_{human-2}(\\vec s)$。 同理，进行多次的类似训练，训练出$P_{human-n}(\\vec s)$，给最后的新策略命名为$P_{human-plus}(\\vec s)$ （感觉一下，这个$P_{human-plus}(\\vec s)$ 应该挺强力的！这里回顾一下$P_{human}(\\vec s)$是什么：是一个函数，$\\vec a=f(\\vec s)$ 可以计算出当前 $\\vec s$ 下的落子 $\\vec a$ 的分布概率） 使用$P_{human-plus}(\\vec s)$和$P_{human}(\\vec s)$进行对弈，发现$P_{human-plus}(\\vec s)$胜率80%，自对弈的方法被证明是有效的。（这里有一个想法，我在之前，一直加粗随机，之所以自对弈有效，就是因为整过MCTS过程中从来没有放弃过随机，如此一来，大量的计算，就更可能覆盖到更多的可能性，对提高棋力可以产生有效的作用同时。因为概率的问题，不断的自我对弈肯定造成下棋的路数集中，后面也会有体现） 但是事实并没有那么美好，Aja Huang发现，使用$P_{human-plus}(\\vec s)$来代替$P_{human}(\\vec s)$进行MCTS反而棋力会下降。 Aja Huang认为是$P_{human-plus}(\\vec s)$走棋的路数太集中，而MCTS需要更加发散的选择才能有更好的效果。 计算局部评价函数（Value Network）考虑到$P_{human-plus}(\\vec s)$的下法太过集中，Aja Huang计算 $v(\\vec s)$ 的策略是： 开局先用$P_{human}(\\vec s)$走L步，有利于生成更多局面 即使如此，Aja Huang还是觉得局面不够多样，为了进一步扩大搜索空间，在L+1步时，完全随机一个 $\\vec a$ 落子，记下这个状态 $v(\\vec s_{L+1})$ 之后使用$P_{human-plus}(\\vec s)$来进行对弈，直到结束时获得结果r，如此不断对弈，由于L也是一个随机数，我们可以得到，开局、中盘、官子等不同阶段的很多局面 $\\vec s$，和这些局面对应的结果r 有了这些训练样本 $\\langle \\vec s,r\\rangle$，还是使用神经网络，把最后一层改成回归而非分类（这里不是用的分类，而是用的回归，拟合），就得到了一个 $v(\\vec s)$ 来输出赢棋的概率 如上图所示，$v(\\vec s)$ 可以给出下一步落在棋盘上任意位置后，如果双方都用$P_{human-plus}(\\vec s)$来走棋，我方赢棋的概率。实验表明，仅仅使用$P_{human}(\\vec s)$来训练 $v(\\vec s)$ 效果不如$P_{human-plus}(\\vec s)$，强化学习是确实有效的。 总结，强化学习的$P_{human-plus}(\\vec s)$主要是用来获得 $v(\\vec s)$ 局部评估函数。表示的含义是 在状态 $\\vec s$ 下，局面的优劣程度，或者说此时的胜率是多少 $v(\\vec s)$ 局部评估函数拥有在线下不断自我进化的能力（这也是AlphaGo可以随时间越来越强的最重要的部分） 感谢你看到这里，我们已经拥有： $P_{human}(\\vec s)$ 我的老师是人类！ MCTS 乱下，我只看输赢 $v(\\vec s)$ 我能判断局势 有了这些我们距离AlphaGo已经不远了 AlphaGo MTCS流程图解 Aja Huang使用MCTS框架融合局面评估函数 $v(\\vec s)$ 的策略是： 使用$P_{human}(\\vec s)$作为初始分开局，每局选择分数最高的方案落子 到第L步后，改用$P_{human-fast}(\\vec s)$把剩下的棋局走完，同时调用 $v(\\vec s_L)$，评估局面的获胜概率，按照如下规则更新整个树的分数​$$\\text{新分数} = \\text{调整后的初始分} + 0.5*\\text{通过模拟得到的赢棋概率} + 0.5*\\text{局面评估分} \\tag {3-1}$$ 前两项和原来一样 如果待更新的节点就是叶子节点，局面评估分就是 $v(\\vec s_L)$ 如果是待更新的节点是上级节点，局面评估分是该叶子节点 $v(\\vec s)$ 的平均值 如果 $v(\\vec s)$ 是表示大局观，$P_{human-fast}(\\vec s)$表示快速演算，那么上面的方法就是二者的并重，并且Aja Huang团队已经用实验证明0.5 0.5的权重对阵其他权重有95%的胜率 详解AlphaGo VS 樊麾 对局走下某一步的计算过程 详解AlphaGo走某一步棋的过程1 a图使用局部评估函数计算出 $\\vec s$ 状态下其他落子点的胜率 b图MCTS中使用局部评估函数加 $P_{human}(\\vec s)$ 得出的结果 c图MCTS中使用$P_{human}(\\vec s)$（复合算法）和$P_{human-fast}(\\vec s)$走子走到底的结果 详解AlphaGo走某一步棋的过程2 d图深度卷积神经网络使用策略函数计算出来的结果 e图使用公式3-1和相关流程计算出的落子概率 f图演示了AlphaGo和樊麾对弈的计算过程，AlphaGo执黑，樊麾执白。红圈是AlphaGo实际落子额地方。1，2，3和后面的数字表示他想象中的之后樊麾下一步落子的地方。白色方框是樊麾的实际落子。在复盘时，樊麾认为1的走法更好（这说明在樊麾落子后AlphaGo也在进行计算） 总结由于状态数有限和不存在随机性，象棋和五子棋这类游戏理论上可以由终局自底向上的推算出每一个局面的胜负情况，从而得到最优策略。例如五子棋就被验证为先手必胜。 AlphaGo的MCTS属于启发式搜索算法 启发式搜索算法：由当前局面开始，尝试看起来可靠的行动，达到终局或一定步数后停止，根据后续局面的优劣反馈，选择最有行动。通俗来说，就是”手下一招子，心想三步棋“ 围棋是一个NP问题，要穷举的话，解空间巨大。现代优化算法的经典之处在于，从围棋的规则来看，在某一个状态，必定有一个或几个较优解，整个AlphaGo就是想方设法的去找这个较优解。利用局面评估函数来对MCTS进行剪枝的思路十分精彩。利用上面的3个算法，结合庞大的并行运算能力，还有Aja Huang团队的辛苦付出，造就了AlphaGo的奇迹。 使用不同组件AlphGo1.0的棋力 最终棋力结果 上图显示了各种算法的棋力，Rollout是走棋演算，也就是MCTS，Value Network是 $v(\\vec s)$ 局面评估函数，Policy Network 是结合$P_{human-plus}(\\vec s)$和$P_{human}(\\vec s)$后计算的策略函数（下一步走在哪里胜率高的深度卷积神经网络） 整个AlphaGo使用的技术，深度卷积网络，强化学习神经网络，都是炙手可热的领域，近年来发展迅猛，日新月异。AlphaGo已经完成了自己历史使命，借助棋类的巅峰【围棋】为叩门砖打开了机器学习自我进化的大门 李世石 VS AlphaGo 1.0——第四局78手挖 赛后AlphaGo之父给出的关键信息：李世石78手“挖”是AlphaGo认为概率极小的点，这一手之后导致的状态 $\\vec s$ 进入到了AlphaGo能处理的范围之外，即之前AlphaGo的自对弈都是建立用自己觉得好的下法来搜索的，那么如果这一手AlphaGo1.0感觉可能性极小，那么用$P_{human}(\\vec s)$自对弈的棋谱中就更加难以覆盖。 但是也需要提到的是，根据比赛中柯洁等人的观战我们知道，如果不是后面AlphaGo进入了混乱模式，78手不一定是一个好棋。只能说这一手，顶到了AlphaGo的软肋，在真正和人的对局中不一定是“神之一手” 根据Deepmind团队给出的数据可以知道，一年前，AphaGo1.0的搜索空间，自对弈深度并不完美。所以Deepmind团队有意的在代码逻辑上让其避免打劫，或者说避免劫争，例如，有两个选择，一个胜率60%但需要打劫，另一个55%但不需要打劫，AlphaGo1.0会选择后者。 那么什么是打劫呢？解释这几个和”劫“有关的围棋术语是： 打劫围棋术语，一方制造事端，和另一方讨价还价的行为。劫材可以用来做价格谈判的筹码。通常是走一手没戏，但对手若不予置理，再走第二手会出棋的局部。寻劫通过目数计算，寻找一些有价值的局部制造事端强迫对手应答。通常价值至少需要和打劫的地方相当或者小不太多，否则对方很容易消劫。利用劫劫胜可杀死对方或者得到利益，劫败也应该让对方付出代价，除非双方劫材大小和数量相差悬殊。 通俗的说是，我在这一片已经处于劣势，我换一个战场，发动进攻，你应不应？可能在另外战场的角力中对这边战场的局势产生影响。可以类比于，五子棋中的冲四。 如果有人观看了这一盘棋，我们也可以听到柯洁在强调，AlphaGo在避免打劫，出现了几手莫名其妙的落子。 总结来说，AlphaGo依靠的是对局外的大量计算，无论是局部评估函数，还是$P_{human-plus}(\\vec s)$都十分依赖对局外的大量的计算。随着时间的推移，AlphaGo在对局过程需要的时间越来越固定，不需要在对局时进行太多的MCTS搜索就能获得AlphaGo的下一手位置，可以预见，MCTS的搜索深度不会太深。当计算量十分庞大的时候，依赖更多是那个120层的Policy Network。 从柯洁的第二盘可以发现，他已经努力的制造在中腹引入多方战斗的带劫争的复杂棋局，十分精彩。可惜，AlphaGo2.0貌似已经完善了自己的阿特留斯之踵。当真无敌，说到这里，我们来谈谈AlphaGo2.0 AlphaGo 2.0 VS 柯洁——虽败犹荣三盘对局，感觉到AlphaGo在这一年内进行了极为深度的训练。最可怕的是AlphaGo通过时间验证了机器学习对于解决NP问题的强大潜力（通过这三盘可以看出已经无限接近解决了这个问题，至少在对人类上）。甚至： 臆想一，是否可以利用AlphaGo来判断规则是否公平（中国和韩日规则的不同，7目半和6目半）。 臆想二，最终AlphaGo的自对弈是接近和棋。可惜AlphaGo已经退役。希望针对Deepmind放出的50盘自对弈棋谱可以研究出一些门道，使得围棋这门竞技本身有更大的突破。 局面函数和策略函数愈发强大，愈加的接近于”围棋之神“。 随着Google TPU的发布，跑在TPU阵列上的AlphaGo如虎添翼，MCTS的走子演算效率更高，速度更快（加速的其实就是$P_{human-plus}(\\vec s)$的落子速度。 关于TPU的设计思路和原理可以参考 In-Datacenter Performance Analysis of a Tensor Processor 对于围棋这个策略单步游戏，是存在N步最优解（不存在i+1步最优解），AlphaGo已经在正确的道路上无限的接近于这个N步最优解，仿佛在某一步已经看到了你无论怎么下都能走到的N步最优解。 人类的每一次失误都会使局部评估函数往胜率移动一点，这一点是十分可怕的，因为算法本身的优越性，大局观对于AlphaGo的逻辑来说本身就是一种刻在骨子里的基因 一是因为AlphaGo每次MCTS计算都会计算到接近分出胜负，具有前瞻性 二是因为局面函数本身就是为了来统计大局形势定义的，具有判断局面优劣的能力 所谓大局观，不就是这种走一步看N步的能力嘛。 对未来的展望——从AlphaGo想开去珍贵的并不是攻克了围棋问题本身，而是这种解决问题的基本模式，可以推而广之到很多领域。 先通过卷积网络学习人类的下法，算出策略函数（Policy Network），再通过模仿进行强化学习，左右互搏，不断自我进化，再加上MCTS的经典的解决问题的启发式搜索算法。 这俨然是一个 模仿➜学习➜优化的过程 或许，模仿人类，是机器学习最终的归途，至于应用领域方面 游戏AI是一个最容易想到的领域，只要能抽象出 State Action Judgement，那么这一套解决问题的方式就可以举一反三，让每一个1V1领域的游戏AI非常强大（OpenAI在Dota2 1V1 Solo上的结果更加证明了这一点），至于合作领域的AI可能需要更大的计算量去计算（OpenAI发布的论文MultiAgent很有启发性），对于实际问题来说获得这样的AI有多大的经济价值值得推敲。 游戏的乐趣就在于不确定性，适当的失误也是竞技类游戏的魅力所在，一个能看到N步最优解的AI会让一个游戏机制，游戏规则变得可数据化，这一点其实是游戏被创造出来的初衷相背离的。 其他方面，只要是人类可以学习出来的事物，比如翻译，编程，都是现在的这套体系可能解决的问题，我们期待未来这套解决问题的方法发挥出无穷的力量吧！ [Reference]知乎Tao Lei大神的回答知乎袁行远大神的回答知乎有关围棋打劫的回答其他文章中引用的论文，链接已经给出","tags":[{"name":"AlphaGo","slug":"AlphaGo","permalink":"https://charlesliuyx.github.io/tags/AlphaGo/"},{"name":"CNN","slug":"CNN","permalink":"https://charlesliuyx.github.io/tags/CNN/"},{"name":"MCTS","slug":"MCTS","permalink":"https://charlesliuyx.github.io/tags/MCTS/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://charlesliuyx.github.io/tags/Deep-Learning/"}]},{"title":"【摘抄】清欢 - 林清玄","date":"2017-05-14T16:26:55.000Z","path":"2017/05/14/清欢/","text":"【阅读内容】个人摘抄，林清玄散文（非常喜欢的散文家） 1少年时代读到苏轼的一阕词，非常喜欢，到现在还能背诵： 细雨斜风作晓寒，淡烟疏柳媚晴滩，入淮清洛渐漫漫。雪沫乳花浮午盏，蓼茸蒿笋试春盘，人间有味是清欢。 这阕词，苏轼在旁边写着“元丰七年十二月二十四日，从泗州刘倩叔游南山”，原来是苏轼和朋友到郊外去玩，在南山里喝了浮着雪沫乳花的淡茶，配着春日山野里的蓼菜、茼蒿、新笋，以及野草的嫩芽等等，然后自己赞叹着：“人间有味是清欢！” 当时所以能深记这阕词，最主要的是爱极了后面这一句，因为试吃野菜的这种平凡的清欢，才使人间更有滋味。“清欢”是什么呢？清欢几乎是难以翻译的，可以说是“清淡的欢愉”，这种清淡的欢愉不是来自别处，正是来自对平静疏淡简朴生活的一种热爱。当一个人可以品味出野菜的清香胜过了山珍海味，或者一个人在路边的石头里看出了比钻石更引人的滋味，或者一个人听林间鸟鸣的声音感受到比提笼遛鸟更感动，或者体会了静静品一壶乌龙茶比起在喧闹的晚宴中更能清洗心灵……这些就是“清欢”。 清欢之所以好，是因为它对生活的无求，是它不讲求物质的条件，只讲究心灵的品味。“清欢”的境界很高，它不同于李白的人生在世不称意，明朝散发弄扁舟那样的自我放逐；或者人生得意须尽欢，莫使金樽空对月那种尽情的欢乐。它也不同于杜甫的人生有情泪沾臆，江水江花岂终极这样悲痛的心事，或者人生不相见，动如参与商；今夕复何夕，共此灯烛光那种无奈的感叹。 活在这个世界上，有千百种人生，文天祥的是人生自古谁无死，留取丹心照汗青，我们很容易体会到他的壮怀激烈。欧阳修的是人生自是有情痴，此恨不关风与月，我们很能体会到他的绵绵情恨。纳兰性德的是人到情多情转薄，而今真个不多情，我们也不难会意到他无奈的哀伤。甚至于像王国维的人生只似风前絮，欢也零星，悲也零星，都作连江点点萍！那种对人生无常所发出的刻骨的感触，也依然能够知悉。 2可是“清欢”就难了！ 尤其是生活在现代的人，差不多是没有清欢的。 什么样是清欢呢？我们想在路边好好的散个步，可是人声车声不断的呼吼而过，一天里，几乎没有纯然安静的一刻。 我们到馆子里，想要吃一些清淡的小菜，几乎是杳不可得，过多的油、过多的酱、过多的盐和味精已经成为中国菜最大的特色，有时害怕了那样的油腻，特别嘱咐厨子白煮一个菜，菜端出来时让人吓一跳，因为菜上挤的色拉比菜还多。 有时没有什么事，心情上只适合和朋友去啜一盅茶、饮一杯咖啡，可惜的是，心情也有了，朋友也有了，就是找不到地方，有茶有咖啡的地方总是嘈杂的。 俗世里没有清欢了，那么到山里去吧！到海边去吧！但是，山边和海湄也不纯净了，凡是人的足迹可以到的地方，就有了垃圾，就有了臭秽，就有了吵闹！ 有几个地方我以前常去的，像阳明山的白云山庄，叫一壶兰花茶，俯望着台北盆地里堆叠着的高楼与人欲，自己饮着茶，可以品到茶中有清欢。像在北投和阳明山间的山路边有一个小湖，湖畔有小贩卖工夫茶，小小的茶几、藤制的躺椅，独自开车去，走过石板的小路，叫一壶茶，在躺椅上静静的靠着，有时湖中的荷花开了，真是惊艳一山的沉默。有一次和朋友去，在躺椅上静静喝茶，一下午竟说不到几句话，那时我想，这大概是“人间有味是清欢”了。 现在这两个地方也不能去了，去了只有伤心。湖里的不是荷花了，是飘荡着的汽水罐子，池畔也无法静静躺着，因为人比草多，石板也被踏损了。到假日的时候，走路都很难不和别人推挤，更别说坐下来喝口茶，如果运气更坏，会遇到呼啸而过的飞车党，还有带伴唱机来跳舞的青年，那时所有的感官全部电路走火，不要说清欢，连欢也不剩了。 要找清欢，一日比一日更困难了。 当学生的时候，有一位朋友住在中和圆通寺的山下，我常常坐着颠踬的公交车去找她，两个人沿着上山的石阶，漫无速度的，走走、坐坐、停停、看看，那时圆通寺山道石阶的两旁，杂乱的长着朱槿花，我们一路走，顺手拈下一朵熟透的朱槿花，吸着花朵底部的花露，其甜如蜜，而清香胜蜜，轻轻的含着一朵花的滋味，心里遂有一种只有春天才会有的欢愉。 3圆通寺是一座全由坚固的石头砌成的寺院，那些黑而坚强的石头坐在山里仿佛一座不朽的城堡，绿树掩映，清风徐徐，站在用石板铺成的前院里，看着正在生长的小市镇，那时的寺院是澄明而安静的，让人感觉走了那样高的山路，能在那平台上看着远方，就是人生里的清欢了。 后来，朋友嫁人，到国外去了。我去过一趟圆通寺，山道已经开辟出来，车子可以环山而上，小山路已经很少人走，就在寺院的门口摆着满满的摊贩，有一摊是儿童乘坐的机器马，叽哩咕噜的童歌震撼半山，有两摊是打香肠的摊子，烤烘香肠的白烟正往那古寺的大佛飘去，有一位母亲因为不准孩子吃香肠而揍打着两个孩子，激烈的哭声尖亢而急促……我连圆通寺的寺门都没有进去，就沉默的转身离开，山还是原来的山，寺还是原来的寺，为什么感觉完全不同了，失去了什么吗？失去的正是清欢。 下山时的心情是不堪的，想到星散的朋友，心情也不是悲伤，只是惆怅，浮起的是一阕词和一首诗，词是李煜的：高楼谁与上？长记秋晴望。往事已成空，还如一梦中！诗是李觏的：人言落日是天涯，望极天涯不见家；已恨碧山相阻隔，碧山还被暮云遮！那时正是黄昏，在都市烟尘蒙蔽了的落日中，真的看到了一种悲剧似的橙色。 我二十岁心情很坏的时候，就跑到青年公园对面的骑马场去骑马，那些马虽然因驯服而动作缓慢，却都年轻高大，有着光滑的毛色。双腿用力一夹，它也会如箭一般呼噜向前窜去，急忙的风声就从两耳掠过，我最记得的是马跑的时候，迅速移动着的草的青色，青茸茸的，仿佛饱含生命的汁液，跑了几圈下来，一切恶的心情也就在风中、在绿草里、在马的呼啸中消散了。 尤其是冬日的早晨，勒着绳，马就立在当地，踢踏着长腿，鼻孔中冒着一缕缕的白气，那些气可以久久不散，当马的气息在空气中消弭的时候，人也好像得到某些舒放了。 骑完马，到青年公园去散步，走到成行的树荫下，冷而强悍的空气在林间流荡，可以放纵的、深深的呼吸，品味着空气里所含的元素，那元素不是别的，正是清欢。 4最近有一天，突然想到骑马，已经有十几年没骑了。到青年公园的骑马场时差一点吓昏，原来偌大的马场已经没有一根草了，一根草也没有的马场大概只有台湾才有，马跑起来的时候，灰尘滚滚，弥漫在空气里的尽是令人窒息的黄土，蒙蔽了人的眼睛。马也老了，毛色斑剥而失去光泽。 最可怕的是，不知道什么时候在马场搭了一个塑料棚子，铺了水泥地，其丑无比，里面则摆满了机器的小马，让人骑用，其吵无比。为什么为了些微的小利，而牺牲了这个马场呢？ 马会老是我知道的事，人会转变是我知道的事，而在有真马的地方放机器马，在马跑的地方没有一株草，则是我不能理解的事。 就在马场对面的青年公园，已经不能说是公园了，人比西门町还拥挤吵闹，空气比咖啡馆还坏，树也萎了，草也黄了，阳光也不灿烂了。从公园穿越过去，想到少年时代的这个公园，心痛如绞，别说清欢了，简直像极了佛经所说的“五浊恶世”！ 生在这个时代，为何“清欢”如此难觅。眼要清欢，找不到青山绿水；耳要清欢，找不到宁静和谐；鼻要清欢，找不到干净空气；舌要清欢，找不到蓼茸蒿笋；身要清欢，找不到清凉净土；意要清欢，找不到智慧明心。如果要享受清欢，唯一的方法是守在自己小小的天地，洗涤自己的心灵，因为在我们拥有愈多的物质世界，我们的清淡的欢愉就日渐失去了。 现代人的欢乐，是到油烟爆起、卫生堪虑的啤酒屋去吃炒蟋蟀；是到黑天暗地、不见天日的卡拉OK去乱唱一气；是到乡村野店、胡乱搭成的土鸡山庄去豪饮一番；以及到狭小的房间里做方城之戏，永远重复着摸牌的一个动作……这些放逸的生活以为是欢乐，想起来毋宁是可悲的。为什么现代人不能过清欢的生活，反而以浊为欢，以清为苦呢？ 一个人以浊为欢的时候，就很难体会到生命清明的滋味，而在欢乐已尽、浊心再起的时候，人间就愈来愈无味了。 5这使我想起东坡的另一首诗来： 梨花淡白柳深青，柳絮飞时花满城；惆怅东栏一株雪，人生看得几清明？ 苏轼凭着东栏看着栏杆外的梨花，满城都飞着柳絮时，梨花也开了遍地，东栏的那株梨花却从深青的柳树间伸了出来，仿佛雪一样的清丽，有一种惆怅之美，但是人生看这么清明可喜的梨花能有几回呢？这正是千古风流人物的性情，这正是清朝大画家盛大士在《溪山卧游录》中说的凡人多熟一分世故，即多一分机智。多一分机智，即少却一分高雅。 也有说山中何所有？岭上多白云，只可自怡悦，不堪持赠君，自是第一流人物。 第一流人物是什么人物？ 第一流人物是在清欢里也能体会人间有味的人物！ 第一流人物是在污浊滔滔的人间，也能找到清欢的人物！","tags":[{"name":"Article","slug":"Article","permalink":"https://charlesliuyx.github.io/tags/Article/"},{"name":"Literature","slug":"Literature","permalink":"https://charlesliuyx.github.io/tags/Literature/"}]},{"title":"English-abbreviation","date":"2017-05-14T00:29:00.000Z","path":"2017/05/13/English-abbreviation/","text":"一些常用的英语缩写的总结 日常生活篇 R.S.V.P: 源自于法语‘Répondez s’il vous plait’，英文解释为’Respond,if you please’.邀请函结尾写这个，表示‘敬请回复’； P.S: 意思是‘post script’,表示‘再多说一句’，一般写完要说的话之后结尾突然想起说什么可以写； ASAP: as soon as possible. 表示‘尽快’，注意听音频发音，可读成A-SAP; ETA: estimated time of arrival. 表示‘预计到达时间’； BYOB: bring your own bottle; 表示‘自带酒水，举办派对时常用’ 吃饭做菜篇 tsp or t : teaspoon 一茶匙 tbs / tbsp/ T: tablespoon 一汤匙 c: cup 一杯 gal: gallon 加仑 lb : pound 磅 pt：pint 品脱 qt: quart 夸脱 出国地图篇 Ave: avenue 大街 Blvd: boulevard 大道 Ln: lane 车道 Rd: road 公路 St: street 街道 教育工作篇 BA: Bachelor of Arts 文学士 BS: Bachelor of Science 理学士 MA: Master of Arts 文科硕士 PA: Personal Assistant 私人助理 VP: Vice President 副总统;副总裁 CEO: Chief Executive Officer 首席执行官 CFO: Chief Financial Officer 首席财务官 COO: Chief Operating Officer 首席运营官 CMO: Chief Marketing Officer 首席营销官 社交聊天篇 JK :just kidding 跟你开玩笑呢 TBD: to be determined 待定 AFAIK: as far as I know 据我所知 BRB: be right back 马上回来 CUL: see you later 回见 TTYL: talk to you later 回聊 CWYL: chat with you later 回聊 LOL: laugh out loud 哈哈 LMAO: laugh my ass off 笑死我了 ROTFL/ ROFL: rolling on the floor laughing 笑到在地上打滚 NP: no problem 没问题,没关系,不客气 IDK: I don’t know 我不知道 ILY: I love you 我爱你 TMI: too much information 信息量太大了； 说的太多了 OIC: Oh, I see. 我明白了 FYI: for your information 顺便告知你 BTW: by the way 顺便说一下 顺便问一下 MYOB: mind your own business 别多管闲事 FAQ: frequently asked questions 经常被问的问题20: WTF: what the fuck 搞毛阿…… 委婉的是WTH: what the hell/heck21: AKA: also known as. 也叫做 TGIF: thank god It’s Friday 谢天谢地又到礼拜五了 TBC: to be continued; to be confirmed 未完待续/ 有待确认 数字字母篇2: to/too4: forB: beC: seeI: eyeO: owe;R: are;U: you;ur: your/you’reY: why","tags":[{"name":"Wiki","slug":"Wiki","permalink":"https://charlesliuyx.github.io/tags/Wiki/"},{"name":"English","slug":"English","permalink":"https://charlesliuyx.github.io/tags/English/"}]},{"title":"【原创】有关中国诗的那些事","date":"2017-05-13T23:40:00.000Z","path":"2017/05/13/有关中国诗的那些事/","text":"【阅读内容】个人高三作文水平巅峰时期写下的有关中国诗的散文一篇，献丑献丑 没有沉淀，文字永远上不了档次。难得空闲，读了些诗，有些感受。 韦应物 独怜幽草涧边生，上有黄鹂深树鸣。春潮带雨晚来急，野渡无人舟自横。 记起这《滁州西涧》，听过一个故事。话说一次国画比赛，题目是以春潮带雨晚来急，野渡无人舟自横这句诗作画。国学博大精深，国画作为其中一支配起诗来，别有遐想。此题甚好，不仅考及画技，更有对国学中诗词的体悟和见解。大家不妨也想想如果是你，你会怎么画？这里先卖个关子。 从诗的字面来说，是这样一种通感：春天近了，潮气依稀可嗅。但谁能像你这样，对一棵在水边生长的小草也充满爱怜？黄鹂在密林深处的低语你都能听到？这需要多么细腻的一颗心。华灯初上，渡口上已经没有人，舟独自横于水上，那是一种空阔的感觉。映照你一生步履，你的细腻出于岁月。你当年49岁，50载，可能不长，但是我知道你的与众不同，你的50载甚至顶得别人几辈子。 韦应物年少荒唐，并未认真读书。安史乱起，韦应物扈从不及，流落秦中。乱后，韦应物折节读书，痛改前非，从一个富贵无赖纨绔一变而为忠厚仁爱的儒者。有些官运，在地方（苏州）任官。韦应物勤于吏职，简政爱民，在苏州刺史届满之后，一贫如洗，寄居无定寺，客死他乡。 享年五十五岁。 别人些许看出的是你不在其位，不得其用的无奈，忧伤。但我看到的，更多是你的豁达，你心中总是美好多于忧伤。 通往远方的路，没有哪条是你不能走的；走在路上的人，没有谁是你不能结交的；结交的朋友，没有谁是你不能推心置腹的。虽然那个时代远没有现在的复杂，但是能捧出一颗完整的心也并不是一件容易的事。韦苏州，你是一个充满诗情的人。 回头看看开头提到的国画比赛优胜者的作品：弥蒙的雾气用模糊的淡墨衬托，远处的群山，夕阳露出半个头。远远的有几簇灯火，近处，一条小舟在几根芦苇中飘荡，船上有位着布衣的蓑翁，嘴里叼根芦苇，帽檐下压，不知是否在闭目养神，两只杜鹃立于船头。起初不懂，“无人”的野渡为何有人呢？其中深意，结合了韦苏州的履历才恍然大悟。 “无人”并不是一只孤舟。韦应物闲居，船上舟子，好似当时的韦应物，在船头打盹，闻着草香，听着鹂鸣。韦应物虽然赋闲苏州，但他并不排斥官场，若有机会，他还是会出仕，只是满足于闲暇。无奈忧伤可能有，但经历了顽劣，奋起，战乱，官场，贬谪，闲居的韦应物，更多的，是看破人生的豁达和满足。 李白总觉中国诗总离不开一个“愁”字。思乡，思亲，忧国，羁旅等等，都和“愁”万缕千丝。我爱这些无奈，悲壮，不舍，甚至愤懑，嘲讽。他们仿佛缩影了人生，视角令人称奇，细腻的令你悸动。 抽刀断水，是最无奈的神话；举杯消愁，是最动情的悲歌。李白潇洒一生，他豪放，甚至一直清贫，有了几个钱，就豪饮一番，将诗情挥洒，更是对“愁”下了如此入理的定义。 拣尽寒枝不肯栖，寂寞沙洲岭李白就犹如谪仙，似乎从来没有受过来自这个世界的温暖。于是，在静夜里，李白写下了床前明月光，疑是地上霜。举头望明月，低头思故乡的千古“愁”词。可是李白的故乡在哪里呢？是陇西？是巴蜀？月华似霜的夜，浪迹天涯的游子李白在梦幻中寻觅故乡，但故乡却比梦幻更飘渺。 李白是复杂的，李白糅合着道家的“出世”和儒家的“入世”思想。所以，顺境时，他仰天大笑出门去，我辈岂是蓬蒿人的潇洒豪情；逆境时，他有弃我去者昨日之日不可留，乱我心者今日之日多烦忧的绵绵愁绪。 那些诗人感动于张谓笔下早梅傲雪不知近水花先发，疑是经冬雪未消的玄妙；陶醉于贺铸风中一川烟草，满城风絮，梅子黄时雨的飘愁；哀婉于苏轼眼中细看来，不是杨花，点点是，离人泪的破碎。 说起苏东坡，一个传奇。 人生如梦，东坡曾经迷惘过；早生华发，东坡曾经惋惜过；十年生死两茫茫，东坡曾经痛苦过。但他不屈，他平和，他豁达。 一蓑烟雨任平生，他淡泊；日啖荔枝三百颗，不辞长作岭南人，他自定；踏雪飞鸿，他淡然。问汝平生功业，黄州惠州儋州，三贬之地，还恰恰就是他留下许多不朽之作的地方。 读着这些诗，深深思索，你会感到作为一个中国人学会了中文，有着五千年的浩瀚历史文化，是多么令你振奋和自豪；国学，遗留的东西，值得我们用一生去参悟。常说高考诗词理解令人头痛，如果怀着这样的心情读诗，你还会怕吗？","tags":[{"name":"Article","slug":"Article","permalink":"https://charlesliuyx.github.io/tags/Article/"},{"name":"Original","slug":"Original","permalink":"https://charlesliuyx.github.io/tags/Original/"}]}]