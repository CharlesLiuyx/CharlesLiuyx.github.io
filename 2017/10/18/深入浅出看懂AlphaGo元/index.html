<!DOCTYPE html>
<html>
<head>
    

    

    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Baidu Tongji -->
<script>var _hmt = _hmt || []</script>
<script async src="//hm.baidu.com/hm.js?15d162fc8e4dd54f3980fc240cbb6b68"></script>
<!-- End Baidu Tongji -->




    <meta charset="utf-8">
    
    
    
    <title>深入浅出看懂AlphaGo元 | Go Further | Stay Hungry, Stay Foolish</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="AlphaGo,Deep Learning">
    <meta name="description" content="【阅读时间】15min - 17min【内容简介】AlphaGo Zero论文原文超详细翻译，并且总结了AlphaGo Zero的算法核心思路，附带收集了网上的相关评论">
<meta name="keywords" content="AlphaGo,Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="深入浅出看懂AlphaGo元">
<meta property="og:url" content="https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/index.html">
<meta property="og:site_name" content="Go Further">
<meta property="og:description" content="【阅读时间】15min - 17min【内容简介】AlphaGo Zero论文原文超详细翻译，并且总结了AlphaGo Zero的算法核心思路，附带收集了网上的相关评论">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure1.png">
<meta property="og:image" content="https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure2.png">
<meta property="og:image" content="https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure3.png">
<meta property="og:image" content="https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure4.png">
<meta property="og:image" content="https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure5.png">
<meta property="og:image" content="https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure6.png">
<meta property="og:image" content="https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure5a.png">
<meta property="og:image" content="https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure5b.png">
<meta property="og:updated_time" content="2017-10-21T09:22:24.508Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深入浅出看懂AlphaGo元">
<meta name="twitter:description" content="【阅读时间】15min - 17min【内容简介】AlphaGo Zero论文原文超详细翻译，并且总结了AlphaGo Zero的算法核心思路，附带收集了网上的相关评论">
<meta name="twitter:image" content="https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure1.png">
    
        <link rel="alternate" type="application/atom+xml" title="Go Further" href="/atom.xml">
    
    <link rel="shortcut icon" href="/img/favicon.png">
    <link rel="stylesheet" href="/css/style.css?v=1.6.7">
    <script>window.lazyScripts=[]</script>

        <!-- custom head --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</head>

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/avatar.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">遥行 Go Further</h5>
          <a href="mailto:297106286@qq.com" title="297106286@qq.com" class="mail">297106286@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Home
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/CharlesLiuyx" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="http://weibo.com/u/6064451001?is_all=1" target="_blank" >
                <i class="icon icon-lg icon-weibo"></i>
                Weibo
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">深入浅出看懂AlphaGo元</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">深入浅出看懂AlphaGo元</h1>
        <h5 class="subtitle">
            
                <time datetime="2017-10-19T03:54:32.000Z" itemprop="datePublished" class="page-time">
  2017-10-18
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#论文正文内容解析"><span class="post-toc-number">1.</span> <span class="post-toc-text">论文正文内容解析</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#AlphaGo-Zero-的强化学习"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">AlphaGo Zero 的强化学习</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#网络结构"><span class="post-toc-number">1.1.1.</span> <span class="post-toc-text">网络结构</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#改进的强化学习算法"><span class="post-toc-number">1.1.2.</span> <span class="post-toc-text">改进的强化学习算法</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#训练步骤总结"><span class="post-toc-number">1.1.3.</span> <span class="post-toc-text">训练步骤总结</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#AlphaGo-Zero训练过程中的经验"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">AlphaGo Zero训练过程中的经验</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#AlphaGo-Zero学到的知识"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">AlphaGo Zero学到的知识</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#论文附录内容"><span class="post-toc-number">2.</span> <span class="post-toc-text">论文附录内容</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#自对弈训练工作流"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">自对弈训练工作流</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#优化参数"><span class="post-toc-number">2.1.1.</span> <span class="post-toc-text">优化参数</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#评估器"><span class="post-toc-number">2.1.2.</span> <span class="post-toc-text">评估器</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#自对弈"><span class="post-toc-number">2.1.3.</span> <span class="post-toc-text">自对弈</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#搜索算法"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">搜索算法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Selcet-Figure2a"><span class="post-toc-number">2.2.1.</span> <span class="post-toc-text">Selcet - Figure2a</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Expand-and-evaluate-Figure-2b"><span class="post-toc-number">2.2.2.</span> <span class="post-toc-text">Expand and evaluate - Figure 2b</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Backup-Figure-2c"><span class="post-toc-number">2.2.3.</span> <span class="post-toc-text">Backup - Figure 2c</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Play-Figure-2d"><span class="post-toc-number">2.2.4.</span> <span class="post-toc-text">Play - Figure 2d</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#神经网络结构"><span class="post-toc-number">2.3.</span> <span class="post-toc-text">神经网络结构</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#数据集"><span class="post-toc-number">2.4.</span> <span class="post-toc-text">数据集</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#图5更多细节"><span class="post-toc-number">2.5.</span> <span class="post-toc-text">图5更多细节</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#各方评论"><span class="post-toc-number">3.</span> <span class="post-toc-text">各方评论</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#总结与随想"><span class="post-toc-number">4.</span> <span class="post-toc-text">总结与随想</span></a></li></ol>
        </nav>
    </aside>
    
<article id="post-深入浅出看懂AlphaGo元"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">深入浅出看懂AlphaGo元</h1>
        <div class="post-meta">
            <time class="post-time" title="2017-10-18 20:54:32" datetime="2017-10-19T03:54:32.000Z"  itemprop="datePublished">2017-10-18</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


            

        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <p>【阅读时间】15min - 17min<br>【内容简介】AlphaGo Zero论文原文超详细翻译，并且总结了AlphaGo Zero的算法核心思路，附带收集了网上的相关评论<br><a id="more"></a></p>
<p>在之前的详解：<a href="https://charlesliuyx.github.io/2017/05/27/AlphaGo%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/">深入浅出看懂AlphaGo</a>中，详细定义的DeepMind团队定义围棋问题的结构，并且深入解读了AlphaGo1.0每下一步都发生了什么事，就在最近，AlphaGo Zero横空出世。个人观点是，如果你看了之前的文章，你就会觉得这是一个水到渠成的事情</p>
<h1 id="论文正文内容解析"><a href="#论文正文内容解析" class="headerlink" title="论文正文内容解析"></a>论文正文内容解析</h1><p>先上干货论文：<a href="https://deepmind.com/documents/119/agz_unformatted_nature.pdf" target="_blank" rel="external">Mastering the Game of Go without Human Knowledge</a> ，之后会主要<strong>以翻译论文关键点</strong>为主，在语言上<strong>尽量易懂，去除翻译腔</strong></p>
<p>AlphaGo Zero，从本质上来说完全不同于打败樊麾，和李世石的版本</p>
<ul>
<li>算法上，<strong>自对弈强化学习，完全从随机落子开始</strong>，不用人类棋谱。之前使用了大量棋谱学习人类的下棋风格）</li>
<li>数据结构上，只有<strong>黑子白子两种状态</strong>。之前包含这个点的气等相关棋盘信息</li>
<li>模型上，使用<strong>一个</strong>神经网络。之前使用了<strong>策略网络（基于深度卷积神经网）</strong>学习人类的下棋风格，<strong>局面网络</strong>（基于左右互搏生成的棋谱，为什么这里需要使用左右互搏是因为现有的数据集不够，没法判断落子胜率这一更难的问题）来计算在<strong>当前局面下每一个不同落子的胜率</strong></li>
<li>策略上，基于训练好的这个神经网，进行简单的<strong>树形搜索</strong>。之前会使用蒙特卡洛算法实时演算并且<strong>加权得出落子的位置</strong></li>
</ul>
<h2 id="AlphaGo-Zero-的强化学习"><a href="#AlphaGo-Zero-的强化学习" class="headerlink" title="AlphaGo Zero 的强化学习"></a>AlphaGo Zero 的强化学习</h2><p>在开始之前，必须再过一遍如何符号化的定义一个围棋问题</p>
<p>围棋问题，棋盘 <code>19×19=361</code> 个交叉点可供落子，每个点三种状态，白（用<code>1</code>表示），黑（用<code>-1</code>表示），无子（用<code>0</code>表示），用 $\vec s$ <strong>描述</strong>此时<strong>棋盘的状态</strong>，即棋盘的<strong>状态向量</strong>记为 $ \vec s$ （state首字母）<br>$$<br>\vec s = (\underbrace{1,0,-1,\ldots}_{\text{361}})<br>$$<br>假设状态 $\vec s$ 下，暂不考虑不能落子的情况， 那么下一步可走的位置空间也是361个。将下一步的<strong>落子行动</strong>也用一个361维的向量来表示，记为 $\vec a$ （action首字母）<br>$$<br>\vec a = (0,\ldots,0,1,0,\ldots)<br>$$<br>公式1.2 假设其中<code>1</code>在向量中位置为<code>39</code>，则  $\vec a$ 表示在棋盘<code>3行1列</code>位置落<strong>白子</strong>，黑白交替进行</p>
<p>有以上定义，我们就把围棋问题转化为。</p>
<blockquote>
<p>任意给定一个状态  $\vec s$ ，寻找最优的应对策略  $\vec a$ ，最终可以获得棋盘上的最大地盘</p>
</blockquote>
<p>简而言之</p>
<blockquote>
<p>看到 $\vec s$ ，脑海中就是<strong>一个棋盘，上面有很多黑白子</strong></p>
<p>看到 $\vec a$ ，脑海中就想象一个人<strong>潇洒的落子</strong></p>
</blockquote>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>新的网络中，使用了一个参数为 $\theta$ （需要通过训练来不断调整） 的<strong>深度神经网络</strong>（对应状态下的应对策略）$f_\theta$ </p>
<ul>
<li>【网络的输入】表示现在棋盘状态的 $\vec s$ 以及所有的<strong>历史落子序列</strong>（即下成这个局面的过程，类似于<strong>棋谱</strong>）</li>
<li>【网络的输出】两个值：落子的概率和一个数值  $f_{\theta}(\mathbf {\vec s}) = (\mathbf p,v)$ <ul>
<li>【落子的概率 $\mathbf p$】 向量表示下一步在每一个可能位置落子的<strong>未处理概率</strong>（包括不下），即  $p_a = Pr(\vec a|\mathbf {\vec s})$  （公式表示在当前输入的条件下落子行动每个点的概率）</li>
<li>【数值 $v$】 是一个数，它的值表示现在准备下当前下这一步棋的选手<strong>在局面 $\vec s$ 下的胜率</strong>（我这里强调局面是因为网络的输入其实包含棋谱，即历史对战过程）</li>
</ul>
</li>
<li>【网络结构】基于Residual Block（大名鼎鼎ImageNet冠军ResNet）的卷积网络，有20个Residual Block，使用批量归一化Batch normalisation与非线性整流器rectifier non-linearities</li>
</ul>
<h3 id="改进的强化学习算法"><a href="#改进的强化学习算法" class="headerlink" title="改进的强化学习算法"></a>改进的强化学习算法</h3><p>自对弈强化学习算法（<a href="https://mubu.com/doc/WNKomuDNl" target="_blank" rel="external">什么是强化学习</a>，非常建议先看看强化学习的一些基本思想和步骤，有利于理解下面策略、价值的概念，推荐<a href="http://www.cnblogs.com/steven-yang/p/6481772.html" target="_blank" rel="external">系列笔记</a>）</p>
<p>在每一个状态  $\vec s$ ，利用<strong>深度神经网络 $f_\theta$ 预测作为参照</strong>执行MCTS搜索（<a href="https://charlesliuyx.github.io/2017/05/27/AlphaGo%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/#MCTS-蒙特卡洛搜索树——走子演算（Rollout）">蒙特卡洛搜索树算法</a>），<strong>MCTS搜索的输出是每一个状态下在不同位置对应的概率 $\boldsymbol \pi$ （注意这里是一个向量，里面的值是MCTS搜索得出的概率值）</strong>，一种策略，从人类的眼光来看，就是看到现在在局面，选择下在每个不同的落子的点的概率。如下面公式的例子，下在$(1,3)$位置的概率是<code>0.92</code>，程序会选择最高概率的点作为<strong>落子点</strong><br>$$<br>\boldsymbol \pi_i = (\underbrace{0.01,0.02,0.92,\ldots}<em>{\text{361}})<br>$$<br>MCTS搜索得出的<strong>落子概率</strong>比 $f</em>\theta$ 输出的<strong>仅使用神经网络输出的落子概率  $\mathbf p$ </strong>更强，因此，MCTS可以被视为一个强力的<strong>策略改善（policy improvement）过程</strong></p>
<p>使用基于MCTS提升后的策略（policy）来进行落子，然后用最终对局的胜者 $z$ 作为价值（Value）的自对弈方法，作为一个强力的<strong>策略评估（policy evaluation）过程</strong></p>
<p>并用上述的规则，完成一个<strong>通用策略迭代</strong>算法去更新神经网络的<strong>参数</strong> $\theta$ ，使得神经网络输出的<strong>落子概率和评估值</strong>，即  $f_{\theta}(\mathbf {\vec s}) = (\mathbf p,v)$  更加贴近<strong>能把这盘棋局赢下的落子方式（使用不断提升的MCST搜索落子策略$\boldsymbol \pi$ 和自对弈的胜者 $z$ 作为调整依据）</strong>。并且，在下轮迭代中使用<strong>新的参数</strong>来进行自对弈</p>
<p>在这里补充<strong>强化学习</strong>的<strong>通用策略迭代</strong>（Generalized Policy Iteration）方法</p>
<ul>
<li><p>从策略 $\pi_0$ 开始</p>
</li>
<li><p><strong>策略评估（Policy Evaluation）</strong>- 得到策略 $\pi_0$ 的价值 $v_{\pi_0}$ （对于围棋问题，即这一步棋是好棋还是臭棋）</p>
</li>
<li><p><strong>策略改善（Policy Improvement）</strong>- 根据价值 $v_{\pi_0}$，优化策略为 $\pi_{0+1}$ （即人类学习的过程，加强对棋局的判断能力，做出更好的判断）</p>
</li>
<li><p>迭代上面的步骤2和3，直到找到最优价值 $v_*$ ，可以得到最优策略 $\pi_*$ </p>
<p><a name="Figure1"></a></p>
</li>
</ul>
<div align="center"><img src="//charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure1.png" alt="Figure 1" width="600px"></div>

<blockquote>
<p>【 a图】表示对局过程。从状态 $s_1$ 时落子 $a_1$ 直到进行落子 $a_t$ 后得出 $s_T$，此时决出胜者 $z$<br>$\pi_i$ 是每一步MCTS搜索得出的结果（哪一步得分最高，所以才是个柱状图），其中<strong>在哪里落子，即 $a_t$ </strong>由MCTS搜索结果的概率来决定，概率最大的为落子点</p>
<p>【b图】表示更新神经网络参数过程。使用原始落子状态 $s_t$ 作为网络输入，得到此棋盘状态下<strong>下一步落子位置的概率</strong> $\mathbf p_t$ 和<strong>此时当前选手的赢棋概率</strong>：评估值 $v_t$ </p>
<p><strong>以最大化 $\mathbf p_t$ 与 $\pi_t$ 相似度和最小化预测的胜者 $v_t$ 和局终胜者 $z$ 的误差来更新神经网络参数 $\theta$ （详见公式1）</strong> ，更新的参数 $\theta$ 将会在下一轮的迭代中继续进行自我对弈</p>
</blockquote>
<p>我们知道，最初的蒙特卡洛树搜索算法是使用随机落子来进行模拟，在AlphaGo1.0中<a href="https://charlesliuyx.github.io/2017/05/27/AlphaGo%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/#利用强化学习增强棋力">使用<strong>局面函数</strong>辅助<strong>策略函数</strong>作为落子的参考</a>进行模拟。在<strong>最新的模型中，蒙特卡洛搜索树使用神经网络 $f_\theta$ 的输出来作为落子的参考</strong>（详见下图Figure 2）</p>
<p>每一条边 $(\vec s,\vec a)$ （每个状态下的落子选择）保存的是三个值：先验概率 $P(\vec s,\vec a)$，访问次数 $N(\vec s,\vec a)$，行动价值 $Q(\vec s,\vec a)$。每一次模拟落子最大化<a href="https://baike.baidu.com/item/UCT%E7%AE%97%E6%B3%95/19451060?fr=aladdin" target="_blank" rel="external">上限置信区间</a> $Q(\vec s,\vec a) + U(\vec s,\vec a)$ 其中 $U(\vec s,\vec a) \propto \frac{P(\vec s,\vec a)}{1 + N(\vec s,\vec a)}$ 直到遇到叶子节点 $s’$，叶子节点（终局）只会被产生一次用于产生<strong>先验概率和评估值</strong>，符号表示即 $f_\theta(s’) = (P(s’,\cdot), V(s’))$ ，每次更新 $Q(\vec s,\vec a)$ 的方式与AlphaGo1.0类似，见<a href="https://charlesliuyx.github.io/2017/05/27/AlphaGo%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/#MCTS-蒙特卡洛搜索树——走子演算（Rollout）">链接</a></p>
<div align="center"><img src="//charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure2.png" alt="Figure 2" width="800px"></div>

<blockquote>
<p>【a图】表示上一段中提到的模拟方式，选 $Q+U$ 更大的作为落子点，这个数值随着每次遍历会增加（赢了就会加分）</p>
<p>【b图】表示已经扩展到了叶子节点</p>
<p>【c图】表示行动分数 $Q$ 等于评估值 $V$ 子树落子的平均值</p>
<p>【d图】表示当搜索完成，返回这个状态的下在每一个位置的概率 $\boldsymbol \pi$，除以 $N^{1/\tau}$（$N$为访问次数，$\tau$ 为控温常数）</p>
<p>更加具体的详解见：<a href="#搜索算法">搜索算法</a></p>
</blockquote>
<p>MCTS可以看成一个自对弈算法根据神经网络的参数 $\theta$ 和根的状态 $\vec s$ 去计算<strong>每一个状态下落子位置</strong></p>
<h3 id="训练步骤总结"><a href="#训练步骤总结" class="headerlink" title="训练步骤总结"></a>训练步骤总结</h3><p>使用MCTS下每一步棋，进行自对弈，使用<strong>强化学习算法（必须了解通用策略迭代的基本方法）</strong>训练神经网络</p>
<ul>
<li>神经网络参数随机初始化 $\theta_0$</li>
<li>每一次迭代过程 $i \geqslant 1$ ，都产生<strong>一盘自对弈过程</strong>（见<a href="#Figure1">Figure-1a</a>）</li>
<li>在第 $t$ 步，MCTS搜索 $\boldsymbol \pi_t = \alpha \theta_{i-1}(s_t)$ 使用前一次迭代的神经网络 $f_{\theta_{i-1}}$，<strong>根据选取策略概率矩阵 $\boldsymbol \pi_t$ 的概率来进行落子</strong></li>
<li>在 $T$ 步停止条件为：双方选择跳过；超过限制下限；棋盘无地落子。棋局停止后，得到最终奖励Reward $r_T \in {-1,+1}$</li>
<li>每 $t$ 时刻每一步中的数据存储为  $\vec s_t,\mathbf \pi_t, z_t$  ，其中 $z_t = \pm r_T$ 表示在第 $t$ 步时的胜者</li>
<li>同时，使用上一步 $\vec s$ 迭代的数据 $(\vec s, \boldsymbol \pi, z)$ 作为样本来训练网络参数 $\theta_i$，<strong>最大化 $\mathbf p_t$ 与 $\pi_t$ 相似度和最小化预测的胜者 $v_t$ 和局终胜者 $z$ 的误差来更新神经网络参数 $\theta$ </strong>，损失函数公式如下</li>
</ul>

$$
l = (z - v)^2 - \boldsymbol {\pi}^T \log(\mathbf p) + c \Vert \theta \Vert ^2 \tag 1
$$

<blockquote>
<p>其中$c$是L2正则化的系数</p>
</blockquote>
<h2 id="AlphaGo-Zero训练过程中的经验"><a href="#AlphaGo-Zero训练过程中的经验" class="headerlink" title="AlphaGo Zero训练过程中的经验"></a>AlphaGo Zero训练过程中的经验</h2><p>最开始，使用完全的随机落子<strong>训练持续了大概3天</strong>。训练过程中，产生490万场自对弈，每次MCTS大约1600次模拟，每一步使用的时间0.4秒。使用了2048个位置的70万个Mini-Batches来进行训练。</p>
<p>训练结果如下，图3</p>
<div align="center"><img src="//charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure3.png" alt="Figure 3" width="1200px"></div>

<blockquote>
<p>a图表示随时间AlphaGo Zero棋力的增长情况，<strong>显示了每一个不同的棋手 $\alpha_{\theta_i}$ 在每一次强化学习迭代时的表现</strong>，可以看到，它的增长曲线非常平滑，没有很明显的震荡，稳定性很好</p>
<p>b图表示的是<strong>预测准确率</strong>基于不同迭代第$i$轮的 $f_{\theta_i}$</p>
<p>c图表示的MSE（平方误差）</p>
</blockquote>
<p>在24小时的学习后，无人工因素的强化学习方案就打败了通过模仿人类棋谱的监督学习方法</p>
<p>为了分别评估结构和算法对结构的影响，得到了，下图4</p>
<div align="center"><img src="//charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure4.png" alt="Figure 4" width="1200px"></div>

<blockquote>
<p>dual-res 表示 AlphaGo Zero<br>sep-conv表示 AlphaGo Lee（几百李世乭的）使用的网络结构（P+V且分开）</p>
</blockquote>
<h2 id="AlphaGo-Zero学到的知识"><a href="#AlphaGo-Zero学到的知识" class="headerlink" title="AlphaGo Zero学到的知识"></a>AlphaGo Zero学到的知识</h2><p>在训练过程中，AlphaGo Zero可以一步步的学习到一些<strong>特殊的围棋技巧（定式）</strong>，入图5</p>
<div align="center"><img src="//charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure5.png" alt="Figure 5" width="900px"></div>

<blockquote>
<p>中间的黑色横轴表示的是学习时间</p>
<p>【a图】对应的5张棋谱展现的是不同阶段AlphaGo Zero在自对弈过过程中展现出来的<strong>围棋定式</strong>上的新发现</p>
<p>【b图】展示在右星位上的定式下法的进化。可以看到训练到50小时，<strong>点三三出现了</strong>，但再往后训练，b图中的<strong>第五种定式</strong>高频率出现，在AlphGa Zero看来，这一种形式似乎更加强大</p>
<p>【c图】展现了前80手自对弈的棋谱伴随时间，明显有很大的提升，在第三幅图中，已经展现出了比较明显的<strong>围</strong>的倾向性</p>
<p>具体频率图见：<a href="#Figure5">频率随时间分布</a></p>
</blockquote>
<p>下图6展示不同种AlphaGo版本的棋力情况</p>
<div align="center"><img src="//charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure6.png" alt="Figure 6" width="900px"></div>

<blockquote>
<p>【a图】随着训练时间的真假，棋力的增强</p>
<p>【b图】最终的AlphaGo Zero训练了<strong>40天，使用40个Risdual Block</strong>。</p>
<p><strong>其中的raw network是每一步的仅仅使用训练好的深度学习神经网的 $\mathbf \p$的输出 $\mathbf p_a$ 来下棋，而不使用MCTS搜索</strong></p>
</blockquote>
<p>最终，AlphaGo Zero 与 AlphaGo Master的<strong>对战比分为89：11</strong>，对局中限制一场比赛在2小时之内</p>
<h1 id="论文附录内容"><a href="#论文附录内容" class="headerlink" title="论文附录内容"></a>论文附录内容</h1><p>我们知道，Nature上的文章一般都是很强的可读性和严谨性，每一篇文章的正文可能只有4-5页，但是附录一般会远长于正文。基本所有你的<strong>技术细节疑惑</strong>都可以在其中找到结果，这里值列举一些我自己比较感兴趣的点，如果你是专业人士，甚至想复现AlphaGo Zero，读原文更好更精确</p>
<h2 id="自对弈训练工作流"><a href="#自对弈训练工作流" class="headerlink" title="自对弈训练工作流"></a>自对弈训练工作流</h2><p>AlphaGo Zero的工作流由三个模块构成，可以异步多线程进行：</p>
<ul>
<li>深度神经网络<strong>参数</strong> $\theta_i$ 根据自对弈数据<strong>持续优化</strong></li>
<li>持续对棋手 $\alpha_{\theta_i}$ <strong>棋力值</strong>进行<strong>评估</strong></li>
<li>使用表现最好的 $\alpha_{\theta_*}$ 用来产生新的<strong>自对弈数据</strong></li>
</ul>
<h3 id="优化参数"><a href="#优化参数" class="headerlink" title="优化参数"></a>优化参数</h3><p>每一个神经网络 $f_{\theta_i}$ 在<strong>64个GPU工作节点</strong>和<strong>19个CPU参数服务器</strong>上进行优化。</p>
<p><strong>批次（batch）每个工作节点32个</strong>，每一个<strong>mini-batch大小为2048</strong>。每一个 <strong>mini-batch 的数据</strong>从最近<strong>50万盘</strong>的自对弈棋谱的状态中联合随机采样。</p>
<p><strong>神经网络权重</strong>更新使用带有<strong>动量（momentum）和学习率退火（learning rate annealing）的随机梯度下降法（SGD）</strong>，损失函数见公式1</p>
<p>学习率退火比率见下表</p>
<table>
<thead>
<tr>
<th style="text-align:center">步数（千）</th>
<th style="text-align:center">强化学习率</th>
<th style="text-align:center">监督学习率</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0-200</td>
<td style="text-align:center">$10^{-2}$</td>
<td style="text-align:center">$10^{-1}$</td>
</tr>
<tr>
<td style="text-align:center">200-400</td>
<td style="text-align:center">$10^{-2}$</td>
<td style="text-align:center">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align:center">400-600</td>
<td style="text-align:center">$10^{-3}$</td>
<td style="text-align:center">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align:center">600-700</td>
<td style="text-align:center">$10^{-4}$</td>
<td style="text-align:center">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align:center">700-800</td>
<td style="text-align:center">$10^{-4}$</td>
<td style="text-align:center">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align:center">&gt;800</td>
<td style="text-align:center">$10^{-4}$</td>
<td style="text-align:center">-</td>
</tr>
</tbody>
</table>
<p>动量参数设置为<strong>0.9</strong></p>
<p>L2正则化系数设置为 $c = 10^{-4}$</p>
<p><strong>优化过程每1000个训练步数</strong>执行一次，并使用这个<strong>新模型</strong>来生成下一个Batch的<strong>自对弈棋谱</strong></p>
<h3 id="评估器"><a href="#评估器" class="headerlink" title="评估器"></a>评估器</h3><p>在使用检查点（checkpoint）新的神经网络去生成自对弈棋谱前，使用<strong>现有的最好网络</strong>来对它进行评估。具体设计方法见论文（不赘述）</p>
<h3 id="自对弈"><a href="#自对弈" class="headerlink" title="自对弈"></a>自对弈</h3><p>通过评估器，现在已经有一个<strong>当前的最好棋手</strong> $\alpha_{\theta_*}$。在每一次迭代中， $\alpha_{\theta_*}$ 自对弈<strong>25000盘</strong>，每一步使用1600次MCTS模拟（每一步大约会花费0.4秒）。</p>
<p>在前30步，温度 $\tau = 1$。在之后的棋局中，温度设为无穷小。并在先验概率中加入狄利克雷噪声。这个噪声保证所有的落子可能都会被尝试</p>
<h2 id="搜索算法"><a href="#搜索算法" class="headerlink" title="搜索算法"></a>搜索算法</h2><p>这一部分详解的AlphaGo Zero的算法核心示意图<a href="#Figure2">Figure2</a></p>
<p>每一个搜索树的中的节点 $\vec s$ （<strong>为了方便，后面简写 $\vec s$ 和 $\vec a$，不使用向量标识</strong>）包含一条边 $(s,a)$ 对应所有可能的落子 $a \in \mathcal A(s)$ ，每一条边存储一个数据集<br>$$<br>{N(s,a), W(s,a), Q(s,a), P(s,a)}<br>$$</p>
<blockquote>
<p>$N(s,a)$ 表示<strong>MCST的访问次数</strong><br>$W(s,a)$ 表示<strong>行动价值的总和</strong><br>$Q(s,a)$ 表示<strong>行动价值的均值</strong><br>$P(s,a)$ 表示选择这条边的<strong>先验概率</strong></p>
</blockquote>
<p>多线程执行多次模拟，每一次迭代过程先重复执行1600次Figure 2中的前3个步骤，再下现在的这一步棋</p>
<h3 id="Selcet-Figure2a"><a href="#Selcet-Figure2a" class="headerlink" title="Selcet - Figure2a"></a>Selcet - Figure2a</h3><p>MCTS中的<strong>选择步骤</strong>和之前的版本相似，详见<a href="https://charlesliuyx.github.io/2017/05/27/AlphaGo%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/#AlphaGo">AlphaGo之前的详解文章</a>，这篇博文<strong>详细通俗</strong>的解读了这个过程。概括来说，假设<code>L</code>步走到叶子节点，当走第 $t<l$ 步时，根据**搜索树的统计概率落子**="" $$="" \vec="" a_t="\operatorname*{argmax}_{\vec" a}(q(\vec="" s_t,="" a)="" +="" u="" (\vec="" a))="" <="" p="">
<p>其中计算 $U (\vec s_t, \vec a)$ 使用PUCT算法的变体</p>

$$
U(\vec s, \vec a) = c_{puct}P(\vec s, \vec a) \frac{\sqrt{\Sigma_{\vec b} N(\vec s, \vec b)}}{1 + N(\vec s, \vec a)}
$$

<p>其中 $c_{puct}$ 是一个常数。这种搜索策略落子选择<strong>最开始</strong>会跟趋向于<strong>高先验概率</strong>和<strong>低访问次数</strong>的，但逐渐的会更加趋向于选择有着<strong>更高评价值</strong>的落子</p>
<h3 id="Expand-and-evaluate-Figure-2b"><a href="#Expand-and-evaluate-Figure-2b" class="headerlink" title="Expand and evaluate - Figure 2b"></a>Expand and evaluate - Figure 2b</h3><p>将叶子节点 $\vec s_\L$ 加到一个队列中de等待输入至神经网络中进行评估， $f_\theta(d_i(\vec s_L)) = (d_i(p), v)$ ，其中 $d_i$ 表示一个<strong>1至8的随机数</strong>来表示双面反射和旋转选择</p>
<p>队列中的不同位置组成一个大小为8的mini-batch输入到神经网络中进行评估。整个MCTS搜索线程被锁死知道评估过程完成。叶子节点被展开，每一条边 $(\vec s_L,\vec a)$被初始化为  $\{N(\vec s_L,\vec a) = 0, W(\vec s_L,\vec a) = 0, Q(\vec s_L,\vec a) = 0, P(\vec s_L,\vec a) = p_a\}$ 。之后将神经网络的输出值 $v$ 传回</p>
<h3 id="Backup-Figure-2c"><a href="#Backup-Figure-2c" class="headerlink" title="Backup - Figure 2c"></a>Backup - Figure 2c</h3><p>沿着回溯的路线将边的统计数据更新。</p>

$$
N(\vec s_t, \vec a_t) = N(\vec s_t, \vec a_t)  + 1 \\
W(\vec s_t, \vec a_t)  = W(\vec s_t, \vec a_t)  + v \\
Q(\vec s_t, \vec a_t)  = \frac{W(\vec s_t, \vec a_t) }{N(\vec s_t, \vec a_t) }
$$

<p>使用虚拟损失（virtual loss）确保每一个线程评估不同的节点</p>
<h3 id="Play-Figure-2d"><a href="#Play-Figure-2d" class="headerlink" title="Play - Figure 2d"></a>Play - Figure 2d</h3><p>进行了一次MCTS搜索后，AlphaGo Zero才从 $\vec s_0$ 状态下走出第一步 $\vec s_0$，与访问次数成幂指数比例</p>

$$
\pi(\vec a|\vec s_0) = \frac {N(\vec s_0,a)^{1/\tau}}{\Sigma_{\vec b} N(\vec s_0, \vec b)^{1/\tau}}
$$

<p>其中 $\tau$ 是一个温度常数用来控制探索等级（level of exploration）。搜索树会在接下来的走子中继续使用，如果孩子节点和落子的位置吻合，它就成为新的根节点，保留子树的所有统计数据，同时丢弃其他的树。<strong>如果根的评价值和它最好孩子的评价值都低于 $v_{resign}$ AlphaGo Zero就认输</strong></p>
<p>与之前的版本的MCTS相比，<strong>AlphaGo Zero最大的不同是没有使用走子网络（Rollout），而是使用一个神经网络</strong></p>
<h2 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h2><p>输入 <code>19×19×17</code>图像。8个值表示现在落子选手的落子状态，8个值表示相应对手的落子状态，C代表现在落子选手的颜色，白0黑1。也就是说，<strong>神经网络的输入，包含了前8步落子（包括现在这一步）的特征信息</strong></p>
<p>具体Residual网络的结构参看论文，不赘述</p>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p><a href="http://gokifu.com/" target="_blank" rel="external">GoKifu数据集</a>，和<a href="https://u-go.net/gamerecords/" target="_blank" rel="external">KGS数据集</a></p>
<h2 id="图5更多细节"><a href="#图5更多细节" class="headerlink" title="图5更多细节"></a>图5更多细节</h2><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="//charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure5a.png" alt="Figure 5a中每种定式出现的频率图" title="">
                </div>
                <div class="image-caption">Figure 5a中每种定式出现的频率图</div>
            </figure>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="//charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/Figure5b.png" alt="Figure 5b中每种定式出现的频率图" title="">
                </div>
                <div class="image-caption">Figure 5b中每种定式出现的频率图</div>
            </figure>
<h1 id="各方评论"><a href="#各方评论" class="headerlink" title="各方评论"></a>各方评论</h1><p><a href="https://zhuanlan.zhihu.com/p/30325845?group_id=905063580597968896" target="_blank" rel="external">量子位汇集各家评论</a></p>
<ul>
<li>不是<strong>无监督学习</strong>，带有明显胜负规则的强化学习是<strong>强监督</strong>的范畴</li>
<li>无需<strong>担心快速的攻克其他领域</strong>，核心还是<strong>启发式搜索</strong></li>
</ul>
<h1 id="总结与随想"><a href="#总结与随想" class="headerlink" title="总结与随想"></a>总结与随想</h1><p><strong>AlphaGo Zero = 一个超强的蒙特卡洛搜索树算法</strong></p>
</l$></p>
        </div>

        <blockquote class="post-copyright">
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2017-10-21T09:22:24.508Z" itemprop="dateUpdated">2017-10-21 02:22:24</time>
</span><br>


        
    </div>
    <footer>
        <a href="https://charlesliuyx.github.io">
            <img src="/img/avatar.jpg" alt="遥行 Go Further">
            遥行 Go Further
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AlphaGo/">AlphaGo</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/&title=《深入浅出看懂AlphaGo元》 — Go Further&pic=https://charlesliuyx.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/&title=《深入浅出看懂AlphaGo元》 — Go Further&source=【阅读时间】15min - 17min【内容简介】AlphaGo Zero论文原文超详细翻译，并且总结了AlphaGo Zero的算法核心思路，附带收集了..." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《深入浅出看懂AlphaGo元》 — Go Further&url=https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/&via=https://charlesliuyx.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between flex-row-reverse">
  

  
    <div class="waves-block waves-effect next">
      <a href="/2017/10/17/【直观详解】线性代数中的正交正规正定转置/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">【直观详解】线性代数中的转置正交正规正定</h4>
      </a>
    </div>
  
</nav>



    













<section class="comments" id="comments">
    <div id="gitment_thread"></div>
    <link rel="stylesheet" href="//unpkg.com/gitment/style/default.css">
    <script src="//unpkg.com/gitment/dist/gitment.browser.js"></script>
    <script>
        var gitment = new Gitment({
            owner: 'CharlesLiuyx',
            repo: 'BlogComment',
            oauth: {
                client_id: '2c1daf79f5a857cab513',
                client_secret: 'd3ec6eb5ef61ef24dbfee8ee9645b70444292bcc',
            },
        })
        gitment.render('comments')
    </script>
</section>




</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        你的鼓励是我前进的力量
        <i class="icon icon-quote-right"></i>
    </h3>
    <ul class="reward-items">
        
        <li>
            <img src="/img/wechat.jpg" title="微信打赏二维码" alt="微信打赏二维码">
            <p>微信</p>
        </li>
        

        
        <li>
            <img src="/img/alipay.jpg" title="支付宝打赏二维码" alt="支付宝打赏二维码">
            <p>支付宝</p>
        </li>
        
    </ul>
</div>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p>
            <span>Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a></span>
            <span>遥行 Go Further &copy; 2017</span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/&title=《深入浅出看懂AlphaGo元》 — Go Further&pic=https://charlesliuyx.github.io/img/avatar.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/&title=《深入浅出看懂AlphaGo元》 — Go Further&source=【阅读时间】15min - 17min【内容简介】AlphaGo Zero论文原文超详细翻译，并且总结了AlphaGo Zero的算法核心思路，附带收集了..." data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《深入浅出看懂AlphaGo元》 — Go Further&url=https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/&via=https://charlesliuyx.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACs0lEQVR42u3ay07dQBAFQP7/p5MtUXJ9T7+ASOUVMsie8mKmOd0fH/H169P16v7fv/38N69+fv7t89vXLjw8PLzB0l8t95mRL7QHTu6//AR4eHh4Z7zqht57ffKW52fma8bDw8P7mbz8xdFS4oIeDw8P7//iJUVtHjokz/zSgwEPDw8v5uWPToKD/E5+SJxnLXh4eHgxr1r+/oSfT/p7eHh4eOOu+rzpNSm7eyv84zl4eHh4B7xqOTtpg+2W5oXhMDw8PLxVXr4dz0esquFsFfmP5+Dh4eGd8ZI2Un6nWjRvRcnNzh4eHh5ekZcPUfUOg4TdK68LQwl4eHh4Z7xqwToJf5OFloer8jYYHh4e3gGv2rJaCA4Gx8Cb4wEPDw9vlfe8veb//Odldy8abh5veHh4eAe8HHNREFef1htrwMPDw9vlVbf4fHOvFt+91lrU+sLDw8M74G099GLpozYYHh4e3gGvGkPkUey8hVYNi8stMTw8PLwWb1IoT2LfXoBb/eh4eHh4F7xk465u62stq2JAXGiA4eHh4Q14k0BhEl70YoXyM/Hw8PCOeZPXTGKL6udozpTh4eHhrfKqsGohnhfcW5EHHh4e3h2vGgpUW1950FBtm0XRLR4eHt4Zr9p2qoa5WyFFfvy8Sanx8PDwxrxJ8z4fq+oBeuC12QQ8PDy81XmkPFBIDoO8TC8fZnh4eHhnvN4m3jta8mB34cLDw8M7huWRQbURVS2aq5/mTX8PDw8Pb4m3e3jkR0KvqdYbaMDDw8O74FVbU8lwQK9czg+kZPwLDw8P75qXD1old7aC2vyjRCk1Hh4e3jfxevFElZ2TFqbJ8PDw8L6ElxfQyRHSG1+Iols8PDy8M141CMgbWsl232uwRQU6Hh4e3gFvXsJWX5Y30gql81aegYeHh/d0/QaM4V/0Z8OoMAAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };



</script>

<script src="/js/main.min.js?v=1.6.7"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.6.7" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->





</body>
</html>
