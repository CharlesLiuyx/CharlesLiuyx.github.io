<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SH.SF Blog</title>
  <subtitle>万古长空，一朝风月</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="/"/>
  <updated>2017-05-28T10:47:24.000Z</updated>
  <id>/</id>
  
  <author>
    <name>SH.SF</name>
    <email>297106826@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深入浅出了解AlphaGo如何下棋</title>
    <link href="/2017/05/27/AlphaGo%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/"/>
    <id>/2017/05/27/AlphaGo运行原理解析/</id>
    <published>2017-05-27T18:51:22.000Z</published>
    <updated>2017-05-28T10:47:24.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><p>围棋问题，棋盘 <code>19 * 19 = 361</code> 个交叉点可供落子，每个点三种状态，白（用<code>1</code>表示），黑（用<code>-1</code>表示），无子（用<code>0</code>表示），用 $\vec s$ <strong>描述</strong>此时<strong>棋盘的状态</strong>，即棋盘的<strong>状态向量</strong>记为 $ \vec s$ （state首字母）。</p>
<a id="more"></a>
<p>$$<br>\vec s = (\underbrace{1,0,-1,\ldots}_{\text{361}})\tag {1-1}<br>$$<br>假设状态 $\vec s$ 下，暂不考虑不能落子的情况， 那么下一步可走的位置空间也是361个。将下一步的<strong>落子行动</strong>也用一个361维的向量来表示，记为 $\vec a$ （action首字母）。<br>$$<br>\vec a = (0,\ldots,0,1,0,\ldots)\tag {1-2}<br>$$<br>公式1.2 假设其中<code>1</code>在向量中位置为<code>39</code>，则  $\vec a$ 表示在棋盘<code>(3,1)</code>位置落<strong>白子</strong>，3为横坐标，1为列坐标</p>
<p>有以上定义，我们就把围棋问题转化为。</p>
<blockquote>
<p>任意给定一个状态  $\vec s$ ，寻找最优的应对策略  $\vec a$ ，最终可以获得棋盘上的最大地盘</p>
</blockquote>
<p>接下来的问题是，如何解决这样一个问题呢？</p>
<p>先上论文！干货第一</p>
<p><a href="http://ai.arizona.edu/sites/ai/files/resources/mastering_the_game_of_go_with_deep_neural_networks_and_tree_search.pdf" target="_blank" rel="external">Mastering the game of Go with deep neural networks and tree search</a></p>
<h1 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h1><p>首先想到，棋盘也是一幅图像，那么在当时最好用的图像处理算法就是<strong>深度卷积神经网络</strong>（Deep Convolutional Neural Network）。</p>
<h2 id="深度卷积神经网络——策略函数（Policy-Network）"><a href="#深度卷积神经网络——策略函数（Policy-Network）" class="headerlink" title="深度卷积神经网络——策略函数（Policy Network）"></a>深度卷积神经网络——策略函数（Policy Network）</h2><p>关于什么是<code>CNN</code>，这篇文章十分靠谱，深入浅出的讲解了什么是<code>CNN</code></p>
<p><a href="http://www.kdnuggets.com/2016/11/intuitive-explanation-convolutional-neural-networks.html" target="_blank" rel="external">An Intuitive Explanation of Convolutional Neural Networks</a> （好像<a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" target="_blank" rel="external">原地址</a>挂了）</p>
<p>大致可以理解为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2017/05/27/AlphaGo运行原理解析/CNN.png" alt="CNN例子" title="">
                </div>
                <div class="image-caption">CNN例子</div>
            </figure>
<p>对一副图像进行处理，给定很多样本进行训练，使得最后的神经网络可以获得指定（<strong>具有分类效果</strong>）的输出。</p>
<p>比如，根据上图可以观察到（这是一个已经训练好的神经网络），最右侧的<strong>输出</strong>是<code>[0.01 , 0.04 , 0.94 , 0.02]</code>，其中第三个值<code>0.94</code>代表的是boat，接近1，所以我们判断这幅图片中有船这个物体（类似的，如果使用这幅图像进行训练，那么<strong>指定</strong>输出应该是[0, 0, 1, 0]，因为图中只有船这个物体）。</p>
<p>总而言之，<strong><code>CNN</code>可以帮助我们提取出图像中有实际含义的特征</strong>，那么这和围棋又有什么关系呢？我们来看看Deepmind团队是怎么运用<code>CNN</code>来解决围棋问题。</p>
<h3 id="深度卷积神经网络解决围棋问题"><a href="#深度卷积神经网络解决围棋问题" class="headerlink" title="深度卷积神经网络解决围棋问题"></a>深度卷积神经网络解决围棋问题</h3><p>2015年，<code>Aja Huang</code>在ICLR的论文<a href="https://arxiv.org/pdf/1412.6564.pdf" target="_blank" rel="external">Move Evaluation in Go Using Deep Convolutional Neural Networks</a>中就提出了如何使用<code>CNN</code>来解决围棋问题。</p>
<p>他从围棋对战平台KGS上获得了人类选手的围棋对弈棋谱，对于每一个状态 $ \vec s$，都会有一个人类进行$ \vec a$的落子，这也就是一个天然训练样本 $ \langle \vec s,\vec a\rangle $，如此可以得到3000万个训练样本。</p>
<p>之后，将 $ \vec s$ 看做一个<code>19*19</code>的二维图像（具体实现依据论文输入数据是<code>19*19*48</code>（48是这个位置的其他信息，比如气等信息，激励函数用的 tanh）使用<code>CNN</code>进行训练，目标函数就是人类落子向量 ${\vec a}’$，通过使用海量的数据，不断让计算机接近人类落子的位置。就可以得到一个<strong>模拟</strong>人类棋手下棋的神经网络。</p>
<p>使用训练的结果，我们可以得到一个神经网络用来计算对于每一个当前棋盘状态 $ \vec s$ ，所对应的落子向量 $ \vec a$ 的概率分布（之所以是概率分布，是因为，计算好的神经网络，输出一般是一个0-1之间的浮点数，越接近1的点表示在这个位置越接近人类的风格，也可以等同于作为人类概率最大的落点。<br>$$<br>\vec a=f(\vec s) \tag{2-1}<br>$$<br>根据公式2.1，我们记<code>f()</code>为<code>P_human</code>，论文中也叫做<code>Policy Network</code>，也称策略函数。表示的含义是</p>
<blockquote>
<p>在 状态 $\vec s$ 下，进行哪一个落子 $\vec a$ 是<strong>最接近人类风格的</strong></p>
</blockquote>
<p>计算出来的直观结果，对应到棋盘上如下图，可以看到，红色的区域的值有60%，次大值位于右方，是35%（此图来自于AlphaGo论文）</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2017/05/27/AlphaGo运行原理解析/PolicyNetwork.png" alt="Policy Network" title="">
                </div>
                <div class="image-caption">Policy Network</div>
            </figure>
<h3 id="深度卷积网络策略的棋力"><a href="#深度卷积网络策略的棋力" class="headerlink" title="深度卷积网络策略的棋力"></a>深度卷积网络策略的棋力</h3><p>很不幸，据<code>Aja Huang</code>本人说，这个网络的棋力大概相当于业余6段所有的的人类选手。远远未能超过当时最强的围棋电脑程序<code>CrazyStone[1,5]</code>。</p>
<p>既然比不过，那么就学习它，<code>Aja Huang</code>打算把<code>P_human</code>和<code>CrazyStone</code>结合一下，那么问题就来了，    <code>CrazyStone</code>是怎么来解决围棋问题的呢？</p>
<p>这是<code>Aja Huang</code>的老师<code>Remi Colulum</code>在2006年对围棋AI做出的另一大重要突破</p>
<p><strong>干货论文送上</strong> MCTS</p>
<p><a href="https://github.com/papers-we-love/papers-we-love/blob/5a54fa883a813e81b1e54bfed9669fc8961dedb4/artificial_intelligence/efficient-selectivity-and-backup-operators-in-monte-carlo-tree-search.pdf" target="_blank" rel="external">Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search</a></p>
<h2 id="MCTS-蒙特卡洛搜索树——走子演算（Rollout）"><a href="#MCTS-蒙特卡洛搜索树——走子演算（Rollout）" class="headerlink" title="MCTS 蒙特卡洛搜索树——走子演算（Rollout）"></a>MCTS 蒙特卡洛搜索树——走子演算（Rollout）</h2><p>蒙特卡洛搜索树（Monte-Carlo Tree Search）是一种<em>大智若愚</em>的方法，它的基本思想是：</p>
<p>首先模拟一盘对决，使用的思路很简单，<strong>随机</strong></p>
<ul>
<li>面对一个空白棋盘 $\vec s_0$，最初我们对棋盘一无所知，假设所有落子的方法<strong>分值</strong>都相等，设为<code>1</code></li>
<li>之后，<strong>【随机】</strong>从<code>361</code>种方法中选一种走法 $\vec a_0$，在这一步后，棋盘状态变为 $\vec s_1$。之后假设对方也和自己一样，<strong>【随机】</strong>走了一步，此时棋盘状态变为 $\vec s_2$</li>
<li>重复以上步骤直到 $\vec s_n$并且双方分出胜负，此时便完整的模拟完了一盘棋，我们假设一个变量<code>r</code>，胜利记为1，失败则为0</li>
</ul>
<p>那么问题就来了，如果这一盘赢了，那意味着这一连串的下法至少比对面那个二逼要明智一些，毕竟我最后赢了，那么我把这次落子方法 $(\vec s_0, \vec a_0)$ 记下来，并把它的分值变化：<br>$$<br>\text{新分数} = \text{初始分数} + r \tag{2-2}<br>$$<br>同理，可以把之后所有随机出来的落子方法 $(\vec s_i, \vec a_i)$ 都应用2-2公式，即都加<code>1</code>分。之后开始第二次模拟，这一次，我们对棋盘不是一无所知了，至少在  $\vec s_0$ 状态我们知道落子方法  $\vec a_0$ 的分值是2，其他都是1，我们使用这个数据的方法是：在这次<strong>随机</strong>中，<strong>我们随机到  $\vec a_0$ 状态的概率要比其他方法高一点</strong>。</p>
<p>之后，我们不断重复以上步骤，这样，那些看起来不错（以最后的胜负来作为判断依据）的落子方案的分数就会越来越高，并且这些落子方案也是比较有前途的，会被更多的选择。</p>
<p>最后，当进行了10万盘棋后，在此状态选择那个<strong>分数最高</strong>的方案落子，此时，才真正下了<strong>这步棋</strong>。这种过程在论文里被称为<strong>Rollout</strong></p>
<p>蒙特卡洛搜索树的方法十分的深刻精巧，充满的创造力，它有一些很有意思的特点：</p>
<ul>
<li>没有任何人工决策的<code>if else</code>逻辑，完全依照规则本身，通过不断的想象（随机）来进行自我对弈，最后提升这一步的质量。有意思的是，其实这也是遵照了人类下棋的思维模式，人类中，水平越高的棋手，算的棋越多，只是人类对于每一个落子的判断能力更加强大，思考中的棋路，也比<strong>随机</strong>方式有效的多，但是机器胜在人多。<em>注意，这一个特点也为之后的提高提供了思路</em>。</li>
<li>MCTS可是持续运行。这种算法在对手思考对策的时候自己也可以思考对策。在对方思考落子的过程中，MCTS也可以继续进行演算，在对面落子后，在用现在棋盘的情况进行演算，并且之前计算的结果一定可以用在现在情况中，因为对手的下的这步棋，很可能也在之前演算的高分落子选择内。这一点十分像人类</li>
<li>MCTS是<strong>完全可并行</strong>的算法</li>
</ul>
<p><code>Aja Huang</code>很快意识到这种方法的缺陷在哪里：初始策略（或者说<strong>随机的落子方式</strong>）太过简单。就如同上面第一条特点所说，人类对每种  $\vec s$ （棋型）都要更强的判断能力，那么我们是否可以用<code>P_human</code>来代替随机呢？</p>
<p><code>Aja Huang</code>改进了MCTS，每一步不使用随机，而是现根据<code>P_human</code>计算得到  $\vec a$ 可能的概率分布，以这儿概率为准来挑选下一个  $\vec a$。一次棋局下完之后，新分数按照下面的方式来更新<br>$$<br>\text{新分数} = \text{调整后的初始分} + \text{通过模拟的赢棋概率} \tag{2-3}<br>$$<br>如果某一步被随机到很多次，就应该主要依据模拟得到的概率而非<code>P_human</code>，就是说当盘数不断的增加，模拟得到的结果可能会好于<code>P_human</code>得到的结果。</p>
<p>所以<code>P_human</code>的初始分会被打个折扣，这也是公式2-3中的调整后的初始分的由来<br>$$<br>\text{调整后的初始分} = \frac{\text{P_human}}{(\text{被随机到的次数} + 1)} \tag{2-4}<br>$$<br>如此一来，就在整张地图上利用<code>P_human</code>快速定位了比较好的落子方案，也增加了其他位置的概率。实际操作中发现，此方案不可行，因为<code>P_human</code><strong>太慢了</strong>。</p>
<p>一次<code>P_human</code>的计算需要<code>3ms</code>，随机算法<code>1us</code>，慢了3000倍，所以，<code>Aja huang</code>训练了一个简化版本的<code>P_human_fast()</code>，把神经网络层数、输入特征减少，耗时下降到<code>2us</code>，基本满足了要求。</p>
<p>更多的，策略是，先以<code>P_human</code>开局，走前面大概20步，之后再使用<code>P_human_fast</code>走完剩下的到最后。兼顾速度和准确性。</p>
<p>综合了深度卷积神经网络和MCTS两种方案，此时的围棋程序已经可以战胜所有其他电脑，虽然和其他人类职业选手还有一定的差距。</p>
<p>2015年2月，<code>Aja Huang</code>在Deepmind的同事在顶级学术期刊nature上发表的文章 <a href="http://gnusha.org/~nmz787/pdf/Human-level_control_through_deep_reinforcement_learning.pdf" target="_blank" rel="external">Human-level control through deep reinforcement learning</a> 用神经网络打游戏。这篇文章，给AlphaGo提供的了新的方向</p>
<h2 id="强化学习——局面函数（Value-Network）"><a href="#强化学习——局面函数（Value-Network）" class="headerlink" title="强化学习——局面函数（Value Network）"></a>强化学习——局面函数（Value Network）</h2><p>强化学习（Reinforcement learning）用来实现<strong>左右互搏和自我进化</strong>，首先说说这篇论文干了一件什么事情，Deepmind团队的大牛们使用强化学习的方法在红白机上打通了200多个游戏，大多数得分都要比人好。</p>
<h3 id="什么是强化学习"><a href="#什么是强化学习" class="headerlink" title="什么是强化学习"></a>什么是强化学习</h3><p>那什么是强化学习呢？这里推荐莫烦大神的 <a href="https://zhuanlan.zhihu.com/p/24807239" target="_blank" rel="external">什么是强化学习</a> 系列教程的知乎专栏，以及另一篇<a href="http://geniferology.blogspot.hk/2015/04/what-is-reinforcement-learning.html" target="_blank" rel="external">强化学习指南</a> 后者对强化学习的基本概念，实现方法进行全面的讲解，含有公式推导。</p>
<p>对于强化学习（Reinforcement learning），它是机器学习的一个分支，特别善於控制一只能够在某个环境下<strong>自主行动</strong>的个体 (autonomous <strong>agent</strong>)，透过和<strong>环境</strong>之间的互动，例如 sensory perception 和 rewards，而不断改进它的 <strong>行为</strong>。</p>
<p>比如，吃豆人游戏，自主行动的个体就是控制的吃豆人，环境就是迷宫，奖励就是吃到的豆子，行为就是上下左右的操作。</p>
<p>强化学习的输入是</p>
<ul>
<li><strong>状态</strong> (<u>S</u>tates) = 环境，例如迷宫的每一格是一个 state</li>
<li><strong>动作</strong> (<u>A</u>ctions) = 在每个状态下，有什么行动是容许的</li>
<li><strong>奖励</strong> (<u>R</u>ewards) = 进入每个状态时，能带来正面或负面的 <strong>价值</strong> (utility)</li>
</ul>
<p>输出是</p>
<ul>
<li><strong>方案</strong> (<u>P</u>olicy) = 在每个状态下，你会选择哪个行动？也是一个函数</li>
</ul>
<p>所以，我们需要根据S，A，R，来确定什么样的P是比较好的，通过不断的进行游戏，获得大量的交互数据，我们可以确定在每一个状态下，进行什么动作能获得最好的分数，而强化学习也就是利用神经网络来拟合这个过程。</p>
<p>例如，打砖块游戏有一个秘诀是把求打到墙后，这样球能自己反弹得分，强化学习程序在玩了600盘后，学到了这个秘诀，也就是说程序会在每一个状态下选择那个更容易把球打到墙后面去的操作。如下图，球快要把墙打穿的时候，评价函数 $v$ 的值会大幅度上升</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2017/05/27/AlphaGo运行原理解析/RL.png" alt="打墙游戏的评价函数图" title="">
                </div>
                <div class="image-caption">打墙游戏的评价函数图</div>
            </figure>
<p>我们可以发现，强化学习的基本思路和MCTS后异曲同工之妙，也是在对游戏完全没有了解的情况，通过不断的训练（进行多盘对弈，和获得进行行动后的分数反馈）来进行训练，和自我提升。</p>
<h3 id="利用强化学习增强棋力"><a href="#利用强化学习增强棋力" class="headerlink" title="利用强化学习增强棋力"></a>利用强化学习增强棋力</h3><p>参考这种思路，<code>Aja Huang</code>给围棋也设计了一个评价函数 $v(\vec s)$ 。此函数的功能是：<strong>量化评估围棋局面</strong>。使用$v(\vec s)$可以让我们在MCTS的过程中<strong>不走完全局</strong>就发现<strong>已经必败</strong>。</p>
<p>在利用<code>P_human</code>走了开局的20步后，<strong>如果有一个 $v(\vec s_i)$ （i为当前状态）可以直接判断是否能赢，得到最后的结果<code>r</code></strong>，不需要搜索到底，可以从效率（剪枝，优化算法时间复杂度）上进一步增加MCTS的威力。</p>
<p>很可惜的，现有的人类棋谱不足以得出这个评价函数。所以<code>Aja Huang</code>决定用<strong>机器和机器对弈</strong>的方法来创造新的对局，也就是AlphaGo的左右互搏。</p>
<h3 id="自对弈"><a href="#自对弈" class="headerlink" title="自对弈"></a>自对弈</h3><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2017/05/27/AlphaGo运行原理解析/RLArchitecture.png" alt="神经网络的训练过程和结构" title="">
                </div>
                <div class="image-caption">神经网络的训练过程和结构</div>
            </figure>
<ul>
<li>先用<code>P_human</code>和<code>P_human</code>对弈，比如1万盘，得到1万个新棋谱，加入到训练集中，训练出<code>P_human_1</code>。</li>
<li>使用<code>P_human_1</code>和<code>P_human_1</code>对弈，得到另1万个新棋谱，加入训练集，训练出<code>P_human_2</code>。</li>
<li>同理，进行多次的类似训练，训练出<code>P_human_n</code>，给最后的新策略命名为<code>P_human_plus</code></li>
</ul>
<p>（这里回顾一下<code>P_human</code>是什么，是一个函数，$\vec a=f(\vec s)$ 可以计算出当前 $\vec s$ 下的落子 $\vec a$ 的分布概率）</p>
<p>使用<code>P_human_plus</code>和<code>P_human</code>进行对弈，发现<code>P_human_plus</code>胜率80%，自对弈的方法被证明是有效的。（这里有一个想法，我在之前，一直加粗随机，之所以自对弈有效，就是因为整过MCTS过程中从来没有放弃过<strong>随机</strong>，如此一来，大量的计算，就更可能覆盖到更多的可能性，对提高棋力可以产生有效的作用，同时，因为概率的问题，不断的自我对弈肯定造成下棋的路数集中，后面也会有体现）</p>
<p>但是事实并没有那么美好，<code>Aja Huang</code>发现，使用<code>P_human_plus</code>来代替<code>P_human</code>进行MCTS反而<strong>棋力会下降</strong>。</p>
<p><code>Aja Huang</code>认为是<code>P_human_plus</code>走棋的路数太集中，而MCTS需要更加发散的选择才能有更好的效果。</p>
<h3 id="计算局部评价函数（Value-Network）"><a href="#计算局部评价函数（Value-Network）" class="headerlink" title="计算局部评价函数（Value Network）"></a>计算局部评价函数（Value Network）</h3><p>考虑到<code>P_human_plus</code>的下法太过集中，<code>Aja Huang</code>计算 $v(\vec s)$ 的策略是：</p>
<ul>
<li>开局先用<code>P_human</code>走<code>L</code>步，有利于生成更多局面</li>
<li>即使如此，<code>Aja Huang</code>还是觉得局面不够多样，为了进一步扩大搜索空间，在<code>L+1</code>步时，完全随机一个 $\vec a$ 落子，记下这个状态 $v(\vec s_{L+1})$ </li>
<li>之后使用<code>P_human_plus</code>来进行对弈，直到结束时获得结果<code>r</code>，如此不断对弈，由于<code>L</code>也是一个随机数，我们可以得到，<strong>开局、中盘、官子</strong>等不同阶段的很多局面 $\vec s$，和这些局面对应的结果<code>r</code></li>
<li><p>有了这些训练样本 $\rangle \vec s,r\langle$，还是使用<strong>神经网络</strong>，把最后一层改成<strong>回归</strong>而非<strong>分类</strong>（这里不是用的分类，而是用的<strong>回归，拟合</strong>），就得到了一个 $v(\vec s)$ 来输出<strong>赢棋的概率</strong></p>
<p><img src="/2017/05/27/AlphaGo运行原理解析/Value Network.png" alt="Value Network"> </p>
</li>
</ul>
<p>如上图所示，$v(\vec s)$ 可以给出下一步落在棋盘上任意位置后，如果双方都用<code>P_human_plus</code>来走棋，我方赢棋的概率。实验表明，仅仅使用<code>P_human</code>来训练$v(\vec s)$效果不如<code>P_human_plus</code>，强化学习是确实有效的。</p>
<p>总结，<strong>强化学习的<code>P_human_plus</code>主要是用来获得 $v(\vec s)$ 局部评估函数</strong>。表示的含义是</p>
<blockquote>
<p>在状态 $\vec s$ 下，<strong>局面的优劣程度，或者说此时的胜率是多少</strong></p>
</blockquote>
<p>走到这里，我们已经拥有<code>P_human</code>，MCTS，$v(\vec s)$，有了这些我们距离AlphaGo已经不远了</p>
<h2 id="AlphaGo"><a href="#AlphaGo" class="headerlink" title="AlphaGo"></a>AlphaGo</h2><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2017/05/27/AlphaGo运行原理解析/MCTS.png" alt="MTCS流程图解" title="">
                </div>
                <div class="image-caption">MTCS流程图解</div>
            </figure>
<p><code>Aja Huang</code>使用MCTS框架融合局面评估函数 $v(\vec s)$ 的策略是：</p>
<ul>
<li>使用<code>P_human</code>作为初始分开局，每局选择分数最高的方案落子</li>
<li>到第<code>L</code>步后，改用<code>P_human_fast</code>把剩下的棋局走完，同时调用 $v(\vec s_L)$，评估局面的获胜概率，按照如下规则更新整个树的分数</li>
</ul>
<p>$$<br>\text{新分数} = \text{调整后的初始分} + 0.5*\text{通过模拟得到的赢棋概率} + 0.5*\text{局面评估分} \tag {3-1}<br>$$</p>
<ul>
<li>前两项和原来一样<ul>
<li>如果待更新的节点就是叶子节点，局面评估分就是 $v(\vec s_L)$ </li>
<li>如果是待更新的节点是上级节点，局面评估分是该叶子节点 $v(\vec s)$ 的平均值</li>
</ul>
</li>
</ul>
<p>如果 $v(\vec s)$ 是表示大局观，<code>P_human_fast</code>表示快速演算，那么上面的方法就是二者的并重，并且<code>Aja Huang</code>团队已经用实验证明0.5 0.5的权重对阵其他权重有95%的胜率</p>
<h3 id="详解AlphaGo-VS-樊麾-对局走下某一步的计算过程"><a href="#详解AlphaGo-VS-樊麾-对局走下某一步的计算过程" class="headerlink" title="详解AlphaGo VS 樊麾 对局走下某一步的计算过程"></a>详解AlphaGo VS 樊麾 对局走下某一步的计算过程</h3><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2017/05/27/AlphaGo运行原理解析/Result1.png" alt="详解AlphaGo走某一步棋的过程1" title="">
                </div>
                <div class="image-caption">详解AlphaGo走某一步棋的过程1</div>
            </figure>
<p><code>a图</code>使用局部评估函数计算出 $\vec s$ 状态下其他落子点的胜率</p>
<p><code>b图</code>MCTS中使用局部评估函数加上<code>P_human</code>得出的结果（公式2-4）</p>
<p><code>c图</code>MCTS中使用<code>P_human</code>（复合算法）和<code>P_human_fast</code>走子走到底的结果</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2017/05/27/AlphaGo运行原理解析/Result2.png" alt="详解AlphaGo走某一步棋的过程2" title="">
                </div>
                <div class="image-caption">详解AlphaGo走某一步棋的过程2</div>
            </figure>
<p><code>d图</code>深度卷积神经网络使用<strong>策略函数</strong>计算出来的结果</p>
<p><code>e图</code>使用公式3-1和相关流程计算出的落子概率</p>
<p><code>f图</code>演示了AlphaGo和樊麾对弈的计算过程，AlphaGo执黑，樊麾执白。红圈是AlphaGo实际落子额地方。1，2，3和后面的数字表示他想象中的之后两房下一步落子的地方。白色方框是樊麾的实际落子。在复盘时，樊麾认为1的走法更好（这说明在樊麾落子后AlphaGo也在进行计算）</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>由于<strong>状态书有限</strong>和<strong>不存在随机性</strong>，象棋和五子棋这类游戏理论上可以由终局字底向上的推算出每一个局面的神符情况，从而得到最优策略。例如五子棋就被验证为<strong>先手必胜</strong>。</p>
<p>AlphaGo的核心思路来自于<strong>启发式搜索算法</strong></p>
<blockquote>
<p>由当前局面开始，尝试看起来<u>可靠的行动</u>，达到终局或一定步数后停止，根据后续<u>局面的优劣</u>反馈，选择最有行动。通俗来说，就是”手下一招子，心想三步棋“</p>
</blockquote>
<p>围棋是一个NP问题，要穷举的话，解空间是不可测度的。现代优化算法的经典之处就在于，对于围棋的规则来说，在某一个状态，必定有一个或几个较优解，整个AlphaGo就是想方设法的去找这个最优解，<strong>利用局面评估函数来对MCTS进行剪枝的思想十分精巧</strong>。利用上面的3个算法的组合，结合庞大的并行运算能力，还有<code>Aja Huang</code>团队的辛苦付出，造就了AlphaGo的奇迹。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="/2017/05/27/AlphaGo运行原理解析/Result.png" alt="最终棋力结果" title="">
                </div>
                <div class="image-caption">最终棋力结果</div>
            </figure>
<p>上图显示了各种算法的棋力，Rollout是走棋计算，也就是<code>P_human_fast</code>，Value Network是$v(\vec s)$ 局面评估函数，Policy Network 是结合<code>P_human_plus</code>和<code>P_human</code>后计算的胜率网络（下一步走在哪里胜率高的深度卷积神经网络）</p>
<p>整个AlphaGo使用的技术，深度卷积网络，强化学习神经网络，都是炙手可热的领域，近年来发展迅猛，日新月异。AlphaGo已经完成了自己历史使命，<strong>借助棋类的巅峰围棋为叩门砖打开了机器学习自我进化的大门</strong></p>
<h2 id="第四局的神之一手"><a href="#第四局的神之一手" class="headerlink" title="第四局的神之一手"></a>第四局的神之一手</h2><p>由上面的分析可以分析得到，这一手之后导致的状态 $\vec s$ 进入到了AlphaGo能处理的范围之外，或者说是在MCTS搜索树的空间之外，AlphaGo利用已有的数据没办法进行非常高效的计算，没有台下大量的数据学习，来建立一个基本覆盖所有状态 $\vec s$ 的局面评估函数是很重要的，另一方面，利用已有的数据来训练<code>P_human_plus</code>，使得这个函数越来越强大，才能覆盖到更多的情况。</p>
<p><strong>总结来说，AlphaGo依靠的是正式对局外的大量计算，无论是局部评估函数，还是<code>P_human_plus</code>都十分依赖大量的计算。</strong>很明显，随着时间的推移，AlphaGo在对局过程需要的时间越来越固定，不需要在对局时进行太多的MCTS搜索就能获得AlphaGo的下一手位置，可以预见，MCTS的搜索深度不会太深，当计算量十分庞大的时候，依赖更多是那个120层的Policy Network。</p>
<h2 id="柯洁面对的AlphaGo-2-0"><a href="#柯洁面对的AlphaGo-2-0" class="headerlink" title="柯洁面对的AlphaGo 2.0"></a>柯洁面对的AlphaGo 2.0</h2><p>这几天的三盘对局，感觉到AlphaGo在这一年内进行了极为深度的训练。</p>
<p><strong>局面函数</strong>和<strong>策略函数</strong>愈发强大，愈加的接近于”围棋之神“。</p>
<p>随着Google TPU的发布，跑在<code>TPU阵列</code>上的AlphaGo如虎添翼，MCTS的走子演算效率更高，速度更快</p>
<p>对于围棋这个策略单步游戏，是绝对存在<strong>N步最优解</strong>（不存在<code>i+1</code>步最优解），AlphaGo已经在正确的道路上无限的接近于这个<strong>N步最优解</strong>，放佛在某一步已经看到了你无论怎么下都能走到的N步最优解。</p>
<p>人类的每一次失误都会使局部评估函数往胜率移动一点，这一点是十分可怕的，因为算法本身的优越性，<strong>大局观</strong>对于AlphaGo的逻辑来说本身就是一种骨子的基因</p>
<ul>
<li>一是因为AlphaGo每次MCTS计算都会计算到接近分出胜负，具有<strong>前瞻性</strong></li>
<li>二是因为局面函数本身就是为了来统计大局形势定义的，具有<strong>判断局面优劣</strong>的能力</li>
</ul>
<p>所谓大局观，不就是这种<strong>走一步看N步的能力</strong>嘛。</p>
<h2 id="对未来的展望——从AlphaGo想开去"><a href="#对未来的展望——从AlphaGo想开去" class="headerlink" title="对未来的展望——从AlphaGo想开去"></a>对未来的展望——从AlphaGo想开去</h2><p>珍贵的并不是攻克了围棋问题本身，<strong>而是这种解决问题的基本模式</strong>，可以推而广之到很多领域。</p>
<p>先通过卷积网络学习人类的下法，算出策略函数（Policy Network），再通过<strong>模仿</strong>进行强化学习，左右互搏，不断自我进化，再加上MCTS的经典的解决问题的启发式搜索算法。</p>
<blockquote>
<p>这俨然是一个 <strong>模仿-&gt;学习-&gt;优化</strong>的过程</p>
</blockquote>
<p>或许，模仿人类，是机器学习最终的归途，至于应用领域方面</p>
<p>游戏AI是一个最容易想到的领域，只要能抽象出Action和State，那么这一套解决问题的方式就可以举一反三，让每一个1V1领域的游戏AI非常强大，至于合作领域的AI可能需要更大的计算量去计算，对于实际问题来说获得这样的AI有多大的经济价值值得推敲。</p>
<p>游戏的乐趣就在于<strong>不确定性</strong>，适当的失误也是竞技类游戏的魅力所在，一个能看到<strong>N步最优解</strong>的AI会让一个游戏机制，游戏规则变得可数据化，这一点其实是游戏被创造出来不愿意见到的。</p>
<p>其他方面，只要是人类可以学习出来的事物，比如翻译，编程，都是现在的这套体系可能解决的问题，我们期待未来这套<strong>解决问题的方法发挥出无穷的力量</strong>吧！</p>
<p>Reference </p>
<p>知乎Tao Lei大神的回答</p>
<p>知乎袁行远大声的回答</p>
<p>其他文章中引用的论文</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;问题分析&quot;&gt;&lt;a href=&quot;#问题分析&quot; class=&quot;headerlink&quot; title=&quot;问题分析&quot;&gt;&lt;/a&gt;问题分析&lt;/h1&gt;&lt;p&gt;围棋问题，棋盘 &lt;code&gt;19 * 19 = 361&lt;/code&gt; 个交叉点可供落子，每个点三种状态，白（用&lt;code&gt;1&lt;/code&gt;表示），黑（用&lt;code&gt;-1&lt;/code&gt;表示），无子（用&lt;code&gt;0&lt;/code&gt;表示），用 $\vec s$ &lt;strong&gt;描述&lt;/strong&gt;此时&lt;strong&gt;棋盘的状态&lt;/strong&gt;，即棋盘的&lt;strong&gt;状态向量&lt;/strong&gt;记为 $ \vec s$ （state首字母）。&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="/categories/Machine-Learning/"/>
    
    
      <category term="AlphaGo" scheme="/tags/AlphaGo/"/>
    
      <category term="CNN" scheme="/tags/CNN/"/>
    
      <category term="MCTS" scheme="/tags/MCTS/"/>
    
      <category term="Deep Learning" scheme="/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>清欢 - 林清玄</title>
    <link href="/2017/05/14/%E6%B8%85%E6%AC%A2/"/>
    <id>/2017/05/14/清欢/</id>
    <published>2017-05-14T16:26:55.000Z</published>
    <updated>2017-05-27T03:38:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1"><a href="#1" class="headerlink" title="1"></a>1</h2><p>少年时代读到苏轼的一阕词，非常喜欢，到现在还能背诵：</p>
<blockquote>
<p>细雨斜风作晓寒，淡烟疏柳媚晴滩，入淮清洛渐漫漫。<br> 雪沫乳花浮午盏，蓼茸蒿笋试春盘，人间有味是清欢。</p>
</blockquote>
<a id="more"></a>
<p>这阕词，苏轼在旁边写着“元丰七年十二月二十四日，从泗州刘倩叔游南山”，原来是苏轼和朋友到郊外去玩，在南山里喝了浮着雪沫乳花的淡茶，配着春日山野里的蓼菜、茼蒿、新笋，以及野草的嫩芽等等，然后自己赞叹着：“人间有味是清欢！”</p>
<p>当时所以能深记这阕词，最主要的是爱极了后面这一句，因为试吃野菜的这种平凡的清欢，才使人间更有滋味。<strong>“清欢”是什么呢？清欢几乎是难以翻译的，可以说是“清淡的欢愉”，这种清淡的欢愉不是来自别处，正是来自对平静疏淡简朴生活的一种热爱。当一个人可以品味出野菜的清香胜过了山珍海味，或者一个人在路边的石头里看出了比钻石更引人的滋味，或者一个人听林间鸟鸣的声音感受到比提笼遛鸟更感动，或者体会了静静品一壶乌龙茶比起在喧闹的晚宴中更能清洗心灵……这些就是“清欢”。</strong></p>
<p>清欢之所以好，是因为它对生活的无求，是它不讲求物质的条件，只讲究心灵的品味。“清欢”的境界很高，它不同于李白的<code>人生在世不称意，明朝散发弄扁舟</code>那样的自我放逐；或者<code>人生得意须尽欢，莫使金樽空对月</code>那种尽情的欢乐。它也不同于杜甫的<code>人生有情泪沾臆，江水江花岂终极</code>这样悲痛的心事，或者<code>人生不相见，动如参与商；今夕复何夕，共此灯烛光</code>那种无奈的感叹。</p>
<p>活在这个世界上，有千百种人生，文天祥的是<code>人生自古谁无死，留取丹心照汗青</code>，我们很容易体会到他的壮怀激烈。欧阳修的是<code>人生自是有情痴，此恨不关风与月</code>，我们很能体会到他的绵绵情恨。纳兰性德的是<code>人到情多情转薄，而今真个不多情</code>，我们也不难会意到他无奈的哀伤。甚至于像王国维的<code>人生只似风前絮，欢也零星，悲也零星，都作连江点点萍！</code>那种对人生无常所发出的刻骨的感触，也依然能够知悉。</p>
<h2 id="2"><a href="#2" class="headerlink" title="2"></a>2</h2><p>可是“清欢”就难了！</p>
<p>尤其是生活在现代的人，差不多是没有清欢的。</p>
<p>什么样是清欢呢？我们想在路边好好的散个步，可是人声车声不断的呼吼而过，一天里，几乎没有纯然安静的一刻。</p>
<p>我们到馆子里，想要吃一些清淡的小菜，几乎是<strong>杳不可得</strong>，过多的油、过多的酱、过多的盐和味精已经成为中国菜最大的特色，有时害怕了那样的油腻，特别嘱咐厨子白煮一个菜，菜端出来时让人吓一跳，因为菜上挤的色拉比菜还多。</p>
<p>有时没有什么事，心情上只适合和朋友去<strong>啜一盅茶</strong>、饮一杯咖啡，可惜的是，心情也有了，朋友也有了，就是找不到地方，有茶有咖啡的地方总是嘈杂的。</p>
<p>俗世里没有清欢了，那么到山里去吧！到海边去吧！但是，<strong>山边和海湄</strong>也不纯净了，凡是人的足迹可以到的地方，就有了垃圾，就有了臭秽，就有了吵闹！</p>
<p>有几个地方我以前常去的，像阳明山的白云山庄，叫一壶兰花茶，俯望着台北盆地里堆叠着的高楼与人欲，自己饮着茶，可以品到茶中有清欢。像在北投和阳明山间的山路边有一个小湖，湖畔有小贩卖工夫茶，小小的茶几、藤制的躺椅，独自开车去，走过石板的小路，叫一壶茶，在躺椅上静静的靠着，有时湖中的荷花开了，<strong>真是惊艳一山的沉默</strong>。有一次和朋友去，在躺椅上静静喝茶，一下午竟说不到几句话，那时我想，这大概是“人间有味是清欢”了。</p>
<p>现在这两个地方也不能去了，去了只有伤心。湖里的不是荷花了，是飘荡着的汽水罐子，池畔也无法静静躺着，因为人比草多，石板也被踏损了。到假日的时候，走路都很难不和别人推挤，更别说坐下来喝口茶，如果运气更坏，会遇到呼啸而过的飞车党，还有带伴唱机来跳舞的青年，那时所有的感官全部电路走火，不要说清欢，连欢也不剩了。</p>
<p>要找清欢，一日比一日更困难了。</p>
<p>当学生的时候，有一位朋友住在中和圆通寺的山下，我常常坐着颠踬的公交车去找她，两个人沿着上山的石阶，漫无速度的，走走、坐坐、停停、看看，那时圆通寺山道石阶的两旁，杂乱的长着朱槿花，我们一路走，顺手拈下一朵熟透的朱槿花，吸着花朵底部的花露，其甜如蜜，而清香胜蜜，轻轻的含着一朵花的滋味，<strong>心里遂有一种只有春天才会有的欢愉</strong>。</p>
<h2 id="3"><a href="#3" class="headerlink" title="3"></a>3</h2><p>圆通寺是一座全由坚固的石头砌成的寺院，那些黑而坚强的石头坐在山里仿佛一座不朽的城堡，绿树掩映，清风徐徐，站在用石板铺成的前院里，看着正在生长的小市镇，那时的寺院是澄明而安静的，让人感觉走了那样高的山路，能在那平台上看着远方，就是人生里的清欢了。</p>
<p>后来，朋友嫁人，到国外去了。我去过一趟圆通寺，山道已经开辟出来，车子可以环山而上，小山路已经很少人走，就在寺院的门口摆着满满的摊贩，有一摊是儿童乘坐的机器马，叽哩咕噜的童歌震撼半山，有两摊是打香肠的摊子，烤烘香肠的白烟正往那古寺的大佛飘去，有一位母亲因为不准孩子吃香肠而揍打着两个孩子，激烈的哭声尖亢而急促……我连圆通寺的寺门都没有进去，就沉默的转身离开，山还是原来的山，寺还是原来的寺，为什么感觉完全不同了，失去了什么吗？失去的正是清欢。</p>
<p>下山时的心情是不堪的，想到星散的朋友，心情也不是悲伤，只是惆怅，浮起的是一阕词和一首诗，词是李煜的：<code>高楼谁与上？长记秋晴望。往事已成空，还如一梦中！</code>诗是李觏的：<code>人言落日是天涯，望极天涯不见家；已恨碧山相阻隔，碧山还被暮云遮！</code>那时正是黄昏，在都市烟尘蒙蔽了的落日中，真的看到了一种悲剧似的橙色。</p>
<p>我二十岁心情很坏的时候，就跑到青年公园对面的骑马场去骑马，那些马虽然因驯服而动作缓慢，却都年轻高大，有着光滑的毛色。双腿用力一夹，它也会如箭一般呼噜向前窜去，急忙的风声就从两耳掠过，我最记得的是马跑的时候，迅速移动着的草的青色，青茸茸的，仿佛饱含生命的汁液，跑了几圈下来，一切恶的心情也就在风中、在绿草里、在马的呼啸中消散了。</p>
<p>尤其是冬日的早晨，勒着绳，马就立在当地，踢踏着长腿，鼻孔中冒着一缕缕的白气，那些气可以久久不散，当马的气息在空气中消弭的时候，人也好像得到某些舒放了。</p>
<p>骑完马，到青年公园去散步，走到成行的树荫下，冷而强悍的空气在林间流荡，可以放纵的、深深的呼吸，品味着空气里所含的元素，那元素不是别的，正是清欢。</p>
<h2 id="4"><a href="#4" class="headerlink" title="4"></a>4</h2><p>最近有一天，突然想到骑马，已经有十几年没骑了。到青年公园的骑马场时差一点吓昏，原来偌大的马场已经没有一根草了，一根草也没有的马场大概只有台湾才有，马跑起来的时候，灰尘滚滚，弥漫在空气里的尽是令人窒息的黄土，蒙蔽了人的眼睛。马也老了，毛色斑剥而失去光泽。</p>
<p>最可怕的是，不知道什么时候在马场搭了一个塑料棚子，铺了水泥地，其丑无比，里面则摆满了机器的小马，让人骑用，其吵无比。为什么为了些微的小利，而牺牲了这个马场呢？</p>
<p>马会老是我知道的事，人会转变是我知道的事，而在有真马的地方放机器马，在马跑的地方没有一株草，则是我不能理解的事。</p>
<p>就在马场对面的青年公园，已经不能说是公园了，人比西门町还拥挤吵闹，空气比咖啡馆还坏，树也萎了，草也黄了，阳光也不灿烂了。从公园穿越过去，想到少年时代的这个公园，心痛如绞，别说清欢了，简直像极了佛经所说的“<strong>五浊恶世</strong>”！</p>
<p>生在这个时代，为何“清欢”如此难觅。眼要清欢，找不到青山绿水；耳要清欢，找不到宁静和谐；鼻要清欢，找不到干净空气；舌要清欢，找不到蓼茸蒿笋；身要清欢，找不到清凉净土；意要清欢，找不到<strong>智慧明心</strong>。如果要享受清欢，唯一的方法是守在自己小小的天地，洗涤自己的心灵，因为在我们拥有愈多的物质世界，我们的清淡的欢愉就日渐失去了。</p>
<p>现代人的欢乐，是到<strong>油烟爆起、卫生堪虑</strong>的啤酒屋去吃炒蟋蟀；是到黑天暗地、不见天日的卡拉OK去乱唱一气；是到乡村野店、胡乱搭成的土鸡山庄去豪饮一番；以及到狭小的房间里做方城之戏，永远重复着摸牌的一个动作……这些放逸的生活以为是欢乐，想起来<strong>毋宁</strong>是可悲的。为什么现代人不能过清欢的生活，反而以浊为欢，以清为苦呢？</p>
<p>一个人以浊为欢的时候，就很难体会到生命清明的滋味，而在欢乐已尽、浊心再起的时候，人间就愈来愈无味了。</p>
<h2 id="5"><a href="#5" class="headerlink" title="5"></a>5</h2><p>这使我想起东坡的另一首诗来：</p>
<blockquote>
<p>梨花淡白柳深青，柳絮飞时花满城；<br> 惆怅东栏一株雪，人生看得几清明？</p>
</blockquote>
<p>苏轼凭着东栏看着栏杆外的梨花，满城都飞着柳絮时，梨花也开了遍地，东栏的那株梨花却从深青的柳树间伸了出来，仿佛雪一样的清丽，有一种惆怅之美，但是人生看这么清明可喜的梨花能有几回呢？这正是千古风流人物的性情，这正是清朝大画家盛大士在《溪山卧游录》中说的<code>凡人多熟一分世故，即多一分机智。多一分机智，即少却一分高雅。</code> 也有说<code>山中何所有？岭上多白云，只可自怡悦，不堪持赠君，自是第一流人物。</code></p>
<p>第一流人物是什么人物？</p>
<p>第一流人物是在<span style="color:red">清欢里也能体会人间有味</span>的人物！</p>
<p>第一流人物是<span style="color:red">在污浊滔滔的人间，也能找到清欢的人物！</span></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1&quot;&gt;&lt;a href=&quot;#1&quot; class=&quot;headerlink&quot; title=&quot;1&quot;&gt;&lt;/a&gt;1&lt;/h2&gt;&lt;p&gt;少年时代读到苏轼的一阕词，非常喜欢，到现在还能背诵：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;细雨斜风作晓寒，淡烟疏柳媚晴滩，入淮清洛渐漫漫。&lt;br&gt; 雪沫乳花浮午盏，蓼茸蒿笋试春盘，人间有味是清欢。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>English-abbreviation</title>
    <link href="/2017/05/13/English-abbreviation/"/>
    <id>/2017/05/13/English-abbreviation/</id>
    <published>2017-05-14T00:29:00.000Z</published>
    <updated>2017-05-26T05:35:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>一些常用的英语缩写的总结</p>
<a id="more"></a>
<h2 id="日常生活篇"><a href="#日常生活篇" class="headerlink" title="日常生活篇"></a>日常生活篇</h2><ol>
<li><code>R.S.V.P</code>: 源自于法语‘Répondez s’il vous plait’，英文解释为’Respond,if you please’.邀请函结尾写这个，表示‘敬请回复’；</li>
<li><code>P.S</code>: 意思是‘post script’,表示‘再多说一句’，一般写完要说的话之后结尾突然想起说什么可以写；</li>
<li><code>ASAP</code>: as soon as possible. 表示‘尽快’，注意听音频发音，可读成A-SAP;</li>
<li><code>ETA</code>: estimated time of arrival. 表示‘预计到达时间’；</li>
<li><code>BYOB</code>: bring your own bottle; 表示‘自带酒水，举办派对时常用’</li>
</ol>
<h2 id="吃饭做菜篇"><a href="#吃饭做菜篇" class="headerlink" title="吃饭做菜篇"></a>吃饭做菜篇</h2><ol>
<li><code>tsp or t</code> : teaspoon 一茶匙</li>
<li><code>tbs / tbsp/ T</code>: tablespoon 一汤匙</li>
<li><code>c</code>: cup 一杯</li>
<li><code>gal</code>: gallon 加仑</li>
<li><code>lb</code> : pound 磅</li>
<li><code>pt</code>：pint 品脱</li>
<li><code>qt</code>: quart 夸脱</li>
</ol>
<h2 id="出国地图篇"><a href="#出国地图篇" class="headerlink" title="出国地图篇"></a>出国地图篇</h2><ol>
<li><code>Ave</code>: avenue 大街</li>
<li><code>Blvd</code>: boulevard 大道</li>
<li><code>Ln</code>: lane 车道</li>
<li><code>Rd</code>: road 公路</li>
<li><code>St</code>: street 街道</li>
</ol>
<h2 id="教育工作篇"><a href="#教育工作篇" class="headerlink" title="教育工作篇"></a>教育工作篇</h2><ol>
<li><code>BA</code>: Bachelor of Arts 文学士</li>
<li><code>BS</code>: Bachelor of Science 理学士</li>
<li><code>MA</code>: Master of Arts 文科硕士</li>
<li><code>PA</code>: Personal Assistant 私人助理</li>
<li><code>VP</code>: Vice President 副总统;副总裁</li>
<li><code>CEO</code>: Chief Executive Officer 首席执行官</li>
<li><code>CFO</code>: Chief Financial Officer 首席财务官</li>
<li><code>COO</code>: Chief Operating Officer 首席运营官</li>
<li><code>CMO</code>: Chief Marketing Officer 首席营销官</li>
</ol>
<h2 id="社交聊天篇"><a href="#社交聊天篇" class="headerlink" title="社交聊天篇"></a>社交聊天篇</h2><ol>
<li><code>JK</code> :just kidding 跟你开玩笑呢</li>
<li><code>TBD</code>: to be determined 待定</li>
<li><code>AFAIK</code>: as far as I know 据我所知</li>
<li><code>BRB</code>: be right back 马上回来</li>
<li><code>CUL</code>: see you later 回见</li>
<li><code>TTYL</code>: talk to you later 回聊</li>
<li><code>CWYL</code>: chat with you later 回聊</li>
<li><code>LOL</code>: laugh out loud 哈哈</li>
<li><code>LMAO</code>: laugh my ass off 笑死我了</li>
<li><code>ROTFL/ ROFL</code>: rolling on the floor laughing 笑到在地上打滚</li>
<li><code>NP</code>: no problem 没问题,没关系,不客气</li>
<li><code>IDK</code>: I don’t know 我不知道</li>
<li><code>ILY</code>: I love you 我爱你</li>
<li><code>TMI</code>: too much information 信息量太大了； 说的太多了</li>
<li><code>OIC</code>: Oh, I see. 我明白了</li>
<li><code>FYI</code>: for your information 顺便告知你</li>
<li><code>BTW</code>: by the way 顺便说一下 顺便问一下</li>
<li><code>MYOB</code>: mind your own business 别多管闲事</li>
<li><code>FAQ</code>: frequently asked questions 经常被问的问题<br>20: <code>WTF</code>: what the fuck 搞毛阿…… 委婉的是WTH: what the hell/heck<br>21: <code>AKA</code>: also known as. 也叫做</li>
<li><code>TGIF</code>: thank god It’s Friday 谢天谢地又到礼拜五了</li>
<li><code>TBC</code>: to be continued; to be confirmed 未完待续/ 有待确认</li>
</ol>
<h2 id="数字字母篇"><a href="#数字字母篇" class="headerlink" title="数字字母篇"></a>数字字母篇</h2><p><code>2</code>: to/too<br><code>4</code>: for<br><code>B</code>: be<br><code>C</code>: see<br><code>I</code>: eye<br><code>O</code>: owe;<br><code>R</code>: are;<br><code>U</code>: you;<br><code>ur</code>: your/you’re<br><code>Y</code>: why</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一些常用的英语缩写的总结&lt;/p&gt;
    
    </summary>
    
    
      <category term="英语积累" scheme="/tags/%E8%8B%B1%E8%AF%AD%E7%A7%AF%E7%B4%AF/"/>
    
  </entry>
  
  <entry>
    <title>有关中国诗的那些事</title>
    <link href="/2017/05/13/%E6%9C%89%E5%85%B3%E4%B8%AD%E5%9B%BD%E8%AF%97%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B/"/>
    <id>/2017/05/13/有关中国诗的那些事/</id>
    <published>2017-05-13T23:40:00.000Z</published>
    <updated>2017-05-26T05:34:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>没有沉淀，文字永远上不了档次。难得空闲，读了些诗，有些感受。</p>
<h3 id="韦应物"><a href="#韦应物" class="headerlink" title="韦应物"></a>韦应物</h3><a id="more"></a>
<blockquote>
<p>独怜幽草涧边生，上有黄鹂深树鸣。春潮带雨晚来急，野渡无人舟自横。</p>
</blockquote>
<p>记起这《滁州西涧》，听过一个故事。话说一次国画比赛，题目是以<code>春潮带雨晚来急，野渡无人舟自横</code>这句诗作画。国学博大精深，国画作为其中一支配起诗来，别有遐想。此题甚好，不仅考及画技，更有对国学中诗词的体悟和见解。大家不妨也想想如果是你，你会怎么画？这里先卖个关子。</p>
<p>从诗的字面来说，是这样一种通感：春天近了，潮气依稀可嗅。但谁能像你这样，对一棵在水边生长的小草也充满爱怜？黄鹂在密林深处的低语你都能听到？这需要多么细腻的一颗心。华灯初上，渡口上已经没有人，舟独自横于水上，那是一种空阔的感觉。映照你一生步履，你的细腻出于岁月。你当年49岁，50载，可能不长，但是我知道你的与众不同，你的50载甚至顶得别人几辈子。</p>
<p>韦应物年少荒唐，并未认真读书。安史乱起，韦应物扈从不及，流落秦中。乱后，韦应物折节读书，痛改前非，从一个富贵无赖纨绔一变而为忠厚仁爱的儒者。有些官运，在地方（苏州）任官。韦应物勤于吏职，简政爱民，在苏州刺史届满之后，一贫如洗，寄居无定寺，客死他乡。 享年五十五岁。</p>
<p>别人些许看出的是你不在其位，不得其用的无奈，忧伤。但我看到的，更多是你的豁达，你心中总是美好多于忧伤。</p>
<p>通往远方的路，没有哪条是你不能走的；走在路上的人，没有谁是你不能结交的；结交的朋友，没有谁是你不能推心置腹的。虽然那个时代远没有现在的复杂，但是能捧出一颗完整的心也并不是一件容易的事。韦苏州，你是一个充满诗情的人。</p>
<p>回头看看开头提到的国画比赛优胜者的作品：弥蒙的雾气用模糊的淡墨衬托，远处的群山，夕阳露出半个头。远远的有几簇灯火，近处，一条小舟在几根芦苇中飘荡，船上有位着布衣的蓑翁，嘴里叼根芦苇，帽檐下压，不知是否在闭目养神，两只杜鹃立于船头。起初不懂，“无人”的野渡为何有人呢？其中深意，结合了韦苏州的履历才恍然大悟。</p>
<p>“无人”并不是一只孤舟。韦应物闲居，船上舟子，好似当时的韦应物，在船头打盹，闻着草香，听着鹂鸣。韦应物虽然赋闲苏州，但他并不排斥官场，若有机会，他还是会出仕，只是满足于闲暇。无奈忧伤可能有，但经历了顽劣，奋起，战乱，官场，贬谪，闲居的韦应物，更多的，是看破人生的豁达和满足。</p>
<h3 id="李白"><a href="#李白" class="headerlink" title="李白"></a>李白</h3><p>总觉中国诗总离不开一个“愁”字。思乡，思亲，忧国，羁旅等等，都和“愁”万缕千丝。我爱这些无奈，悲壮，不舍，甚至愤懑，嘲讽。他们仿佛缩影了人生，视角令人称奇，细腻的令你悸动。</p>
<p><strong>抽刀断水，是最无奈的神话；举杯消愁，是最动情的悲歌。</strong>李白潇洒一生，他豪放，甚至一直清贫，有了几个钱，就豪饮一番，将诗情挥洒，更是对“愁”下了如此入理的定义。</p>
<p><code>拣尽寒枝不肯栖，寂寞沙洲岭</code>李白就犹如谪仙，似乎从来没有受过来自这个世界的温暖。于是，在静夜里，李白写下了<code>床前明月光，疑是地上霜。举头望明月，低头思故乡</code>的千古“愁”词。可是李白的故乡在哪里呢？是陇西？是巴蜀？月华似霜的夜，浪迹天涯的游子李白在梦幻中寻觅故乡，但故乡却比梦幻更飘渺。</p>
<p>李白是复杂的，李白糅合着道家的“出世”和儒家的“入世”思想。所以，顺境时，他<code>仰天大笑出门去，我辈岂是蓬蒿人</code>的潇洒豪情；逆境时，他有<code>弃我去者昨日之日不可留，乱我心者今日之日多烦忧</code>的绵绵愁绪。   </p>
<h3 id="那些诗人"><a href="#那些诗人" class="headerlink" title="那些诗人"></a>那些诗人</h3><p>感动于张谓笔下早梅傲雪<code>不知近水花先发，疑是经冬雪未消</code>的玄妙；<br>陶醉于贺铸风中<code>一川烟草，满城风絮，梅子黄时雨</code>的飘愁；<br>哀婉于苏轼眼中<code>细看来，不是杨花，点点是，离人泪</code>的破碎。</p>
<p>说起苏东坡，一个传奇。</p>
<p><code>人生如梦</code>，东坡曾经迷惘过；<code>早生华发</code>，东坡曾经惋惜过；<code>十年生死两茫茫</code>，东坡曾经痛苦过。但他不屈，他平和，他豁达。</p>
<p><code>一蓑烟雨任平生</code>，他淡泊；<code>日啖荔枝三百颗，不辞长作岭南人</code>，他自定；<code>踏雪飞鸿</code>，他淡然。<code>问汝平生功业，黄州惠州儋州</code>，三贬之地，还恰恰就是他留下许多不朽之作的地方。</p>
<p>读着这些诗，深深思索，你会感到作为一个中国人学会了中文，有着五千年的浩瀚历史文化，是多么令你振奋和自豪；国学，遗留的东西，值得我们用一生去参悟。常说高考诗词理解令人头痛，如果怀着这样的心情读诗，你还会怕吗？</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;没有沉淀，文字永远上不了档次。难得空闲，读了些诗，有些感受。&lt;/p&gt;
&lt;h3 id=&quot;韦应物&quot;&gt;&lt;a href=&quot;#韦应物&quot; class=&quot;headerlink&quot; title=&quot;韦应物&quot;&gt;&lt;/a&gt;韦应物&lt;/h3&gt;
    
    </summary>
    
    
      <category term="随笔" scheme="/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
</feed>
