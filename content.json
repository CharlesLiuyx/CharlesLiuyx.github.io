[{"title":"深入浅出看懂AlphaGo元","date":"2017-10-19T03:54:32.000Z","path":"2017/10/18/深入浅出看懂AlphaGo元/","text":"【阅读时间】21min - 24min 9715【内容简介】AlphaGo1.0详解链接，这篇AlphaGo Zero论文原文超详细翻译，并且总结了AlphaGo Zero的算法核心思路，附带收集了网上的相关评论 在之前的详解：深入浅出看懂AlphaGo中，详细定义的DeepMind团队定义围棋问题的结构，并且深入解读了AlphaGo1.0每下一步都发生了什么事，就在最近，AlphaGo Zero横空出世。个人观点是，如果你看了之前的文章，你就会觉得这是一个水到渠成的事情 令，如果你只对这个事件感兴趣的，而不想了解论文和技术细节，链接奉上，欢迎跳过到最后评论和总结部分（但这部分网上的大牛太多了，知乎答案内最高票对结合围棋的分析很漂亮！建议阅读） 上限置信区间算法（UCT），一种博弈树搜索算法，是AlphaGo中一个重要组成部分：MCTS搜索算法中的核心 法国南巴黎大学的数学家西尔万·热利(SylvainGelly)与巴黎技术学校的王毅早(YizaoWang，音译)将UCT集成到一个他们称之为MoGo的程序中。该程序的胜率竟然比先前最先进的蒙特卡罗扩展算法几乎高出了一倍。 2007年春季，MoGo在大棋盘比赛中也击败了实力稍弱的业余棋手，充分展示了能力。科奇什（UCT算法发明者）预言，10年以后，计算机就能攻克最后的壁垒，终结人类职业棋手对围棋的统治。今年是2017年，AlphaGo系列横空出世。10年，总有着天才的人具有先知般的远见。详见UTC算法 【小发现】看完论文发现，这篇文章的接受时间是2017年4月7号，审核完成时间是2017年9月13号，而在乌镇对阵柯洁（2017年5月23号）用的可能是AlphaGo Master（这里没法证据来证明到底是AlphaGo Zero还是AlphaGo Master）。这个团队也是无情啊，人类再一次感觉被耍了，根据Elo得分，Deepmind团队可能在赛前就透露过吧，即使是Master也有4858分啊，对于一个棋手来说，我感受到的是风萧萧兮易水寒决绝的背影。为柯洁的勇气打Call，当真围棋第一人，天下无双 论文正文内容详细解析先上干货论文：Mastering the Game of Go without Human Knowledge ，之后会主要以翻译论文为主，在语言上尽量易懂，避免翻译腔 AlphaGo Zero，从本质上来说完全不同于打败樊麾和李世石的版本 算法上，自对弈强化学习，完全从随机落子开始，不用人类棋谱。之前使用了大量棋谱学习人类的下棋风格） 数据结构上，只有黑子白子两种状态。之前包含这个点的气等相关棋盘信息 模型上，使用一个神经网络。之前使用了策略网络（基于深度卷积神经网）学习人类的下棋风格，局面网络（基于左右互搏生成的棋谱，为什么这里需要使用左右互搏是因为现有的数据集不够，没法判断落子胜率这一更难的问题）来计算在当前局面下每一个不同落子的胜率 策略上，基于训练好的这个神经网，进行简单的树形搜索。之前会使用蒙特卡洛算法实时演算并且加权得出落子的位置 AlphaGo Zero 的强化学习问题描述在开始之前，必须再过一遍如何符号化的定义一个围棋问题 围棋问题，棋盘 19×19=361 个交叉点可供落子，每个点三种状态，白（用1表示），黑（用-1表示），无子（用0表示），用 $\\vec s$ 描述此时棋盘的状态，即棋盘的状态向量记为 $ \\vec s$ （state首字母）$$\\vec s = (\\underbrace{1,0,-1,\\ldots}_{\\text{361}})$$假设状态 $\\vec s$ 下，暂不考虑不能落子的情况， 那么下一步可走的位置空间也是361个。将下一步的落子行动也用一个361维的向量来表示，记为 $\\vec a$ （action首字母）$$\\vec a = (0,\\ldots,0,1,0,\\ldots)$$公式1.2 假设其中1在向量中位置为39，则 $\\vec a$ 表示在棋盘3行1列位置落白子，黑白交替进行 有以上定义，我们就把围棋问题转化为。 任意给定一个状态 $\\vec s$ ，寻找最优的应对策略 $\\vec a$ ，最终可以获得棋盘上的最大地盘 简而言之 看到 $\\vec s$ ，脑海中就是一个棋盘，上面有很多黑白子 看到 $\\vec a$ ，脑海中就想象一个人潇洒的落子 网络结构新的网络中，使用了一个参数为 $\\theta$ （需要通过训练来不断调整） 的深度神经网络$f_\\theta$ 【网络输入】19×19×170/1值：现在棋盘状态的 $\\vec s$ 以及7步历史落子记录。最后一个位置记录黑白，0白1黑，详见 【网络输出】两个输出：落子概率（362个输出值）和一个评估值（[-1,1]之间）记为 $f_{\\theta}(\\mathbf {\\vec s}) = (\\mathbf p,v)$ 【落子概率 $\\mathbf p$】 向量表示下一步在每一个可能位置落子的概率，又称先验概率 （加上不下的选择），即 $p_a = Pr(\\vec a|\\mathbf {\\vec s})$ （公式表示在当前输入条件下在每个可能点落子的概率） 【评估值 $v$】 表示现在准备下当前这步棋的选手在输入的这八步历史局面 $\\vec s$ 下的胜率（我这里强调局面是因为网络的输入其实包含历史对战过程） 【网络结构】基于Residual Network（大名鼎鼎ImageNet冠军ResNet）的卷积网络，包含20或40个Residual Block（残差模块），加入批量归一化Batch normalisation与非线性整流器rectifier non-linearities模块 改进的强化学习算法自对弈强化学习算法（什么是强化学习，非常建议先看看强化学习的一些基本思想和步骤，有利于理解下面策略、价值的概念，推荐系列笔记） 在每一个状态 $\\vec s$ ，利用深度神经网络 $f_\\theta$ 预测作为参照执行MCTS搜索（蒙特卡洛搜索树算法），MCTS搜索的输出是每一个状态下在不同位置对应的概率 $\\boldsymbol \\pi$ （注意这里是一个向量，里面的值是MCTS搜索得出的概率值），一种策略，从人类的眼光来看，就是看到现在局面，选择下在每个不同的落子的点的概率。如下面公式的例子，下在$(1,3)$位置的概率是0.92，有很高概率选这个点作为落子点 $$ \\boldsymbol \\pi_i = (\\underbrace{0.01,0.02,0.92,\\ldots}_{\\text{361}}) $$ MCTS搜索得出的落子概率比 $f_\\theta$ 输出的仅使用神经网络输出的落子概率 $\\mathbf p$ 更强，因此，MCTS可以被视为一个强力的策略改善（policy improvement）过程 使用基于MCTS提升后的策略（policy）来进行落子，然后用自对弈最终对局的胜者 $z$ 作为价值（Value），作为一个强力的策略评估（policy evaluation）过程 并用上述的规则，完成一个通用策略迭代算法去更新神经网络的参数 $\\theta$ ，使得神经网络输出的落子概率和评估值，即 $f_{\\theta}(\\mathbf {\\vec s}) = (\\mathbf p,v)$ 更加贴近能把这盘棋局赢下的落子方式（使用不断提升的MCST搜索落子策略$\\boldsymbol \\pi$ 和自对弈的胜者 $z$ 作为调整依据）。并且，在下轮迭代中使用新的参数来进行自对弈 在这里补充强化学习的通用策略迭代（Generalized Policy Iteration）方法 从策略 $\\pi_0$ 开始 策略评估（Policy Evaluation）- 得到策略 $\\pi_0$ 的价值 $v_{\\pi_0}$ （对于围棋问题，即这一步棋是好棋还是臭棋） 策略改善（Policy Improvement）- 根据价值 $v_{\\pi_0}$，优化策略为 $\\pi_{0+1}$ （即人类学习的过程，加强对棋局的判断能力，做出更好的判断） 迭代上面的步骤2和3，直到找到最优价值 $v_*$ ，可以得到最优策略 $\\pi_*$ 【 a图】表示自对弈过程 $s_1,\\ldots,s_T$。在每一个位置 $s_t$ ，使用最新的神经网络 $f_\\theta$ 执行一次MCTS搜索 $\\alpha_\\theta$ 。根据搜索得出的概率 $a_t \\sim \\boldsymbol \\pi_i$ 进行落子。终局 $s_T$ 时根据围棋规则计算胜者 $z$$\\pi_i$ 是每一步时执行MCTS搜索得出的结果（柱状图表示概率的高低） 【b图】表示更新神经网络参数过程。使用原始落子状态 $\\vec s_t$ 作为输入，得到此棋盘状态 $\\vec s_t$ 下下一步所有可能落子位置的概率分布 $\\mathbf p_t$ 和当前状态 $\\vec s_t$ 下选手的赢棋评估值 $v_t$ 以最大化 $\\mathbf p_t$ 与 $\\pi_t$ 相似度和最小化预测的胜者 $v_t$ 和局终胜者 $z$ 的误差来更新神经网络参数 $\\theta$ （详见公式1） ，更新参数 $\\theta$ ，下一轮迭代中使用新神经网络进行自我对弈 我们知道，最初的蒙特卡洛树搜索算法是使用随机来进行模拟，在AlphaGo1.0中使用局面函数辅助策略函数作为落子的参考进行模拟。在最新的模型中，蒙特卡洛搜索树使用神经网络 $f_\\theta$ 的输出来作为落子的参考（详见下图Figure 2） 每一条边 $(\\vec s,\\vec a)$ （每个状态下的落子选择）保存的是三个值：先验概率 $P(\\vec s,\\vec a)$，访问次数 $N(\\vec s,\\vec a)$，行动价值 $Q(\\vec s,\\vec a)$。 每次模拟（模拟一盘棋，直到分出胜负）从根状态开始，每次落子最大化上限置信区间 $Q(\\vec s,\\vec a) + U(\\vec s,\\vec a)$ 其中 $U(\\vec s,\\vec a) \\propto \\frac{P(\\vec s,\\vec a)}{1 + N(\\vec s,\\vec a)}$ 直到遇到叶子节点 $s’$ 叶子节点（终局）只会被产生一次用于产生先验概率和评估值，符号表示即 $f_\\theta(s’) = (P(s’,\\cdot), V(s’))$ 模拟过程中遍历每条边 $(\\vec s, \\vec a)$ 时更新记录的统计数据。访问次数加一 $N(\\vec s,\\vec a) += 1$；更新行动价值为整个模拟过程的平均值，即 $Q(\\vec s, \\vec a) = \\frac {1}{N(\\vec s, \\vec a)}\\Sigma_{\\vec s'|\\vec s, \\vec a \\Rightarrow \\vec s'}V(\\vec s')$ ，$\\vec s’|\\vec s, \\vec a \\Rightarrow \\vec s’$ 表示在模拟过程中从 $\\vec s$ 走到 $\\vec s’$的所有落子行动 $\\vec a$ 【a图】表示模拟过程中遍历时选 $Q+U$ 更大的作为落子点 【b图】叶子节点 $s_L$ 的扩展和评估。使用神经网络对状态 $s_L$ 进行评估，即 $f_\\theta(s_L) = (P(s_L,\\cdot), V(s_L))$ ，其中 $\\mathbf P$ 的值存储在叶子节点扩展的边中 【c图】更新行动价值 $Q$ 等于此时根状态 $\\vec s$ 所有子树评估值 $V$ 的平均值 【d图】当MCTS搜索完成后，返回这个状态 $\\vec s$ 下每一个位置的落子概率 $\\boldsymbol \\pi$，成比例于 $N^{1/\\tau}$（$N$为访问次数，$\\tau$ 为控温常数） 更加具体的详解见：搜索算法 MCTS搜索可以看成一个自对弈过程中决定每一步如何下的依据，根据神经网络的参数 $\\theta$ 和根的状态 $\\vec s$ 去计算每个状态下落子位置的先验概率，记为 $\\boldsymbol \\pi = \\alpha_\\theta(\\vec s)$ ，幂指数正比于访问次数 $\\pi_{\\vec a} \\propto N(\\vec s, \\vec a)^{1/\\tau}$，$\\tau$ 是温度常数 训练步骤总结使用MCTS下每一步棋，进行自对弈，强化学习算法（必须了解通用策略迭代的基本方法）的迭代过程中训练神经网络 神经网络参数随机初始化 $\\theta_0$ 每一轮迭代 $i \\geqslant 1$ ，都自对弈一盘（见Figure-1a） 第 $t$ 步：MCTS搜索 $\\boldsymbol \\pi_t = \\alpha \\theta_{i-1}(s_t)$ 使用前一次迭代的神经网络 $f_{\\theta_{i-1}}$，根据MCTS结构计算出的落子策略 $\\boldsymbol \\pi_t$ 的联合分布进行【采样】再落子 在 $T$ 步 ：双方都选择跳过；搜索时评估值低于投降线；棋盘无地落子。根据胜负得到奖励值Reward $r_T \\in \\{-1,+1\\}$。 MCTS搜索下至中盘的过程的每一个第 $t$ 步的数据存储为 $\\vec s_t,\\mathbf \\pi_t, z_t$ ，其中 $z_t = \\pm r_T$ 表示在第 $t$ 步时的胜者 同时，从上一步 $\\vec s$ 迭代时自对弈棋局过程中产生的数据 $(\\vec s, \\boldsymbol \\pi, z)$ （$\\vec s$ 为训练数据，$\\boldsymbol \\pi, z$ 为标签）中采样（这里的采样是指选Mini-Batch）来训练网络参数 $\\theta_i$， 神经网络 $f_{\\theta_i}(\\vec s) = (\\mathbf p, v)$以最大化 $\\mathbf p_t$ 与 $\\pi_t$ 相似度和最小化预测的胜者 $v_t$ 和局终胜者 $z$ 的误差来更新神经网络参数 $\\theta$ ，损失函数公式如下 $$ l = (z - v)^2 - \\boldsymbol {\\pi}^T \\log(\\mathbf p) + c \\Vert \\theta \\Vert ^2 \\tag 1 $$ 其中 $c$ 是L2正则化的系数 AlphaGo Zero训练过程中的经验最开始，使用完全的随机落子训练持续了大概3天。训练过程中，产生490万场自对弈，每次MCTS大约1600次模拟，每一步使用的时间0.4秒。使用了2048个位置的70万个Mini-Batches来进行训练。 训练结果如下，图3 【a图】表示随时间AlphaGo Zero棋力的增长情况，显示了每一个不同的棋手 $\\alpha_{\\theta_i}$ 在每一次强化学习迭代时的表现，可以看到，它的增长曲线非常平滑，没有很明显的震荡，稳定性很好 【b图】表示的是预测准确率基于不同迭代第$i$轮的 $f_{\\theta_i}$ 【c图】表示的MSE（平方误差） 在24小时的学习后，无人工因素的强化学习方案就打败了通过模仿人类棋谱的监督学习方法 为了分别评估结构和算法对结构的影响，得到了，下图4 【dual-res】表示 AlphaGo Zero（20个模块），策略头和评估值头由一个网络产生【sep-res】表示使用20个残差木块，策略头和评估值头被分成两个不同的网络【dual-conv】表示不用ResNet，使用12层卷积网，同时包括策略头和评估值头【sep-conv】表示 AlphaGo Lee（击败李世乭的）使用的网络结构，策略头和评估值头被分成两个不同的网络 头的概念详见网络结构 AlphaGo Zero学到的知识在训练过程中，AlphaGo Zero可以一步步的学习到一些特殊的围棋技巧（定式），如图5 中间的黑色横轴表示的是学习时间 【a图】对应的5张棋谱展现的是不同阶段AlphaGo Zero在自对弈过过程中展现出来的围棋定式上的新发现 【b图】展示在右星位上的定式下法的进化。可以看到训练到50小时，点三三出现了，但再往后训练，b图中的第五种定式高频率出现，在AlphGa Zero看来，这一种形式似乎更加强大 【c图】展现了前80手自对弈的棋谱伴随时间，明显有很大的提升，在第三幅图中，已经展现出了比较明显的围的倾向性 具体频率图见：出现频率随训练时间分布图 AlphaGo Zero的最终实力之后，最终的AlphaGo Zero 使用40个残差模块，训练接近40天。在训练过程中，产生了2900万盘的自对弈棋谱，使用了310万个Mini-Batches来训练神经网络，每一个Mini-Batch包含了2048个不同的状态。（覆盖的状态数是63亿（$10^{10}$），但和围棋的解空间 $2^{361} \\approx 10^{108}$ 相比真的很小，也从侧面反映出，围棋中大部分选择都是冗余的。在一个棋盘局面下，根据先验概率，估计只有15-20种下法是值得考虑的） 被评测不同版本使用计算力的情况，AlphaGo Zero和AlphaGo Master被部署到有4个TPUs的单机上运行（主要用于做模型的输出预测Inference和MCTS搜索），AlphaGo Fan（打败樊麾版本）和AlphaGo Lee（打败李世乭版本） 分布式部署到机器群里，总计有176GPUs和48GPUs（Goolge真有钱）。还加入了raw network，它是每一步的仅仅使用训练好的深度学习神经网的输出 $\\mathbf p_a$ 为依据选择最大概率点来落子，不使用MCTS搜索（Raw Network裸用深度神经网络的输出已经十分强大，甚至已经接近了AlphaGo Fan） 下图6展示不同种AlphaGo版本的棋力情况 【a图】随着训练时间棋力的增强曲线 【b图】裸神经网络得分3055，AlphaGo Zero得分5185，AlphaGo Master得分4858，AlphaGo Lee得分3738，AlphaGo Fan得分3144 最终，AlphaGo Zero 与 AlphaGo Master的对战比分为89：11，对局中限制一场比赛在2小时之内（新闻中的零封是对下赢李世乭的AlphaGo Lee） 论文附录内容我们知道，Nature上的文章一般都是很强的可读性和严谨性，每一篇文章的正文可能只有4-5页，但是附录一般会远长于正文。基本所有你的技术细节疑惑都可以在其中找到结果，这里值列举一些我自己比较感兴趣的点，如果你是专业人士，甚至想复现AlphaGo Zero，读原文更好更精确 围棋领域先验知识AlphaGo Zero最主要的贡献是证明了没有人类的先验知识机器也可以在性能上超越人类。为了阐释清楚这种贡献来自于何处，我们列举一些AlphaGo Zero使用到的知识，无论是训练过工程中的还是MCTS搜索中的。如果你想把AlphaGo Zero的思路应用的到解决其他游戏问题上，这些内容可能需要被替换 围棋基本规则无论实在MCTS搜索中的模拟还是自对弈的过程，都依赖游戏最终的胜负规则，并且在落子过程中，根据规则还可以排除一部分不可以落子的点（比如已经落子的点，无法确认在AlphaGo Zero还有气为零的点不能下这个规则，因为不记录气的信息了。但可以写一个函数来判断当前局面 $\\vec s$ 下下一步所有可能的落子点，不一定非得计算这个信息，这个过程可以完全多线程） Tromp-Taylor规则在AlphaGo Zero中使用的是PSK（Positional Superko）禁全同规则（中国，韩国及日本使用），只要这一手（不包括跳过）会导致再现之前的局面，就禁止。 旋转与镜面对于围棋来说，几个状态 $\\vec s$ 在经过旋转或反射后是完全相同的，这种规律可以用来优化训练数据和MCTS搜索中的子树替换策略。并且因为贴目（黑棋先下优势贴目7目半）规则存在，不同状态 $\\vec s$ 换颜色也是相同的。这个规则可以用来使用当前下子的棋手的角度来表示棋盘 除了以上的三个规则，AlphaGo Zero 没有使用其他任何先验知识，它仅仅使用深度神经网络对叶子节点进行评估并选择落子位置。它没有使用任何Rollout Policy(这里指的应该是AlphaGo之前版本的快速走子策略)或者树形规则，MCTS搜索也没有使用其他的标准启发式规则或者先验常识规则去进行增强 整个算法从随机初始化神经网络参数开始。网络结构和超参数选择 见下一节。MCTS搜索的超参数 $c_puct$ 由高斯过程优化决定，为了优化自对弈的性能，使用了一个神经网络进行预训练。对于一个大规模网络的训练过程（40个残差模块，40天），使用一个小规模网络（20个残差模块，3天）来反复优化MCTS搜索的超参数 $c_puct$。整个训练过程没有任何人工干预 自对弈训练工作流AlphaGo Zero的工作流由三个模块构成，可以异步多线程进行： 深度神经网络参数 $\\theta_i$ 根据自对弈数据持续优化 持续对棋手 $\\alpha_{\\theta_i}$ 棋力值进行评估 使用表现最好的 $\\alpha_{\\theta_*}$ 用来产生新的自对弈数据 优化参数每一个神经网络 $f_{\\theta_i}$ 在64个GPU工作节点和19个CPU参数服务器上进行优化。 每个工作节点的批次（Batch）大小是32，每一个mini-batch大小为2048。每一个 mini-batch 的数据从最近50万盘的自对弈棋谱的状态中联合随机采样。 神经网络权重更新使用带有动量（momentum）和学习率退火（learning rate annealing）的随机梯度下降法（SGD），损失函数见公式1 学习率退火比率见下表 步数（千） 强化学习率 监督学习率 0-200 $10^{-2}$ $10^{-1}$ 200-400 $10^{-2}$ $10^{-2}$ 400-600 $10^{-3}$ $10^{-3}$ 600-700 $10^{-4}$ $10^{-4}$ 700-800 $10^{-4}$ $10^{-5}$ &gt;800 $10^{-4}$ - 动量参数设置为0.9 方差项和交叉项的权重相同，原因是奖励值被归一化到 $r \\in {-1,+1}$ L2正则化系数设置为 $c = 10^{-4}$ 优化过程每1000个训练步数执行一次，并使用这个新模型来生成下一个Batch的自对弈棋谱 评估器为了保证生成数据的质量（不至于棋力反而下降），在使用新的神经网络去生成自对弈棋谱前，用现有的最好网络 $f_{\\theta_*}$ 来对它进行评估 【评估神经网络 $f_{\\theta_i}$ 的方法】使用 $f_{\\theta_i}$ 进行MCTS搜索得出的 $\\alpha_{\\theta_i}$ 的性能（得到 $\\alpha_{\\theta_i}$ 的MCTS搜索过程中使用 $f_{\\theta_i}$ 去估计叶子节点的位置和先验概率，详见MCTS搜索这一节） 每一个评估由400盘对局组成，MCTS搜索使用1600次模拟，将温度参数设为无穷小 $\\tau \\Rightarrow 0$（目的是为了使用最多访问次数的落子下法去下，追求最强的棋力），如果新的选手 $\\alpha_{\\theta_i}$ 在这400盘中胜率大于55%，将这个选手更新为最佳选手 $\\alpha_{\\theta_*}$ ，用来产生下一轮的自对弈棋谱，并且设为下一轮的比较对象 自对弈通过评估器，现在已经有一个当前的最好棋手 $\\alpha_{\\theta_*}$，使用它来产生数据。每次迭代中， $\\alpha_{\\theta_*}$ 自对弈25000盘，其中每一步MCTS搜索模拟1600次（模拟的每次落子大约0.4秒，这里的一次表示的就是MCTS搜索中走到叶子节点，得出胜负结果） 前30步，温度 $\\tau = 1$，与MCTS搜索中的访问次数成正比，目的是保证前30步下法的多样性。在之后的棋局中，温度设为无穷小。并在先验概率中加入狄利克雷噪声 $P(\\vec s, \\vec a) = (1 - \\epsilon) p_{\\vec a} + \\epsilon \\eta_{\\vec a}$ ，其中 $\\eta \\sim Dir(0.03)$ 且 $\\epsilon = 0.25$。这个噪声保证所有的落子可能都会被尝试，但也可能下出臭棋 投降阈值 $v_{rerign}$ 自动设为错误正类率（如果AlphaGo没有投降可以赢的比例）小于5%，为了测量错误正类(false positives)，在10%的自对弈中关闭投降机制，必须下完 监督学习为了进行对比，我们还使用监督学习训练了一个参数为 $\\theta_{SL}$ 神经网络。神经网络的结构和AlphaGo Zero相同。数据集 $(\\vec s, \\boldsymbol \\pi, z)$ 随机采样自KGS数据集，人类的落子策略位置即设置 $\\pi_a = 1$ 。使用同样的超参数和损失函数，但是平方误差的系数为0.01，学习率图参照上表的第二列。其他超参数和上一节相同 比AlphaGo1.0z中使用两种网络，使用这种结构的网络，可以有效的防止过拟合。并且实验也证明这个网络结构的的效果要好于之前的网络 MCTS搜索算法这一部分详解的AlphaGo Zero的算法核心示意图Figure2 AlphaGo Zero使用的是比AlphaGo1.0中更简单的异步策略价值MCTS搜索算法（APV-MCTS）的变种 搜索树中的节点 $\\vec s$ 包含一条边 $(\\vec s,\\vec a)$ 对应所有可能的落子 $\\vec a \\in \\mathcal A(\\vec s)$ ，每一条边中存储一个数据，包含下列公式的四个值$${N(\\vec s,\\vec a), W(\\vec s,\\vec a), Q(\\vec s,\\vec a), P(\\vec s,\\vec a)}$$ $N(\\vec s,\\vec a)$ 表示MCST搜索模拟走到叶子节点的过程中的访问次数$W(\\vec s,\\vec a)$ 表示行动价值（由路径上所有的 $v$ 组成）的总和$Q(\\vec s,\\vec a)$ 表示行动价值的均值$P(\\vec s,\\vec a)$ 表示选择这条边的先验概率（一个单独的值） 多线程（并行）执行多次模拟，每一次迭代过程先重复执行1600次Figure 2中的前3个步骤，计算出一个 $\\boldsymbol \\pi$ ，根据这个向量下现在的这一步棋 Selcet - Figure2aMCTS中的选择步骤和之前的版本相似，详见AlphaGo之前的详解文章，这篇博文详细通俗的解读了这个过程。概括来说，假设L步走到叶子节点，当走第 $t &lt; L$ 步时，根据搜索树的统计概率落子 $$ \\vec a_t = \\operatorname*{argmax}_{\\vec a}(Q(\\vec s_t, \\vec a) + U (\\vec s_t, \\vec a)) $$ 其中计算 $U (\\vec s_t, \\vec a)$ 使用PUCT算法的变体 $$ U(\\vec s, \\vec a) = c_{puct}P(\\vec s, \\vec a) \\frac{\\sqrt{\\Sigma_{\\vec b} N(\\vec s, \\vec b)}}{1 + N(\\vec s, \\vec a)} $$ 其中 $c_{puct}$ 是一个常数。这种搜索策略落子选择最开始更趋向于高先验概率和低访问次数的，但逐渐的会更加趋向于选择有着更高行动价值的落子 $c_{puct}$ 使用贝叶斯高斯过程优化来确定 Expand and evaluate - Figure 2b将叶子节点 $\\vec s_L$ 加到队列中等待输入至神经网络进行评估， $f_\\theta(d_i(\\vec s_L)) = (d_i(p), v)$ ，其中 $d_i$ 表示一个1至8的随机数来表示双方向镜面和旋转（从8个不同的方向进行评估，如下图所示，围棋棋型在很多情况如果从视觉角度来提取特征来说是同一个节点，极大的缩小了搜索空间） 队列中8个不同位置组成一个大小为8的mini-batch输入到神经网络中进行评估。整个MCTS搜索线程被锁死直到评估过程完成（这个锁死是保证并行运算间同步）。叶子节点被展开(Expand)，每一条边 $(\\vec s_L,\\vec a)$被初始化为 $$ {N(\\vec s_L,\\vec a) = 0 ;\\; W(\\vec s_L,\\vec a) = 0; \\; Q(\\vec s_L,\\vec a) = 0\\\\P(\\vec s_L,\\vec a) = p_a} $$ 这里的 $p_a$ 由将 $\\vec s$ 输入神经网络得出 $\\mathbf p$ （包括所有落子可能的概率值 $p_a$），然后将神经网络的输出值 $v$ 传回（backed up） Backup - Figure 2c沿着扩展到叶子节点的路线回溯将边的统计数据更新（如下列公式所示） $$ N(\\vec s_t, \\vec a_t) = N(\\vec s_t, \\vec a_t) + 1 \\\\ W(\\vec s_t, \\vec a_t) = W(\\vec s_t, \\vec a_t) + v \\\\ Q(\\vec s_t, \\vec a_t) = \\frac{W(\\vec s_t, \\vec a_t) }{N(\\vec s_t, \\vec a_t) } $$ 注解：在 $W(\\vec s_t, \\vec a_t)$ 的更新中，使用了神经网络的输出 $v$，而最后的价值就是策略评估中的这一状态的价值 $Q(\\vec s, \\vec a)$ 使用虚拟损失（virtual loss）确保每一个线程评估不同的节点 Play - Figure 2d完成MCTS搜索（并行重复1-3步1600次，花费0.4s）后，AlphaGo Zero才从 $\\vec s_0$ 状态下走出第一步 $\\vec a_0$，与访问次数成幂指数比例 $$ \\boldsymbol \\pi(\\vec a|\\vec s_0) = \\frac {N(\\vec s_0,a)^{1/\\tau}}{\\Sigma_{\\vec b} N(\\vec s_0, \\vec b)^{1/\\tau}} $$ 其中 $\\tau$ 是一个温度常数用来控制探索等级（level of exploration）。它是热力学玻尔兹曼分布的一种变形。温度较高的时候，分布更加均匀（走子多样性强）；温度降低的时候，分布更加尖锐（多样性弱，追求最强棋力） 搜索树会在接下来的自对弈走子中复用，如果孩子节点和落子的位置吻合，它就成为新的根节点，保留子树的所有统计数据，同时丢弃其他的树。如果根的评价值和它最好孩子的评价值都低于 $v_{resign}$ AlphaGo Zero就认输 MCTS搜索总结与之前的版本的MCTS相比，AlphaGo Zero最大的不同是没有使用走子网络（Rollout），而是使用一个整合的深度神经网络；叶子节点总会被扩展，而不是动态扩展；每一次MCTS搜索线程需要等待神经网络的评估，之前的版本性能评估（evaluate）和返回（backup）是异步的；没有树形策略 至于很重要的一个关键点：每一次模拟的中的叶子节点L的深度 【个人分析】是由时间来决定，根据论文提到的数据，0.4秒执行1600次模拟，多线程模拟，在时限内能走到的深度有多深就是这个叶子节点。可以类比为AlphaGo 1.0中的局面函数（用来判断某个局面下的胜率的），也就是说不用模拟到终盘，在叶子节点的状态下，使用深度神经网的输出 $v$ 来判断现在落子的棋手的胜率 网络结构网络输入数据输入数据的维度 19×19×17，其中存储的两个值0/1，$X_t^i = 1$表示这个交叉点有子，$0$ 表示这个交叉点没有子或是对手的子或 $t&lt;0$。使用 $Y_t$ 来记录对手的落子情况。 从状态 $\\vec s$ 开始，记录了倒退回去的15步，双方棋手交替。最后一个19×19的存储了前面16步每一个状态对应的棋子的黑白颜色。1黑0白 【个人理解】为了更加直观的解释，如果是上面的局部棋盘状态 $\\vec s$，接下里一步是黑棋落子，走了4步，那么输入数据是什么样的呢？ $$ \\mathbf X_2= \\begin{pmatrix} & \\vdots & \\vdots & \\\\ \\cdots & 0 & 1 & \\cdots \\\\ \\cdots & 1 & 0 & \\cdots \\\\ & \\vdots & \\vdots &\\end{pmatrix} \\mathbf Y_2= \\begin{pmatrix} & \\vdots & \\vdots & \\\\ \\cdots & 1 & 0 & \\cdots \\\\ \\cdots & 0 & 1 & \\cdots \\\\ & \\vdots & \\vdots &\\end{pmatrix} \\\\ \\mathbf X_1= \\begin{pmatrix} & \\vdots & \\vdots & \\\\ \\cdots & 0 & 0 & \\cdots \\\\ \\cdots & 1 & 0 & \\cdots \\\\ & \\vdots & \\vdots &\\end{pmatrix} \\mathbf Y_1= \\begin{pmatrix} & \\vdots & \\vdots & \\\\ \\cdots & 0 & 0 & \\cdots \\\\ \\cdots & 0 & 1 & \\cdots \\\\ & \\vdots & \\vdots &\\end{pmatrix} \\\\ \\mathbf C = \\begin{pmatrix} & \\vdots & \\vdots & \\\\ \\cdots & 1 & 0 & \\cdots \\\\ \\cdots & 0 & 1 & \\cdots \\\\ & \\vdots & \\vdots &\\end{pmatrix} $$ 同理，如果有8步的话，也就是16个对应的 $X$ 和 $Y$ 加一个 $C$ 来表示现在的棋盘状态（注意，这里面包含的历史状态）。这里的数据类型是Boolean，非常高效，并且表达的信息也足够 至于使用八步的原因。个人理解，一方面是为了避免循环劫，另一方面，选择八步也可能是性能和效果权衡的结果（从感知上来说当然信息记录的越多神经网络越强，奥卡姆剃刀定理告诉我们，简单即有效，一味的追求复杂，并不是解决问题的最佳途径） 深度神经网结构整个残差塔使用单独的卷机模块组成，其中包含了19或39个残差模块，详细结构参数如下图所示 还有一个分别的策略输出与评估值输出，详细结构参数如下图所示 数据集GoKifu数据集，和KGS数据集 图5更多细节 Figure 5a中每种定式出现的频率图 Figure 5b中每种定式出现的频率图 总结与随想AlphaGo Zero = 启发式搜索 + 强化学习 + 深度神经网络，你中有我，我中有你，互相对抗，不断自我进化。使用深度神经网络的训练作为策略改善，蒙特卡洛搜索树作为策略评价的强化学习算法 之后提出一些我在看论文时带着的问题，最后给出我仔细看完每一行论文后得出的回答，如有错误，请批评指正！ 问题与个人答案训练好的Alpha Zero在真实对弈时，在面对一个局面时如何决定下在哪个位置？评估器的落子过程即最终对弈时的落子过程（自对弈中的落子就是真实最终对局时的落子方式）：使用神经网络的输出 $\\mathbf p$ 作为先验概率进行MCTS搜索，每步1600次（最后应用的版本可能和每一步的给的时间有关）模拟，前30步采样落子，剩下棋局使用最多访问次数来落子，得到 $\\boldsymbol \\pi$ ，然后选择落子策略中最大的一个位置落子 AlphaGo Zero的MCTS搜索算法和和上个版本的有些什么区别？最原始MCTS解析，AlphaGo Lee加上策略函数和局面函数改进后的MCTS解析 对于AlphaGo Zero来说 最大的区别在于，模拟过程中依据神经网络的输出 $\\mathbf p$ 的概率分布采样落子。采样是关键词，首先采样保证一定的随机特性，不至于下的步数过于集中，其次，如果模拟的盘数足够多，那这一步就会越来越强 其次，在返回（Bakcup）部分每一个点的价值（得分），使用了神经网络的输出 $v$。这个值也是策略评估的重要依据 AlphaGo Zero 中的策略迭代法是如何工作的？策略迭代法（Policy Iteration）是强化学习中的一种算法，简单来说：以某种策略（ $\\pi_0$ ）开始，计算当前策略下的价值函数（ $v_{\\pi_0}$ ）；然后利用这个价值函数，找到更好的策略（Evaluate和Improve）；接下来再用这个更好的策略继续前行，更新价值函数……这样经过若干轮的计算，如果一切顺利，我们的策略会收敛到最优的策略（ $\\pi_*$ ），问题也就得到了解答。 $$ \\pi_0 \\xrightarrow{E} v_{\\pi_0} \\xrightarrow{I} \\pi_1 \\xrightarrow{E} v_{\\pi_1} \\xrightarrow{I} \\pi_2 \\xrightarrow{E} \\cdots \\xrightarrow{I} \\pi_* \\xrightarrow{E} v_* $$ 对于AlphaGo Zero来说，详细可见论文，简单总结如下 策略评估过程，即使用MCTS搜索每一次模拟的对局胜者，胜者的所有落子（$\\vec s$）都获得更好的评估值 策略提升过程，即使用MCTS搜索返回的更好策略 $\\boldsymbol \\pi$ 迭代过程，即神经网络输出 $\\mathbf p$ 和 $v$ 与策略评估和策略提升返回值的对抗（即神经网络的训练过程） 总的来说，有点像一个嵌套过程，MCST算法可以用来解决围棋问题，这个深度神经网络也可以用来解决围棋问题，而AlphaGo Zero将两者融合，你中有我，我中有你，不断对抗，不对自我进化 AlphaGo Zero 最精彩的部分哪部分？ $$ l = (z - v)^2 - \\boldsymbol {\\pi}^T \\log(\\mathbf p) + c \\Vert \\theta \\Vert ^2 $$ 毫无悬念的，我会选择这个漂亮的公式，看懂公式每一项的来历，即产生的过程，就读懂了AlphaGo Zero。这个公式你中有我，我中有你，这是一个完美的对抗，完美的自我进化 第二我觉得很精彩的点子是将深度神经网络作为一个模块嵌入到了强化学习的策略迭代法中。最关键的是，收敛速度快，效果好，解决各种复杂的局面（比如一个关于围棋棋盘的观看角度可以从八个方向来看的细节处理的很好，又如神经网络的输入状态选择了使用历史八步） 随想和评论量子位汇集各家评论 不是无监督学习，带有明显胜负规则的强化学习是强监督的范畴 无需担心快速的攻克其他领域，核心还是启发式搜索 模型简约漂亮，充满整合哲学的优雅，可怕的是效果和效率也同样极高 AlphaGo项目在经历了把书读厚的过程后，已经取得了瞩目的成就依旧不满足现状，现通过AlphaGo Zero把书读薄，简约而不简单，大道至简，九九归一，已然位列仙班了 随着AlphaGo Zero的归隐，DeepMind已经正式转移精力到其他的任务上了。期待这个天才的团队还能搞出什么大新闻！ 对于围棋这项运动的影响可能是：以后的学围棋手段会发生变化，毕竟世界上能复现AlphaGo Zero的绝对很多，那么AlphaGo Zero的实力那就是棋神的感觉，向AlphaGo Zero直接学习不是更加高效嘛？另，围棋受到的关注也应该涨了一波，是利好 感觉强化学习会越来越热，对于和环境交互这个领域，强化学习更加贴近于人类做决策的学习方式。个人预测，强化学习会在未来会有更多进展！AlphaGo Zero 可能仅仅是一个开头 以上！鞠躬！","tags":[{"name":"AlphaGo","slug":"AlphaGo","permalink":"https://charlesliuyx.github.io/tags/AlphaGo/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://charlesliuyx.github.io/tags/Deep-Learning/"}]},{"title":"【直观详解】线性代数中的转置正交正规正定","date":"2017-10-17T18:01:27.000Z","path":"2017/10/17/【直观详解】线性代数中的正交正规正定转置/","text":"【阅读时间】【内容简介】从【直观理解】线性代数的本质笔记出发，继续讨论几个线性代数中的概念，正交，正规，正定及转置的直观解释 在之前的【直观理解】线性代数的本质的笔记中，详细讨论了特征值与特征向量的几何直观意义 起初，研究线性代数，也是因为深入了解矩阵（变换）对机器学习中的很多优美公式的推导和理解有帮助。上篇笔记中，3B1B团队的讲解内容中没有涉及几个线性代数中的概念，且这些概念在做矩阵分解时会被用到。以上一篇笔记中的直观理解为基础（矩阵 = 变换）在这里做一个整理和记录 正交矩阵可能很多人已经有一个概念：正交（Orthogonal） = 垂直。但我们知道，正交的一定垂直，垂直的不一定正交（比如空间中两个不相交直线垂直）。提及垂直，首先出现你脑海中的特点是什么呢？我想是勾股数 $a^2 + b^2 = c^2$ ， 还有 $\\cos (\\frac {\\pi}{2}) = 1$ 那什么是正交矩阵呢？在讲这个概念之前，变换中有一种特殊变换：旋转变换。这种变换除了原点外没有特征向量，特征值恒为1，不对网格进行伸缩。 三维情况下，单位矩阵（对角线为1，其他为0，即基向量构成的矩阵）$\\mathbf E = \\left [ \\begin{smallmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0&amp;0&amp;1 \\end{smallmatrix} \\right ]$ 如下图所示 $\\mathbf E$ 中的三个基向量分别记为 $\\mathbf {X_a}= \\left [ \\begin{smallmatrix} 1 \\\\ 0 \\\\ 0 \\end{smallmatrix} \\right ] $ ，$\\mathbf {Y_a} = \\left [ \\begin{smallmatrix} 0 \\\\ 1 \\\\ 0 \\end{smallmatrix} \\right ] $，$\\mathbf {Z_a} = \\left [ \\begin{smallmatrix} 1 \\\\ 0 \\\\ 0 \\end{smallmatrix} \\right ] $ ，用下标a来表示。之后对这个矩阵 $\\mathbf E$ 应用一个旋转变换，以 $(0,-0.6,0.8)$ 为旋转轴，转90°。得到三个新的向量，用下标b来表示，记为 $\\mathbf {X_b}= \\left [ \\begin{smallmatrix} 0 \\\\ 0.8 \\\\ 0.6 \\end{smallmatrix} \\right ] $ ，$\\mathbf {Y_b} = \\left [ \\begin{smallmatrix} -0.8 \\\\ -0.36 \\\\ 0.48 \\end{smallmatrix} \\right ] $，$\\mathbf {Z_b} = \\left [ \\begin{smallmatrix} -0.6 \\\\ 0.48 \\\\ -0.64 \\end{smallmatrix} \\right ] $ 根据基变换原理，易得旋转变换的矩阵表达式 $\\mathbf R = \\left [ \\begin{smallmatrix} 0&amp;-0.8 &amp;-0.6 \\\\ 0.8&amp;-0.36&amp;0.48 \\\\ 0.6&amp;0.48&amp;-0.64 \\end{smallmatrix} \\right ]$ 计算得特征向量为","tags":[{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Linear Algebra","slug":"Linear-Algebra","permalink":"https://charlesliuyx.github.io/tags/Linear-Algebra/"}]},{"title":"程序员技能图谱","date":"2017-10-14T02:08:30.000Z","path":"2017/10/13/程序员技能图谱/","text":"【阅读时间】百科类【内容简介】原文来自StuQ的开源项目。核心目的是提纲挈领不同领域的技术栈。高层次来说，技术无止境，终身学习会越来越重要，一份由行内的资深人士总结的技术，工具和概念图谱有很强的指导作用，帮你节省时间。低层次来说，一份知识图谱也同领域聊天中装逼的神器，名词概念是有专业隔离的 记录工具使用幕布，方便一键生成思维导图，并且快速搜索查找 人工智能AI机器学习 大数据总 Hadoop Web 前端总 移动性能优化 HTML5开发 Angular 2 Server 后端架构师 OpenResty 直播技术 CDN技术 DNS排查 云计算总 容器 Container 微服务 MicroService 微服务架构和实践 智能运维总 DBA DevOps Kubernetes 安全总 测试总 移动无线测试 移动开发iOS开发 Android App 开发 Android ROM 开发 Android 架构师 智能硬件嵌入式开发 开发语言总 Golang Clojure Python Haskell Node.js Ruby Java 开发工具Git 技能清单CTO技能","tags":[{"name":"Acknowledge Graph","slug":"Acknowledge-Graph","permalink":"https://charlesliuyx.github.io/tags/Acknowledge-Graph/"},{"name":"Program","slug":"Program","permalink":"https://charlesliuyx.github.io/tags/Program/"},{"name":"IT","slug":"IT","permalink":"https://charlesliuyx.github.io/tags/IT/"}]},{"title":"【直观详解】线性代数的本质","date":"2017-10-07T05:56:56.000Z","path":"2017/10/06/【直观详解】线性代数的本质/","text":"【阅读时间】1小时左右【内容简介】将只停留在数值运算和公式的线性代数推进到可视化几何直观（Visual Geometric Intuition）的领悟上，致敬3B1B的系列视频的笔记，动图也都来自于视频。内容涉及到基变换，叉积，逆矩阵，点积，特征向量与特征值。每一章节都有一句经典的名言，非常有启发性 在笔记开始之前，想象学习一个事物（概念）的场景：我们需要学习正弦函数，$\\sin (x)$，非常不幸的是，你遇到了一本相当装逼的教材，它告诉你，正弦函数是这样的： $$ \\sin (x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} + \\cdots + (-1)^n\\frac{x^{2n+1}}{(2n+1)!} + \\cdots $$ 的确很厉害的样子，并且，计算器就是这样算 $\\sin (x)$，知道了这个的确“挺酷的”。对你来说，你的作业可能就是回家把 $x= \\frac{\\pi}{6}$ 带到公式里面，发现，好神奇！竟然越算越接近0.5。此时你对$\\sin (x)$与三角形之间的几何直观只有一些模糊的概念。这样的学习过程就十分悲催了。为什么呢？ 再假设一个场景，接下来，物理课，正弦函数随处可见，下图场景中，其他人能快速的大概估计出这个值是0.7。而刚“学过”正弦函数的你，内心戏可能是这样的：这些人忒diao了吧？你莫不是在玩我？ 你可能会觉得这些做物理的人脑子也太强了，我弱爆了。其实，你需要只是一个几何直观的灌输而已，这也从侧面佐证了一个好的老师（这里的好老师真的不是他本身的学术能力有多强，而在于他擅不擅长站在学习者的角度不断的修正教学方法。甚至，模拟学生的学习过程提前预知所需要的基础概念）是有多么重要。 教学不同层次的人：初学，入门，掌握，理解，都是不同的。解释的角度，方式都完全不同。更加不幸的是，为了能更加通用的用理论来描述现实生活中的规律，人类已经做了很多工作，我们常说：越通用，越抽象（越难以理解）。这对于初学者来说堪称一段噩梦 这个例子比较极端，但只为强调一件事：直观理解很重要，或者说，学习方法很重要。好的学习方法即你如何直观的去理解（可能是几何的，或是现实中的具体例子）一个抽象的事物，并层次化的建立知识与知识间的联系，构建并健壮属于自己的知识图谱。个人观点是，这种【学习方法】是最高效的。它唯一的难度在于，需要一定的基础知识打底，一定的量变结合方法论（点拨或领悟）就是质变。换句话说，想躺着学习？不存在的 根据生物学家我们知道，人对具体的事物（动画＞图形＞数字＞未建立直观理解的文字）更敏感，记忆速度更快。这篇笔记的对象3B1B团队生产的内容目的就是从为了帮助人们建立直观概念的角度来教学，在如今中国应试教育风行的大背景下，它会超越你的认知：学习如追番般期待，真不是一个调侃！ 我是极度反对现代大学的线性代数课程中（甚至数学类课程）的教学方法的，在计算上（做题）花费了大量时间。而工程中，有计算机，绝不会有任何一个人去笔算矩阵的逆或特征值。如果现在的老师反驳：做计算的目的是为了让你通过大量的联系（重复）去记牢概念，我也一直坚信：学习知识的最快捷径是带有思考的重复，但那是带思考的重复，有一些直观的方法在帮助你理解和记忆上比做题有效率的多 注解，因为这是一篇个人笔记，我个人已经深刻理解的内容，或者我觉得是很基本的内容我会略过或默认。好消息是，我自己也是一个理解力非常捉急的人，所以还是会比较详细的 什么是矩阵？矩阵 = 变换的数字表达 向量究竟是什么 引入一些数作为坐标是一种鲁莽的行为 ——赫尔曼·外尔 The introduction of numbers as coordinates is an act of violence - Hermann Weyl 这部分，讲向量，扎实的读者完全可以跳过 向量的定义 What对于向量的这个概念，大家一定并不陌生，但是这次让我们从数学，物理，计算机三个角度来看待如何定义这个【向量】这个概念 物理专业角度 向量是空间中的箭头 决定一个向量的是：它的长度和它所指的方向 计算机专业角度 向量是有序的数字列表 向量不过是“列表”一个花哨的说法 向量的维度等于“列表”的长度 数学专业角度从数学来说，它的本质就是通用和抽象，所以，数学家希望概括这两种观点 向量可以是任何东西，只需要保证：两个向量相加及数字与向量相乘是有意义的即可 向量加法和向量乘法贯穿线性代数始终，十分重要 可以通过上图直观的感受到数学家（这个很牛逼的灰色的$\\pi$）在想什么，有种【大道】的逼格。左边是物理角度，右边是计算机角度，但是很抱歉，我能用一些抽象的定义和约束让你们变成一个东西 坐标系把向量至于坐标系中，坐标正负表示方向，原点为起点，可完美把两个不同的角度融合 向量加法 物理：首尾相连 Motion 计算机：坐标相加 向量乘法 物理：缩放 Scaling 计算机：坐标和比例相乘 线性组合、张成的空间与基 数学需要的不是天赋，而是少量的自由想象，但想象太过自由又会陷入疯狂 ——安古斯·罗杰斯 Mathematics requires a small dose, not of genius, but of an imaginative freedom which, in a larger dose, would be insanity - Angus K. Rodgers 本部分继续加深一个概念，为何向量加法与向量乘法是那么重要，并从始至终贯穿整个线性代数 线性组合这个概念再好理解不过，空间中不共线的两个不为零向量都可以表示空间中的任意一个向量，写成符号语言就是：$a \\mathbf{\\vec v} + b \\mathbf{\\vec w}$ 至于为什么被称为“线性”，有一种几何直观：如果你固定其中一个标量，让另一个标量自由变化，所产生的向量终点会描出一条直线 空间的基 Basis对于我们常见的笛卡尔坐标系，有一个最直观一组基：$\\{{\\hat {\\imath}} ,{\\hat {\\jmath}} \\}$ ，即单位向量：${\\hat {\\imath}}=(1,0)$ 和$\\hat {\\jmath} =(0,1)$ ，通过 ${\\hat {\\imath}}$ 和 ${\\hat {\\jmath}}$的拉伸与相加可以组成笛卡尔坐标系中的任意一个向量 张成的空间 Span同理，举一反三的来说，我们可以选择不同的基向量，并且这些基向量构成的空间称为：张成的空间 所有可以表示为给定向量（基）线性组合（刚刚讲了这个概念）的向量的集合，被称为给定向量（基）张成的空间 如果你继续思考一下，会发现一个特点：并不是每一组给定向量都可以张成一个空间，若这两个向量共线（2D），共面（3D），它们就只能被限制在一个直线或面中，类似于“降维打击”。通过这个直观的思考可以引出一个概念：线性相关 线性相关关于什么是线性相关，有两种表达 【表达一】你有多个向量，并且可以移除其中一个而不减小张成的空间（即2D共线或3D共面），我们称它们（这些向量）线性相关 【表达二】其中一个向量，可以表示为其他向量的线性组合，因为这个向量已经落在其他向量张成的空间之中 如果从统计学角度来说，这些向量之中有冗余。这一堆向量中，我们只需要其中几个（取决于维度）就可以表示其他所有的向量。 向量空间一组基的严格定义有了这些对名次（概念）的直观理解，来看看数学家们是如何严谨的定义向量空间的一组基： 向量空间的一组基是张成该空间的一个线性无关向量集 用这样的步骤来慢慢导出这个定义，个人感觉，远比在课堂的第一分钟就将这句让你迷惑的话丢给你好的多，抽象的东西只有在慢慢推倒中你才能发现它的精巧之处，非常优雅且迷人 矩阵与线性变换 很遗憾，Matrix（矩阵）是什么是说不清的。你必须得自己亲眼看看 ——墨菲斯 Unfortunately, no one can be told what the Matrix is. You have to see it yourself -Morpheus 矩阵，最直观的理解当然是一个写成方阵的数字 $\\left[\\begin{smallmatrix} 1&2 \\\\ 3&4 \\end{smallmatrix}\\right]$，这几节的核心是为了说明：矩阵其实就是一种向量变换（至于什么是变换下面会讲），并附带一种不用死记硬背的考虑矩阵向量乘法的方法 变换【变换】本质上是【函数】（左）的一种花哨的说法，它接受输入内容，并输出对应结果，矩阵变换（右），同理，如下图 那既然两者意思相同，为何还要新发明一个词语装逼呢？其实不然，搞学术，不严谨就会出现纰漏。常说编程出现Bug，其实就是不严谨的一种体现，在写Code前，没有考虑到可能性的全集（虽然在一些大型程序中，考虑全集的做法有时候是没必要的，这是一对关于编程困难程度和不出的bug的博弈Trade-off），但是【变换】这个名词和不严谨其实没什么关系…… 【变换】的表达方法暗示了我们可以用运动的方法来理解【向量的函数】这一概念，可以用可视化的方法来展现这组【变换】即输入-输出关系 这世界上有非常多优美的变换，如果你将他们编程，并可视化，就能得到下图 线性变换我们说具有以下两个性质的就是线性变换（直观可视化如下图）： 直线在变换后仍然保持为直线，不能有所弯曲 原点必须保持固定 一点扩展，如果保持保持直线但原点改变就称为：仿射变换（Affine Transformation） 一句话总结来说是：线性变换是“保持网格线平行且等距分布”的变换 如何用数值描述线性变换这里需要使用上一节提到的工具，空间的基，也就是单位向量（基向量）：${\\hat {\\imath}}=(1,0)$ 和$\\hat {\\jmath} =(0,1)$ 你只需要关注两个基向量 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 变换后的位置即可。例如， $\\hat {\\imath}$ 变换到 $(3,1)$ 的位置， $\\hat {\\jmath}$ 变换到$(1,2)$ 的位置，并把 $\\hat {\\imath}$ 变换后坐标立起来作为方阵的第一列（绿色表示）， $\\hat {\\jmath}$ 变换后的坐标立起来作为方阵的第二列（红色表示）。 构成了一个矩阵：$\\begin{bmatrix} \\color{green}3&\\color{red}1 \\\\ \\color{green}1&\\color{red}2 \\end{bmatrix}$，假设我们想要知道目标向量$(-1,2)$进行变换后的位置，那么这个矩阵就是对变换过程最好的描述，一动图胜千言 Step1：绿色 $\\hat {\\imath}$ （x轴）进行移动（变换）Step2：红色 $\\hat {\\jmath}$ （y轴）进行移动（变换）Step3：目标向量x轴坐标值与 $\\hat {\\imath}$ 变换后向量进行向量乘法Step4：目标向量y轴坐标值与 $\\hat {\\jmath}$ 变换后向量进行向量乘法Step5：两者进行向量加法，得到线性变换结果 更加一般的情况，我们用变量来代替其中的具体值：绿色代表 $\\hat {\\imath}$ 变换后的向量，红色代表 $\\hat {\\jmath}$ 变换后的向量 $$ \\begin{bmatrix} \\color{green}{a}&\\color{red}b \\\\ \\color{green}c&\\color{red}d \\end{bmatrix} \\begin{bmatrix}x\\\\y\\end{bmatrix} = \\underbrace{x \\begin{bmatrix}\\color{green}a\\\\\\color{green}c \\end{bmatrix} + y \\begin{bmatrix} \\color{red}b\\\\\\color{red}d\\end{bmatrix}}_{\\text{直观的部分这里}} = \\begin{bmatrix} \\color{green}{a}\\color{black}{x}+\\color{red}{b}\\color{black}{y}\\\\\\color{green}{c}\\color{black}{x}+\\color{red}{d}\\color{black}{y}\\end{bmatrix} $$ 上面的公式就是我们常说的矩阵乘法公式，现在，不要强行背诵，结合可视化的直观动图，你一辈子都不会忘记的 【线性】的严格定义在给出一个数学化抽象的解释前，先做一下总结： 【线性变换】是操纵空间的一种手段，它保持网格线平行且等距分布，并保持原点不动 【矩阵】是描述这种变换的一组数字，或者说一种描述线性变换的语言 在数学上，【线性】的严格定义如下述公式，这些性质，会在之后进行讨论，也可以在这里就进行一些思考，为什么说向量加法和向量乘法贯穿线性代数始终，毕竟是线性代数，很重要的名次就是线性二字 $$ L(\\mathbf {\\vec v} + \\mathbf {\\vec w}) = L(\\mathbf{\\vec v}) +L(\\mathbf{\\vec w}) \\qquad “可加性” \\\\ L(c \\mathbf{\\vec v}) = c L(\\mathbf{\\vec v}) \\qquad “成比例” $$ 矩阵乘法与线性变换复合 据我的经验，如果丢掉矩阵的话，那些涉及矩阵的证明可以缩短一半 ——埃米尔·阿廷 It is my experience that proofs involving matrices can be shortened by 50% if one throws the matrices out -Emil Artin 复合变换如果对一个向量先进行一次旋转变换，再进行一次剪切变换（ $\\hat {\\imath}$ 保持不变第一列为（1,0）， $\\hat {\\jmath}$ 移动到坐标（1,1）） ，如下图所示 那么如果通过旋转矩阵和剪切矩阵来求得这个符合矩阵呢？为了解决这个问题，我们定义这个过程叫做矩阵的乘法 矩阵乘法在这里我们发现，矩阵乘法的变换顺序是从右往左读的（这一个常识很重要，你得明白这一点，有基本概念），进一步联系和思考发现，和复合函数的形式，如 $f(g(x))$ ，是一致的 那么如何求解矩阵乘法呢？对线性代数有印象的同学你们现在能马上记起来那个稍显复杂的公式吗？如果有些忘记了，那么，现在，就有一个一辈子也忘不了的直观解释方法： M1矩阵的第一列表示的是 $\\hat {\\imath}$ 变换的位置，先把它拿出来，M2矩阵看成对这个变换过的 $\\hat {\\imath}$ 进行一次变换（按照同样的规则），就如上图所示。同理，针对 $\\hat {\\jmath}$ 一样的操作过程，就可以得出这个表达式。这里我也不把它写出来了，按照这种思路，并且把上面的动图多看几遍，如果还能忘记，那就要去补一补基本的对几何图形的反应能力了（这也是一种能力，包括三维想象力，心算能力，都和记忆或肌肉一样，不锻炼，是不可能躺着被提高的） 计算规则证明有了上面的想法，可以自己尝试证明一下，矩阵乘法的交换律是否成立？矩阵乘法的结合律呢？你会发现，原来这么直观，根本不需要动笔算 三维空间对三维空间内扩展的话，你会发现，显示生活中的每一种形态改变都能用一个3*3的矩阵来表示这个变换，这在机器人，自动化操作领域是非常重要的，因为你可以把现实生活很难描述的动作通过一个矩阵来表示，是一个连接数字和现实的重要桥梁和工具 行列式 计算的目的不在于数字本身，而在于洞察其背后的意义 ——理查德·汉明（没错，是发明汉明码的那个人） The purpose of computation is insight, not numbers - Richard Hamming 行列式定义的由来我们注意到，有一些变换在结果上拉伸了整个网格，有一些则是压缩了，那如何度量这种压缩和拉伸呢？或者换一种更容易思考的表达，某一块面积的缩放比例是多少？ 其实，根据我们之前讲的基向量，我们只需要知道 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 组成的面积为1的正方形面积缩放了多少就代表所有的情况。因为线性变换有一个性质：网格线保持平行且等距分布 所以，这个特殊的缩放比例，即线性变换对面积产生改变的比例，就是行列式 特别的，我们可以发现，如果一个矩阵的行列式为0，意味着它把这个空间降维了，并且矩阵的列线性相关 其中，正负表达的是方向，类似于纸的翻面。从数学来说，$\\hat {\\jmath}$ 起始状态在 $\\hat {\\imath}$ 的左侧，如果经过变换，变为在右侧，就添加负号。三维情况下，右手定位为正 计算行列式为了连接行列式的计算公式和几何直观，我们现考虑 $\\begin{bmatrix} \\color{green}a&\\color{red}b \\\\ \\color{green}c&\\color{red}d \\end{bmatrix}$ 其中的b c 为0，那么，a表示 $\\hat {\\imath}$ 在x轴缩放比例，d表示 $\\hat {\\jmath}$ 在y轴缩放比例，ad表示拉伸倍数，同理来说，bc表示的就是压缩倍数，两者的和就是缩放比例。如果你还是对公式念念不忘，那么下面这张图可能可以帮到你 行列式直观理解的好处在这里，可以思考一下如何证明 $det(M_1M_2) = det(M_1)det(M_2)$ ，你会发现太简单不过了 逆矩阵、列空间与零空间 提出正确的问题比回答它更难 ——格奥尔格·康托尔 To ask the right question is harder than to answer it - Georg Cantor 首先，这一节并不涉及计算的方法，相关名次有：高斯消元法 Gaussian elimination、行阶梯形 Row echelon form。这里着眼的是对抽象的概念建立一个几何直观的理解，计算的任务就交给计算机去做 矩阵的用途 描述对空间的操作，3*3矩阵描述的三维变换 可以帮助我们解线性方程组（Linear System of equation） 线性方程组 上图就是一个整理好的线性方程组，一般形式 $\\mathbf A \\mathbf{\\vec x} = \\mathbf{\\vec v}$ ，其中 $\\mathbf{\\vec x}$ 是待求向量。使用之前的几何直观来翻译个公式即，$\\mathbf{\\vec x}$ 经过 $\\mathbf A$ 矩阵变换后，恰好落在 $\\mathbf{\\vec v}$ 上，如下图 既然使用了 $\\mathbf A$ 这个矩阵变换，那么之前讲解的概念：行列式应用在这里就很有意思了。根据之前提到的，行列式直观来说就是矩阵变换操作面积的缩放比例。我们可以思考，$det(\\mathbf A) = 0$ 意味着缩放比例为0，即降维了。很大可能找不到解，唯一的可能性，比如平面压缩成直线，这个直线恰好落在 $\\mathbf{\\vec v}$ 上才有解。这也是为什么计算行列式的值可以判断方程是否有解的几何直观 接下来思考如何求 $\\mathbf{\\vec x}$ 。逆向思考，从 $\\mathbf{\\vec v}$ 出发，进行某一个矩阵变换，恰好得到 $\\mathbf{\\vec x}$ 。而这个反过来的矩阵变换，就称为 $\\mathbf A$ 矩阵的逆矩阵，写成公式是：$$\\mathbf A^{-1}\\mathbf A\\mathbf{\\vec x} =\\mathbf A^{-1} \\mathbf{\\vec v} \\implies \\mathbf{\\vec x} =\\mathbf A^{-1} \\mathbf{\\vec v}$$ 逆矩阵所谓逆，就是反过来的意思。根据基向量代表整个空间，已经变换过的 $\\hat {\\imath}’$ 和 $\\hat {\\jmath}’$ 如何通过一个矩阵变换，变回 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ ，这个矩阵就是逆矩阵 ，写作 $\\mathbf A^{-1}$，直观理解如下图 逆矩阵乘原矩阵等于恒等变换，写作 $\\mathbf A \\mathbf A^{-1} = \\mathbf I$ 。$\\mathbf I$ 矩阵表示基向量，对角线元素为1，其余为0（矩阵说对角线，默认为左上方到右下方） 列空间其实这只是之前一直在提到过的概念，在线性方程组中，这么描述：所有可能得输出向量 $\\mathbf A \\mathbf{\\vec v}$ 构成的集合被称为A的列空间。这么说不太好理解，可以从名称“列空间”入手，矩阵的列是什么呢？我们之前已经多次强调了，就是 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 变换后的坐标。即矩阵的列表示基向量变换后的坐标（位置），变换后的向量张成的空间就是所有可能得输出向量 简单说即：列张成的空间 = 列空间，即矩阵的列所张成的空间，如下图。 检查一下自己是否完全理解，就思考下面句话为什么这么说：零向量一定在列空间中（列空间很好理解） 秩 Rank秩这个概念相信很多学习线性代数的同学，因为中文字，秩，本身就不熟（和正则化有点类似），所以秩也就非常难以理解了。秩是秩序，联想为秩序的程度。但是，因为你已经看了这个教程，矩阵的秩在现在你拥有的几何直观下，理解起来，当真的小菜一碟 我们已经建立了一种深刻的认知：矩阵 = 变换，那么变换后空间的维度，就是这个矩阵的秩。更加精确的定义是：列空间的维数（如果你可以焕然大：原来这两句话是一个意思，那么我觉得你对矩阵的理解已经有了质的提高） 零空间变换后落在原点的向量的集合，称为这个矩阵（再次强调矩阵 = 变换的数字表达）的零空间或核，如果感觉没理解，可以看看下图 【图1】二维压缩到一个直线（一维），有一条直线（一维）的点被压缩到原点【图2】三维压缩到一个面（二维），有一条直线（一维）的点被压缩到原点【图3】三维压缩到一条线（一维），有一条直线（二维）的点被压缩到原点 【注意】压缩就是变换，变换就是矩阵，其实说的就是矩阵 满秩 = 列空间 + 零空间 总结 从几何角度理解线性方程组的一个高水平概述 每个方程组都有一个线性变换与之联系，当逆变换存在时，你就能用这个逆变换求解方程组 不存在逆变换时，列空间的概念让我们清楚什么时候存在解 零空间的概念有助于我们理解所有可能得解的集合是什么样的 非方阵 在这个小测试里，我让你们求一个2*3矩阵的行列式。让我感到非常可笑的是，你们当中竟然有人尝试去做 ——佚名 On this quiz, I asked you to find the determinant of a 2*3 matrix. Some of you, to my great amusement, actually tried to do this - no name listed 几何意义首先从一个特例出发，考虑3×2（3行2列）矩阵的几何意义，从列空间我们得知，第一列表示的是 $\\hat {\\imath}$ 变换后的位置（现在是一个有三个坐标的值，即三维），第二列同理是 $\\hat {\\jmath}$ 。总结来说，3×2矩阵的几何意义是将二维空间映射到三维空间上 此时从特例到一般化推倒，我们可以得到一个结论：n*m 的几何意义是将m维空间（输入空间）映射到n维空间（输出空间）上 注意这里的输入空间，输出空间的概念，阅读方向同样也是从右向左的（靠右的是输入，靠左的是输出） 非方阵乘法如果你已经学过线性代数的大学课程，你可能有一些影响，并不是任意两个非方阵都可以进行矩阵乘法，必须满足一些条件，例如，$\\mathbf M_1 \\mathbf M_2$（非方阵）计算中，假设 $\\mathbf M_2$ 为2×3的矩阵，那么 $\\mathbf M_1$的列必须等于 $\\mathbf M_2$ 的行，否则这个乘法是没法计算的。 当我们有了变换的几何直观后，这个概念只要自己思考推倒一次，也是一辈子都忘不了的 直观解释是：矩阵的行是这个变换的输出空间维数，而列是变换的输入空间维数。矩阵乘法从右向左读，第一个变换的 $\\mathbf M_2$ 的输出向量的维度（ $\\mathbf M_2$ 的行）必须和第二个变换 $\\mathbf M_1$ 的输入向量（ $\\mathbf M_1$ 的列）维度相等，才可以计算。也就是说，类似于插头和插座的关系，我只有三头插座，你来一个双头插头肯定没法用的 非方阵行列式这里有一个很好玩的概念，非方阵的行列式呢？都不是一个维度的变换，如同归零者和咱们谈判一样，你和我谈缩放比例？不存在的 点积与对偶性 卡尔文：你知道吗，我觉数学不是一门科学，而是一种宗教霍布斯：一种宗教？卡尔文：是啊。这些公式就像奇迹一般。你取出两个数，把它们相加时，它们神奇地成为了一个全新的数！没人能说清这到底是怎么发生的。你要么完全相信，要么完全不信 什么是点积个相同维数的向量，或是两个相同长度的数组。求它们的点积，就是将相应坐标配对，求出每一对坐标的乘积，然后将结果相加，一动图胜千言 几何直观来说，$\\mathbf{\\vec v} \\cdot \\mathbf{\\vec w}$ 可以想象成向量 $\\mathbf{\\vec w}$ 朝着过原点和向量 $\\mathbf{\\vec v}$ 的直线上的正交（垂直）投影，然后把投影的长度和向量 $\\mathbf{\\vec v}$ 的长度乘起来就是点击的值。其中正负号代表方向，两个向量成锐角，大于0；钝角，小于0 点积的顺序你可能会发现，顺序在线性代数其实是很重要的，而对于 $\\mathbf{\\vec v} \\cdot \\mathbf{\\vec w}$ 和 $\\mathbf{\\vec w} \\cdot \\mathbf{\\vec v}$ 它们的结果是相同，为什么呢？ 解释的方法为：首先假设 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 长度相同，利用对称轴，两个向量互相的投影相等；接下来如果你缩放其中一个到原来的两倍，对称性被破坏，但是缩放比例没变，最终乘法的结果也没变，一动图胜千言 点积与投影这个时候问题就来了，这种直观的乘法与加法的组合运算：点积为何和投影长度的乘积有关？这个问题非常有意思，因为回答这个问题的过程用到了十分精彩的直觉和思维方式 首先，需要建立多维空间到一维空间的线性变换（描述为1×n的矩阵，列代表对应的基向量压缩到一维空间的位置），即函数（自变量对应多维空间，$f(x)$ 最后的输出为一维空间，也就是数轴上的点，一个确定的数）的概念。一动图胜千言 你会发现，n×1 表示的是坐标和1×n表示的多维到一维的变换（矩阵）之间有某种联系，即将向量转化为数的线性变换和这个向量本身有着某种关系 接下来，我们想象一个情景，这个被压缩成的一条线（数轴）放置在一个坐标系（二维空间）中，且空间所有向量都经过一个变换被压缩到这个数轴上。记这个数轴的单位向量为 $\\mathbf{\\vec u}$，一动图胜千言 再然后，我们需要考虑的问题变为，坐标系中的 $\\hat {\\imath}$ 与 $\\hat {\\jmath}$ 是如何被压缩到这条直线上的呢（基向量表征整个空间的变换）？即求一个1×2的矩阵内的值，第一列表示 $\\hat {\\imath}$ 变换后的位置（在这条数轴上），第二列表示 $\\hat {\\jmath}$ 变换后的位置。可以直接给出结论，这个变换的数值恰好就是 $\\mathbf{\\vec u}$ 在这个坐标系中的坐标 $(u_x,u_y)$ ，推倒方法使用到了对称性，一动图胜千言 动图中的白色虚线就是对称轴，目的就是确定变换后 $\\hat {\\imath}$ 与 $\\hat {\\jmath}$ 的位置，即描述变换的矩阵（再次重复，列表示坐标，行表示变换）。 推倒完毕，把这个过程总结成一个动图 矩阵的向量乘积和点积的计算公式一样，且恰好由压缩这一变换理念，与投影正好联系了起来。关键点，在于压缩变换 = 投影 对偶性在证明的过程中，有一个很关键的点就是使用了对称轴（对称理念）。在数学中，对偶性定义为：两种数学事物之间自然而又出乎意料的对应关系。刚刚推倒的内容是数学上“对偶性”的一个实例，即无论何时你看到一个二维到一维的变换，空间中会存在为一个向量 $\\mathbf{\\vec v}$ 与之相关 总结 点积是理解投影的有力几何工具 方便检验两个向量的指向是否相同 更进一步，两个向量点乘，就是将其中一个向量转化为线性变换 向量放佛是一个特定变换的概念性记号。对一般人类来说，想象空间中的向量比想象这个空间移动到数轴上更加容易 叉积 每一个维度都很特别 ——杰弗里·拉加里亚斯 从他（格罗滕迪克）和他的作为中，我还学到了一点：不以高难度的证明为傲，因为难度高意味着我们还不理解。理想的情况是能够绘出一幅美景，而其中的证明显而易见 二维情况下的叉积类比$\\mathbf{\\vec v}$ 与 $\\mathbf{\\vec w}$ 张成的平行四边形的面积，即 $\\mathbf{\\vec v} \\times \\mathbf{\\vec w}$ ，结果方向的确定考虑 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 的相对位置关系，与其相同，为正；否则，为负 通过这个定义，结合几何直观，我们可以发现几个有趣的结论 越接近垂直的 $\\mathbf{\\vec v}$ 与 $\\mathbf{\\vec w}$ 构成的面积越大 并且叉积的分配律成立 真正的叉积定义真正的叉积是在三维情况下被定义出来的：通过两个三维向量（$\\mathbf{\\vec v}$ 与 $\\mathbf{\\vec w}$ ）产生一个新的三维向量 $\\mathbf{\\vec p}$ ，向量 $\\mathbf{\\vec p}$ 的长度就是 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 组成平行四边形的面积 ，向量的方向与平行四边形（所在平面）垂直，并用右手定则确定方向，食指为 $\\mathbf{\\vec v}$ ，中指为 $\\mathbf{\\vec w}$ ，大拇指即 $\\mathbf{\\vec p}$ 叉积计算公式 其中 $\\hat {\\imath}$ $\\hat {\\jmath}$ $\\hat k$ 三个基向量后的数字就是对应向量 $\\mathbf{\\vec p}$ 的坐标值 第一次学这个计算方法的时候，估计没几个人能想清楚它为什么是这样的形式，甚至老师也说不清，只是告诉学生，我们这么记下来，定义是这样的定义的。但是，既然是直观讲解，必须把这里的来由探明清楚 叉积计算的几何直观在开始前，先再次加深一次对偶性的概念：每当你看到一个（多维）空间到数轴的线性变换时，它都与那个空间中的唯一一个向量对应。即应用线性变换到某个向量和与这个向量点乘等价 恰好，叉积的运算过程给出了对偶性的一个绝佳的实例：根据 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 定义一个从三维空间到数轴的特定线性变换，找到这个变换的对偶向量，这个对偶向量就是 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 的叉积 首先，我们知道三维情况的，求一个3×3矩阵的行列式，就是求这三个向量张成的平行六面体的体积，然后，把第一列（向量）换成一个自变量，后两列（两向量）记为 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ ，那么我们就有 这样形式的函数 $f()$ ，如右图所示，即平行六面体随白色向量 $(x,y,z)$ 的随机游走而不断改变。然后，问题就变成了，我们需要根据 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 找到一个变换（一个矩阵，或者说函数），使得上述等式成立。 并且因为 $f()$ 是线性的，可以利用对偶性，一动图胜千言 对偶性：即应用线性变换到某个向量和与这个向量点乘等价，即我们可以把1×3的变换（矩阵用来描述变换），立起来（转置），并写成点乘的形式。并把这个向量记为 $\\mathbf{\\vec p}$ 其中向量的颜色左右对应，并且行列式的值就是右图中平行四面体的体积，然后，我们就把问题进一步变成了：寻找向量 $\\mathbf{\\vec p}$ 使得上述等式成立 根据点积的性质得知，当你把一个向量与其他向量点积的几何解释是，把其他向量投影到 $\\mathbf{\\vec p}$ 上，然后将投影长度与 $\\mathbf{\\vec p}$ 的长度相乘。而我们知道，对于一个平行六面体来说，体积等于底面积乘以高，高于底面积垂直，所以，作为背投影对象的 $\\mathbf{\\vec p}$ 必须和 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 构成的平面垂直，方面已经找到 至于长度，可以看到，一个向量与其他向量点积的几何解释是，把其他向量投影到 $\\mathbf{\\vec p}$ 上，然后将投影长度与 $\\mathbf{\\vec p}$ 的长度相乘，其中投影长度就是 $(x,y,z)$ 向量的长度，根据公式的形式，可以观察得，向量 $\\mathbf{\\vec p}$ 的长度作为第二项，只有当长度等于平行四面体面积时，上述公式（图片中的点积=行列式值的公式）才能成立。 至此，又一次利用对偶性发现了一些事物之间自然而又出乎意料的对应关系。通过几何直观来了解计算公式的由来，也是一种加深印象，深刻理解的有效途径 总结在这里总结一下涉及到的过程，也可以通过阅读看看是否直观的理解每句话来判断掌握程度 首先定义了一个三维空间到数轴的线性变换（函数 $f()$ ），它是根据向量 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 来定义的 接着通过两种不同的方式来考虑这个变换的对偶向量 这种计算方法引导你在第一列中插入 $\\hat {\\imath}$ $\\hat {\\jmath}$ $\\hat k$ ，然后计算行列式 在几何直观上，这个对偶向量一定与 $\\mathbf{\\vec v}$ 和 $\\mathbf{\\vec w}$ 垂直，并且其长度与这两个向量张成的平行四边形的面积相同 基变换 数学是一门赋予不同事物相同名称的艺术 ——昂利·庞加莱 Mathematics is the art of givinh the same name to different things -Henri Poincare 坐标系与基向量坐标系指：发生在向量与一组数之间的任意转化，如果假设有一个向量，使用 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 来描述是 $[\\begin{smallmatrix} 3 \\\\ 2 \\end{smallmatrix}]$ ，我们把这种描述称为：我们的语言。如果有另一组基向量，$\\hat {\\imath}’ = [\\begin{smallmatrix} 2 \\\\ 1 \\end{smallmatrix}] $ 和 $\\hat {\\jmath}’ = [\\begin{smallmatrix} -1 \\\\ 1 \\end{smallmatrix}]$ （写成列向量的形式是为了形式上的统一）来描述同样一个向量变成 $[\\begin{smallmatrix} \\frac{5}{3} \\\\ \\frac{1}{3} \\end{smallmatrix}]$ ，我们把这种语言记为：詹妮弗的语言 基变换我们在之前的解释中已经说明了，在不同的【语言】之间的转化使用矩阵向量乘法，在上面的例子中，转移矩阵是 $\\mathbf T = [\\begin{smallmatrix} 2 &amp; -1 \\\\ 1 &amp; 1 \\end{smallmatrix}]$ ，矩阵的列表示用我们的语言表达詹妮弗的基向量，称为基变换。 反过来，就是求转移矩阵的逆 $\\mathbf T^{-1}$ ，称为基变换矩阵的逆，作用是可以表示从詹妮弗的基向量转换回我们的语言需要做的变换。 如何转化一个矩阵接下来使用一个具体的例子：变换左旋转90°，在我们的语言中，和詹妮弗的语言分别是如何互相转换的来加深印象 左乘基变换矩阵（矩阵的列代表的是用我们的语言描述詹妮弗语言的基向量）：需要被转换的詹妮弗的语言：$[\\begin{smallmatrix} -1 \\\\ 2 \\end{smallmatrix}]$ ➜ 使用我们的语言描述来描述同一个向量 左乘线性变换矩阵（表示的变化为：左旋转90°）：➜变换的后的向量（还是以我们的语言来描述） 左乘基变换矩阵的逆：➜变换后的向量（用詹妮弗的语言来描述） 这三个矩阵合起来就是用詹妮弗语言描述的一个线性变换 总结表达式 $\\mathbf A^{-1} \\mathbf M \\mathbf A$ 暗示着一种数学上的转移作用 中间的 $\\mathbf M$ 代表一种你所见的转换（例子中的90°旋转变换） 两侧的矩阵 $\\mathbf A$ 代表着转移作用（不同坐标系间的基向量转换），即就是视角上的转换 矩阵乘积仍然表示着同一个变换，只不过从其他人的角度来看 这给了很多域变换的应用一个直观的理解，把这简单的几行记录清晰， 特征向量与特征值在这一部分中，你会发现，前面提到的所有几何直观：线性变换，行列式，线性方程组，基变换会穿插其中。不仅给了你一个机会检验之前的理解是否深刻（在这一节，会添加一些超链接，方便你进行复习和定位），更多的，现在，是拼装起来感受成就感的时刻了！ What首先，我们假设坐标系的一个基变换（对 $\\hat {\\imath}$ 和 $\\hat {\\jmath}$ 张成的空间做一个线性变换 ），即 $\\hat {\\imath}’ = [\\begin{smallmatrix} 3\\\\ 0 \\end{smallmatrix}] $ 和 $\\hat {\\jmath}’ = [\\begin{smallmatrix} 1 \\\\ 2 \\end{smallmatrix}]$ 。在变换的过程中，空间内大部分的向量都离开了它所张成的空间（即这个向量原点到终点构成的直线） ，还有一部分向量留在了它所张成的空间，矩阵对它仅仅是拉伸或者压缩而已，如同一个标量。 如上图，是给出例子中，x轴所有向量被伸长为原来的3倍，一个明显留在张成空间内的例子。另一个比较隐藏的，是$(-1,1)$这个向量，其中的任意一个向量被伸长为原来的2倍 变换中被留在张成空间内的向量，就是特征向量（上例x轴和$(-1,1)$） 其中每个向量被拉伸或抽缩的比例因子，就是特征值（上例3和2） 正负表示变换的过程中是否切翻转了方向 Why三维情况，如果能找到这个不变的向量，即旋转轴（特征值必须为1） 理解线性变换的作用的关键（或者说更好的描述一个变换），更好的方法是求出它的特征向量和特征值 How从计算角度来看特征值和特征向量，里面包含了很多对以前只是回顾和整合 根据特征向量和特征值的定义，使用数学的方法来表示即$$\\mathbf A \\mathbf{\\vec v} = \\lambda \\mathbf{\\vec v}$$ $\\mathbf A$ 是求特征值和特征向量的变换矩阵；$\\mathbf{\\vec v}$ 是特征向量；$\\lambda$ 是特征值；目标是找 $\\mathbf{\\vec v}$ 和 $\\lambda$ 至于为何会用这个式子来定义特征向量和特征值呢，我们继续观察这个式子中的 $\\lambda \\mathbf{\\vec v}$ ，考虑到右边是一个矩阵乘法，我们希望左右都是一个矩阵乘法，这样方便等价和计算。观察发现，$\\lambda \\mathbf{\\vec v}$ 就是给 $\\mathbf{\\vec v}$ 中每一个元素都乘以 $\\lambda$ 。对角矩阵 $\\mathbf I$ 且对角线元素为 $\\lambda$ 的矩阵也能有同样的变换结果，得到下列表达式$$\\mathbf A \\mathbf{\\vec v} = (\\lambda \\mathbf I ) \\mathbf{\\vec v} \\implies (\\mathbf A - \\lambda \\mathbf I ) \\mathbf{\\vec v} = 0$$观察这个等式你会发现：可以把 $\\mathbf A - \\lambda \\mathbf I$ 矩阵看成一个对 $\\mathbf{\\vec v}$ 矩阵的变换，目的是把 $\\mathbf{\\vec v}$ 压缩到更低的维度。而空间压缩对应的恰好就是变换矩阵的行列式为0（期待你在品读这句话的时候感受到满满的成就感，实在有难度，再结合下图） 上图显示随 $\\lambda$ 可视化的变化情况，从这幅图中，使用的例子是 $[\\begin{smallmatrix} 2 &amp; 2 \\\\ 1 &amp; 3 \\end{smallmatrix}]$ ，特征值恰好是1 特征向量的特殊情况旋转变换解出特征值能发现答案是 $\\pm i$ ，没有特征向量存在，即特征值出现复数的情况一般对应于变换中的某种旋转 剪切变换Shear变换。x轴不变，只有一个特征值，为1（$(\\lambda-1)^2=0$） 伸缩变换特征值只有一个，但是是空间中所有的向量都是特征向量 特征基对角矩阵：只有对角线非零的矩阵。解读它的方法是：所有的基向量都是特征向量。因为之前提到过，矩阵的第一列是 $\\hat {\\imath}$ ，第二列是 $\\hat {\\jmath}$ ，往后同理。这样就能发现，如果一列只有对应的位置非零，那么这个坐标轴本身就就是特征向量 一组基向量（同样是特征向量）构成的集合被称为一组：特征基 对角矩阵有一个好处是计算方便，多次矩阵乘法非常容易 这时我们就希望利用对角矩阵（基向量为特征向量）的便于计算的特性，利用上一节提到的基向量变换的方法，把特征向量作为基，对每一个矩阵进行变换后再进行计算，最后再左乘变换矩阵的逆求回原矩阵得到结果，如下图所示 但需要说明的是，并不是所有的矩阵都能对角化，比如Shear变换，它的特征向量不够多，不足以张成一个空间 抽象向量空间线性代数的一切概念，如行列式和特征向量，它们并不受所选坐标系的影响，但是这两者是暗含于空间中的性质 这里所说的空间是什么意思呢？ 函数与向量从某种意义上来说，函数实际上也只是另一种向量，对于函数来说，也有可加性，可比性 $$ (f+g)(x) = f(x)+g(x)\\\\(2f)(x) = 2f(x) $$ 你能发现，这两个性质和向量加法与向量乘法是息息相关的。所以我们对于矩阵中所有定义的概念和方法，都可以相对应的应用到函数中。如函数的线性变换：函数接受一个函数，并把它变成另一个函数。如微积分中可以找到一个形象的例子——导数。关于这一点，你听到的可能是【算子】，而不是【变换】，但他们所要表达的思想是一样的 以导数为例，既然两者是一个东西，那么我们可不可以使用矩阵来描述多项式空间呢？ 如上图，以取 $x$ 的不同幂次方作为基函数，然后既可以写出求导变换的矩阵。这更进一步佐证了开篇提到的关键句子，矩阵 = 变换的数字表达 线性代数 函数 线性变换 线性算子 点积 内积 特征向量 特征函数 如上表一样，相同的概念只是在不同的领域有着不同的名称罢了。 有很多类似向量的不同事物，只要你处理的对象具有合理的数乘和相加的概念，线性代数中所有关于向量，线性变换和其他的概念都应该使用与它。作为数学家，你可能希望你发现的规律不只对一个特殊情况适用，对其他类似向量的事物都有普适性 向量空间这些类似向量的事物，比如箭头、一组数、函数等，他们构成的集合被称为：向量空间 向量加法和向量数乘的规则 - 被称为公理，如下图 它仅仅是一个待查列表，以保证向量加法和数乘的概念确实是你所希望的那样。这些公理是一种媒介，用来连接数学家和所有想要把这些结论应用于新的向量空间的人 仅仅根据这些公理描述一个空间，而不是集中于某一个特定的向量上。简而言之，这就是为什么你阅读的每一本教科书都会根据可加性和成比例来定义线性变换 总结对于【向量是什么】这个问题，数学家会直接忽略不作答。向量的形式并不重要，只要相加和数乘的概念遵守八条公理即可。就和问“3”究竟是什么一样。在数学中，他被看作是所有三个东西的集合的抽象概念，从而让你用一个概念就能推导出所有三个东西的集合。向量也是如此，它有很多种体现，但是数学把它抽象成【向量空间】这样一个无形（抽象）的概念 普适的代价是抽象（abstractness is the price of generality）。学习的过程只能来源于解决问题，来源于带有思考的不断重复，但如果你具备了正确的直观，你会再以后的学习中更加高效","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Linear Algebra","slug":"Linear-Algebra","permalink":"https://charlesliuyx.github.io/tags/Linear-Algebra/"}]},{"title":"【直观详解】什么是PCA、SVD","date":"2017-10-06T01:31:12.000Z","path":"2017/10/05/【直观详解】什么是PCA、SVD/","text":"【阅读时间】【内容简介】 在说明一个解释型内容的过程中，我一直坚信，带有思考的重复的是获取的知识的唯一捷径，所以会加入很多括号的内容，即另一种说法（从不同角度或其他称呼等），这样有助于理解。加粗的地方我也认为是比较重要的关键字或者逻辑推导，学习有一个途径就是划重点，做笔记。 What &amp; Why PCA（主成分分析）PCA，Principal components analyses，主成分分析。广泛应用于降维，有损数据压缩，特征提取和数据可视化。也被称为Karhunen-Loeve变换 从降维的方法角度来看，有两种PCA的定义方式，这里需要有一个直观的理解：什么是变换（线性代数基础），想整理一下自己线性代数的可以移步我的另一篇文章：【直观详解】线性代数的本质 但是总的来说，PCA的核心目的是寻找一个方向（找到这个方向意味着二维中的点可以被压缩到一条直线上，即降维），这个方向可以： 最大化正交投影后数据的方差（让数据在经过变换后更加分散） 紫色的直线 $u_1$ 即是关于 ${x_1,x_2}$ 二维的正交投影的对应一维表示PCA定义为使绿色点集的方差最小（方差是尽量让绿色所有点都聚在一坨）其中的蓝线是原始数据集（红点）到低纬度的距离，这可以引出第二种定义方式 最小化投影造成的损失（下图中所有红线（投影造成的损失）加起来最小） 投影造成的损失 PCA 主成分分析主要目的是为了减少数据维数，其中Auto-encoder也是一种精巧的降维手段 What &amp; Why SVD（奇异值分解）SVD，Singular Value Decomposition，奇异值分解。最直观的解释如下图所示 我们知道，矩阵描述的是一种变换（如果对这个概念有疑惑的，欢迎移步我的博客笔记：线性代数的本质）奇异值分解是矩阵分解的其中一种。换句话说，从上图的圆变换为右边的椭圆，通过一个 $\\mathbf M$ 矩阵就可以做到，但是，我们知道，非方阵是很不好处理的，我们希望，可以把 $\\mathbf M$ 矩阵表示的变换，分解为其他几种变换的组合（注意，分解之后，被分解的分量包含 $\\mathbf M$ 的信息，我们可以使用这些分量来进行操作），这几个变换我们希望是方阵，或者有特殊的性质。 $$ \\mathbf M = \\mathbf U \\cdot \\mathbf \\Sigma \\cdot \\mathbf V^* $$ $\\mathbf M$ 是一个m×n阶矩阵（输入为n维向量，输出为m维向量 $\\mathbf U$ 的列组成一套基向量，m×m阶矩阵，为$\\mathbf M \\mathbf M^*$ 的特征向量 $\\mathbf \\Sigma$ 对角矩阵，对角线上的值称为奇异值，可视为在输入与输出之间进行的标量的“伸缩尺度控制”。为 $\\mathbf M \\mathbf M^*$ 或 $\\mathbf M^* \\mathbf M$ 的非零特征值的平方根 $\\mathbf V^*$ 是 $\\mathbf V$ 的共轭转置（实数域即 $\\mathbf V^T$），n×n阶矩阵，$\\mathbf V$ 的列组成一套基向量，为 $\\mathbf M^* \\mathbf M$ 的特征向量 这里我们发现这个 $\\mathbf U$ 还有 $\\mathbf V$ 都是方阵，恰好满足之前的需求 且有 $\\mathbf U \\mathbf U^T = \\mathbf I_n$ 同时 $\\mathbf V \\mathbf V^T = \\mathbf I_m$ ，所以 $\\mathbf U$ 和 $\\mathbf V$ 是正交矩阵，而我们知道，正交矩阵对应的变换，就是旋转变换 对于 $\\mathbf \\Sigma$ 来说，我们知道特征值就是表示的度量伸缩程度的因子，即上图中的伸缩压缩程度（图中很直观的体现了这一点） 总结，SVD就是把一个非方阵（压缩变换）分解为一个旋转➜伸缩压缩➜旋转三个变换（矩阵），如上图所示 How PCA","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"}]},{"title":"【直观详解】什么是正则化","date":"2017-10-04T01:10:12.000Z","path":"2017/10/03/【直观详解】什么是正则化/","text":"【阅读时间】7min - 9min【内容简介】主要解决什么是正则化，为什么使用正则化，如何实现正则化，外加一些对范数的直观理解并进行知识整理以供查阅 Why &amp; What 正则化我们总会在各种地方遇到正则化这个看起来很难理解的名词，其实它并没有那么高冷，很好理解。 首先，从使用正则化的目的角度：正则化是为了防止过拟合 如上图，红色这条“想象力”过于丰富上下横跳的曲线就是过拟合情形。结合上图和正则化的英文 Regularizaiton-Regular-Regularize，直译应该是：规则化（加个“化”字变动词，自豪一下中文还是强）。什么是规则？你妈喊你6点前回家吃饭，这就是规则，一个限制。同理，在这里，规则化就是说给需要训练的目标函数加上一些规则（限制），让他们不要自我膨胀。正则化，看起来，挺不好理解的，追其根源，还是“正则”这两字在中文中实在没有一个直观的对应，如果能翻译成规则化，更好理解。但我们一定要明白，搞学术，概念名词的准确是十分重要，对于一个重要唯一确定的概念，为它安上一个不会产生歧义的名词是必须的，正则化的名称没毛病，只是从如何理解的角度，要灵活和类比。 我思考模式的中心有一个理念：每一个概念，被定义就是为了去解决一个实际问题（问Why&amp;What），接着寻找解决问题的方法（问How），这个“方法”在计算机领域被称为“算法”（非常多的人在研究）。我们无法真正衡量到底是提出问题重要，还是解决问题重要，但我们可以从不同的解决问题的角度来思考问题。一方面，重复以加深印象。另一方面，具有多角度的视野，能让我们获得更多的灵感，真正做到链接并健壮自己的知识图谱 How 线性模型角度对于线性模型来说，无论是Logistic Regression、SVM或是简单的线性模型，都有一个基函数 $\\phi()$，其中有很多 $\\mathbf w$ （参数）需要通过对损失函数 $E()$ 求极小值（或最大似然估计）来确定，求的过程，也就是使用训练集的训练过程：梯度下降到最小值点。最终，找到最合适的 $\\mathbf w$ 确定模型。从这个角度来看，正则化是怎么做的呢？ 二次正则项我们看一个线性的损失函数（真实值和预测值的误差） $$ E(\\mathbf w) =\\frac{1}{2} \\sum_{n=1}^{N}\\{t_n-\\mathbf w^T \\phi (\\mathbf x_n)\\}^2 \\tag{1} $$ $E(\\mathbf w)$ 是损失函数（又称误差函数），E即Evaluate，有时候写成L即Loss$t_n$ 是测试集的真实输出，又称目标变量【对应第一幅图中的蓝色点】$\\mathbf w$ 是权重（需要训练的部分，未知数）$\\phi()$ 是基函数，例如多项式函数，核函数测试样本有n个数据整个函数直观解释就是误差方差和，$\\frac{1}{2}$ 只是为了求导后消去方便计算 加正则化项，得到最终的误差函数（Error function） $$ \\frac{1}{2} \\sum_{n=1}^{N}\\{t_n-\\mathbf w^T \\phi (\\mathbf x_n)\\}^2 + \\frac{\\lambda}{2} \\mathbf w^T \\mathbf w \\tag{2} $$ (2)式被称为目标函数（评价函数）= 误差函数（损失函数） + 正则化项$\\lambda$ 被称为正则化系数，越大，这个限制越强 2式对 $\\mathbf w$ 求导，并令为0（使误差最小），可以解得 $$ \\mathbf w = (\\lambda \\mathbf I + \\Phi^T \\Phi)^{-1}\\Phi^T\\mathbf t $$ 这是最小二乘法的解形式，所以在题目中写的是从“最小二乘角度”。至于为何正则化项是 $\\frac{\\lambda}{2} \\mathbf w^T \\mathbf w$ 在之后马上解释 一般正则项直观的详解为什么要选择二次正则项。首先，需要从一般推特例，然后分析特例情况的互相优劣条件，可洞若观火。一般正则项是以下公式的形式 $$ \\frac{1}{2} \\sum_{n=1}^{N}\\{t_n-\\mathbf w^T \\phi (\\mathbf x_n)\\}^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{M} {\\vert w_j \\vert}^q \\tag{3} $$ M是模型的阶次（表现形式是数据的维度），比如M=2，就是一个平面（二维）内的点 若q=2就是二次正则项。高维度没有图像表征非常难以理解，那就使用二维作为特例来理解。这里令M=2，即 $\\mathbf x =\\{x_1,x_2\\} \\;\\mathbf w=\\{w_1,w_2\\}$ ，令q=0.5 q=1 q=2 q=4 有 横坐标是$w_1$纵坐标是$w_2$绿线是等高线的其中一条，换言之是一个俯视图，而z轴代表的是 $ \\frac{\\lambda}{2} \\sum_{j=1}^{M} {\\vert w_j \\vert}^q$ 的值 空间想象力不足无法理解的读者希望下方的三维图像能给你一个直观的领悟（与绿线图一一对应） q=2是一个圆非常好理解，考虑 $z = w_1^2 + w_2^2 $ 就是抛物面，俯视图是一个圆。其他几项同理（必须强调俯视图和等高线的概念，z轴表示的是正则项项的值） 蓝色的圆圈表示没有经过限制的损失函数在寻找最小值过程中，$\\mathbf w$的不断迭代（随最小二乘法，最终目的还是使损失函数最小）变化情况，表示的方法是等高线，z轴的值就是 $E(\\mathbf w)$$w^*$ 最小值取到的点 可以直观的理解为（帮助理解正则化），我们的目标函数（误差函数）就是求蓝圈+红圈的和的最小值（回想等高线的概念并参照3式），而这个值通在很多情况下是两个曲面相交的地方 可以看到二次正则项的优势，处处可导，方便计算，限制模型的复杂度，即 $\\mathbf w$ 中M的大小，M是模型的阶次，M越大意味着需要决定的权重越多，所以模型越复杂。在多项式模型多，直观理解是每一个不同幂次的 $x$ 前的系数，0（或很小的值）越多，模型越简单。这就从数学角度解释了，为什么正则化（规则化）可以限制模型的复杂度，进而避免过拟合 不知道有没有人发现一次正则项的优势，$w^*$ 的位置恰好是 $w_1=0$ 的位置，意味着从另一种角度来说，使用一次正则项可以降低维度（降低模型复杂度，防止过拟合）二次正则项也做到了这一点，但是一次正则项做的更加彻底，更稀疏。不幸的是，一次正则项有拐点，不是处处可微，给计算带来了难度，很多厉害的论文都是巧妙的使用了一次正则项写出来的，效果十分强大 How 神经网络模型角度我们已经知道，最简单的单层神经网，可以实现简单的线性模型。而多隐含层的神经网络模型如何来实现正则化？（毕竟神经网络模型没有目标函数） M表示单层神经网中隐含层中的神经元的数量 上图展示了神经网络模型过拟合的直观表示 我们可以通过一系列的推导得知，未来保持神经网络的一致性（即输出的值不能被尺缩变换，或平移变换），在线性模型中的加入正则项无法奏效 所以我们只能通过建立验证集（Validation Set），拉网搜索来确定M的取值（迭代停止的时间），又称为【提前停止】 这里有一个尾巴，即神经网络的不变量（invariance），我们并不希望加入正则项后出现不在掌控范围内的变化（即所谓图像还是那个图像，不能乱变）。而机器学习的其中一个核心目的也是去寻找不同事物（对象）的中包含信息的这个不变量（特征）。卷积神经网络从结构上恰恰实现了这种不变性，这也是它强大的一个原因 范数我并不是数学专业的学生，但是我发现在讲完线性模型角度后，有几个概念可以很轻松的解答，就在这里献丑把它们串联起来，并做一些总结以供查阅和对照。 我们知道，范数（norm）的概念来源于泛函分析与测度理论，wiki中的定义相当简单明了：范数是具有“长度”概念的函数，用于衡量一个矢量的大小（测量矢量的测度） 我们常说测度测度，测量长度，也就是为了表征这个长度。而如何表达“长度”这个概念也是不同的，也就对应了不同的范数，本质上说，还是观察问题的方式和角度不同，比如那个经典问题，为什么矩形的面积是长乘以宽？这背后的关键是欧式空间的平移不变性，换句话说，就是面积和长成正比，所以才有这个 没有测度论就没有（现代）概率论。而概率论也是整个机器学习学科的基石之一。测度就像尺子，由于测量对象不同，我们需要直尺量布匹、皮尺量身披、卷尺量房间、游标卡尺量工件等等。注意，“尺子”与刻度（寸、米等）是两回事，不能混淆。 范数分为向量范数（二维坐标系）和矩阵范数（多维空间，一般化表达），如果不希望太数学化的解释，那么可以直观的理解为：0-范数：向量中非零元素的数量；1-范数：向量的元素的绝对值；2-范数：是通常意义上的模（距离） 向量范数关于向量范数，先再把这个图放着，让大家体会到构建知识图谱并串联知识间的本质（根）联系的好处 p-范数 $$ \\Vert\\mathbf x \\Vert_p =(\\sum\\limits_{i=1}^{N}\\vert x_i \\vert^p)^{\\frac{1}{p}} $$ 向量元素绝对值的p次方和的 $\\frac{1}{p}$ 次幂。可以敏捷的发现，这个p和之前的q从是一个东西，随着p越大，等高线图越接近正方形（正无穷范数）；越小，曲线弯曲越接近原点（负无穷范数） 而之前已经说明，q的含义是一般化正则项的幂指数，也就是我们常说的2范数，两者在形式上是完全等同的。结合范数的定义，我们可以解释一般化正则项为一种对待求参数 $\\mathbf w$ 的测度，可以用来限制模型不至于过于复杂 $-\\infty$-范数 $$ \\Vert \\mathbf x \\Vert_{-\\infty} = arg \\operatorname*{min}_{i}{\\vert x_i \\vert} $$ 所有向量元素中绝对值的最小值 1-范数 $$ \\Vert \\mathbf x \\Vert_1 = \\sum\\limits_{i=1}^{N}\\vert x_i \\vert $$ 向量元素绝对值之和，也称街区距离（city-block） $$ \\begin{matrix} 4 & 3 & 2 & 3 & 4 \\\\ 3 & 2 & 1 & 2 & 3\\\\ 2 & 1 & 0 & 1 & 2 \\\\ 3 & 2 & 1 & 2 &3 \\\\ 4&3 & 2 &3 &4 \\\\ \\end{matrix} $$ 2-范数 $\\Vert\\mathbf x \\Vert_2 = \\sqrt{\\sum\\limits_{i=1}^{N} x_i^2}$ ：向量元素的平方和再开方。Euclid范数，也称欧几里得范数，欧氏距离 $$ \\begin{matrix} 2.8&2.2&2&2.2&2.8 \\\\ 2.2&1.4&1&1.4&2.2 \\\\ 2&1&0&1&2 \\\\ 2.2&1.4&1&1.4&2.2 \\\\ 2.8&2.2&2&2.2&2.8 \\\\ \\end{matrix} $$ $\\infty$-范数 $\\Vert \\mathbf x \\Vert_{\\infty} = arg \\operatorname*{max}_{i}{\\vert x_i \\vert}$ ：所有向量元素中绝对值的最大值，也称棋盘距离（chessboard），切比雪夫距离 $$ \\begin{matrix} 2 & 3 & 2 & 2 & 2 \\\\ 2 & 1 & 1 & 1 & 2\\\\ 2 & 1 & 0 & 1 & 2 \\\\ 2 & 1 & 1 & 1 &2 \\\\ 2&2 & 2 &2 &2 \\\\ \\end{matrix} $$ 矩阵范数1-范数 $$ \\Vert \\mathbf A \\Vert_{1} = arg \\operatorname*{max}_{1 \\leqslant j \\leqslant n}\\sum\\limits_{i=1}^m{\\vert a_{i,j} \\vert} $$ 列和范数，即所有矩阵列向量绝对值之和的最大值 $\\infty$-范数 $$ \\Vert \\mathbf A \\Vert_{\\infty} = arg \\operatorname*{max}_{1 \\leqslant i \\leqslant n}\\sum\\limits_{j=1}^m{\\vert a_{i,j} \\vert} $$ 行和范数，即所有矩阵行向量绝对值之和的最大值 2-范数 $\\Vert \\mathbf A \\Vert_{2} = \\sqrt{\\lambda_{max}(\\mathbf A^* \\mathbf A) }$ p=2且m=n方阵时，称为谱范数。矩阵 $\\mathbf A$ 的谱范数是 $\\mathbf A$ 最大的奇异值或半正定矩阵 $\\mathbf A^T \\mathbf A$ 的最大特征值的平方根 $\\mathbf A^*$ 为 $\\mathbf A$ 的共轭转置，实数域等同于 $\\mathbf A^T$ F-范数 $\\Vert \\mathbf A \\Vert_{F} = \\sqrt{ \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n \\vert a_{i,j}\\vert^2 }$ Frobenius范数（希尔伯特-施密特范数，这个称呼只在希尔伯特空间），即矩阵元素绝对值的平方和再开平方 核范数 $\\Vert \\mathbf A \\Vert_{*} = \\sum\\limits_{i=1}^n \\lambda_i$ ：$\\lambda_i$ 若 $\\mathbf A$ 矩阵是方阵，称为本征值。若不是方阵，称为奇异值，即奇异值/本征值之和 总结相信每个人在学习过程中都有过看书时，遇到0-范数正则化，或者1-范数正则化，2-范数正则化的表达时很迷惑。写到这里，希望大家能对这些看起来无法理解的晦涩名词有一个融会贯通的理解和感知！ Learning with intuitive and get Insight 以上！鞠躬！","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"}]},{"title":"Pandas-Wiki","date":"2017-09-30T17:44:25.000Z","path":"2017/09/30/Pandas-Wiki/","text":"【阅读时间】百科类【内容简介】pandas函数库相关操作手册，供查阅使用。笔记对象来自集智 必备库的导入 123import pandas as pdimport numpy as npimport matplotlib.pyplot as plt 创建对象创建一个Series对象：传递一个数值列表作为参数，令Pandas使用默认索引。 12345678910s = pd.Series([1,3,5,np.nan,6,8])print(s)&gt;0 1.01 3.02 5.03 NaN4 6.05 8.0dtype: float64 创建一个DataFrame对象：传递待datetime索引和列标签的Numpy数组作为参数。 123456789101112131415161718# 首先创建一个时间序列dates = pd.date_range('20130101', periods=6)print(dates)# 创建DataFrame对象，指定index和columns标签df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))print(df)&gt; DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D') A B C D2013-01-01 0.194508 -0.897507 0.224745 0.0902602013-01-02 2.412146 -1.191852 -1.644737 0.1909712013-01-03 -0.674645 0.395960 1.425822 -0.7182312013-01-04 0.349046 0.315026 2.058357 -0.8823452013-01-05 1.467093 0.146932 -0.680309 -0.5191552013-01-06 2.223284 -0.305247 -0.559043 1.017710 也可以传递词典来构建DataFrame，该词典的元素是形如Series的对象。 12345678910111213141516171819df2 = pd.DataFrame(&#123; 'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]), 'F' : 'foo' &#125;)print(df2)# 查看df2对象各列的数据类型print(df2.dtypes)&gt; A B C D E F0 1.0 2013-01-02 1.0 3 test foo1 1.0 2013-01-02 1.0 3 train foo2 1.0 2013-01-02 1.0 3 test foo3 1.0 2013-01-02 1.0 3 train fooA float64B datetime64[ns]C float32D int32E categoryF objectdtype: object 观察数据查看一个DataFrame对象的前几行和最后几行。 1234567891011121314print(df.head())print(df.tail(3))# 默认情况下，.head()和.tail()输出首尾的前5行，也可以手动指定输出行数。&gt; A B C D2013-01-01 0.194508 -0.897507 0.224745 0.0902602013-01-02 2.412146 -1.191852 -1.644737 0.1909712013-01-03 -0.674645 0.395960 1.425822 -0.7182312013-01-04 0.349046 0.315026 2.058357 -0.8823452013-01-05 1.467093 0.146932 -0.680309 -0.519155 A B C D2013-01-04 0.349046 0.315026 2.058357 -0.8823452013-01-05 1.467093 0.146932 -0.680309 -0.5191552013-01-06 2.223284 -0.305247 -0.559043 1.017710 查看索引(index)，列标签(columns)和Numpy数组形式的内容。 .describe()返回简约的统计信息，在工程中非常实用。 1234567891011121314151617181920212223242526272829303132# 索引print(df.index)&gt;DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04','2013-01-05', '2013-01-06'],dtype='datetime64[ns]', freq='D')# 列标签print(df.columns)&gt;Index(['A', 'B', 'C', 'D'], dtype='object')# 数值print(df.values)&gt;[[ 0.19450849 -0.89750748 0.22474509 0.09025968] [ 2.41214612 -1.19185153 -1.64473716 0.19097097] [-0.67464536 0.39595956 1.42582221 -0.71823105] [ 0.3490457 0.31502599 2.05835699 -0.88234524] [ 1.46709286 0.14693219 -0.68030862 -0.51915519] [ 2.2232837 -0.30524727 -0.55904285 1.01771006]]# 统计print(df.describe())&gt; A B C Dcount 6.000000 6.000000 6.000000 6.000000mean 0.995239 -0.256115 0.137473 -0.136798std 1.231715 0.663815 1.391936 0.711615min -0.674645 -1.191852 -1.644737 -0.88234525% 0.233143 -0.749442 -0.649992 -0.66846250% 0.908069 -0.079158 -0.167149 -0.21444875% 2.034236 0.273003 1.125553 0.165793max 2.412146 0.395960 2.058357 1.017710 灵活调教你的DataFrame：转置与排序 12345678# 转置print(df.T)# 按轴排序，逐列递减print(df.sort_index(axis=1, ascending=False))# 按值排序，'B'列逐行递增print(df.sort_values(by='B')) 选中尽管标准Python/Numpy的选中表达式很直观也很适合互动，但在生产代码中还是推荐使用Pandas里的方法如.at,.iat,.loc,.iloc,.ix等。 获取DataFrame里选中的一列与Series等效。 12345678910111213141516171819202122print(df[\"A\"]) # 与df.A相同&gt;2013-01-01 0.1945082013-01-02 2.4121462013-01-03 -0.6746452013-01-04 0.3490462013-01-05 1.4670932013-01-06 2.223284Freq: D, Name: A, dtype: float64# 用[]分割DataFrameprint(df[0:3])print(df['20130102':'20130104'])&gt; A B C D2013-01-01 0.194508 -0.897507 0.224745 0.0902602013-01-02 2.412146 -1.191852 -1.644737 0.1909712013-01-03 -0.674645 0.395960 1.425822 -0.718231 A B C D2013-01-02 2.412146 -1.191852 -1.644737 0.1909712013-01-03 -0.674645 0.395960 1.425822 -0.7182312013-01-04 0.349046 0.315026 2.058357 -0.882345 按标签选择12345678910111213141516171819202122232425262728293031323334# 选中一整行print(df.loc[dates[0]])&gt;A 0.194508B -0.897507C 0.224745D 0.090260Name: 2013-01-01 00:00:00, dtype: float64 # 按标签选中复数列（所有行，输出只显示前5行）print(df.loc[:,['A','B']])&gt; A B2013-01-01 0.194508 -0.8975072013-01-02 2.412146 -1.1918522013-01-03 -0.674645 0.3959602013-01-04 0.349046 0.3150262013-01-05 1.467093 0.1469322013-01-06 2.223284 -0.305247# 行/列同时划分（包括起止点）print(df.loc['20130102':'20130104',['A','B']])&gt; A B2013-01-02 2.412146 -1.1918522013-01-03 -0.674645 0.3959602013-01-04 0.349046 0.315026# 返回一个元素（两个方法等效）print(df.loc[dates[0],'A'])print(df.at[dates[0],'A'])&gt;0.1945084944020.194508494402 按位置选择12345678910111213141516171819202122232425# 位置索引为3的行（从0开始，所以其实是第4行）print(df.iloc[3])&gt;A 0.349046B 0.315026C 2.058357D -0.882345Name: 2013-01-04 00:00:00, dtype: float64 # 按位置索引分割DataFrameprint(df.iloc[3:5,0:2])print(df.iloc[[1,2,4],[0,2]])&gt; A B2013-01-04 0.349046 0.3150262013-01-05 1.467093 0.146932# 直接定位一个特定元素df.iloc[1,1]df.iat[1,1]&gt; A C2013-01-02 2.412146 -1.6447372013-01-03 -0.674645 1.4258222013-01-05 1.467093 -0.680309 布尔值索引123456789101112131415161718192021# 用一列的值来选择数据print(df.A &gt; 0)&gt;2013-01-01 True2013-01-02 True2013-01-03 False2013-01-04 True2013-01-05 True2013-01-06 TrueFreq: D, Name: A, dtype: bool # 使用.isin()函数过滤数据df2 = df.copy()df2['E'] = ['one', 'one','two','three','four','three']# 提取df2中'E'值属于['two', 'four']的行print(df2[df2['E'].isin(['two','four'])])&gt; A B C D E2013-01-03 -0.674645 0.395960 1.425822 -0.718231 two2013-01-05 1.467093 0.146932 -0.680309 -0.519155 four 设置赋值 1234567891011121314151617# 为DataFrame创建一个新的列，其值为时间顺序（与df相同）的索引值s1 = pd.Series([1,2,3,4,5,6], index=pd.date_range('20130102', periods=6))print(s1)df['F'] = s1# 按标签赋值df.at[dates[0],'A'] = 0# 按索引赋值df.iat[0,1] = 0# 用Numpy数组赋值df.loc[:,'D'] = np.array([5] * len(df))# 最终结果print(df) 缺失数据Pandas默认使用np.nan来代表缺失数据。Reindexing允许用户对某一轴上的索引改/增/删，并返回数据的副本。 12345678910111213# 创建DataFrame对象df1，以dates[0:4]为索引，在df的基础上再加一个新的列'E'（初始均为NaN）df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E'])# 将'E'列的前两个行设为1df1.loc[dates[0]:dates[1],'E'] = 1print(df1)&gt; A B C D E2013-01-01 0.194508 -0.897507 0.224745 0.090260 1.02013-01-02 2.412146 -1.191852 -1.644737 0.190971 1.02013-01-03 -0.674645 0.395960 1.425822 -0.718231 NaN2013-01-04 0.349046 0.315026 2.058357 -0.882345 NaN 处理缺失数据12345678# 剔除df1中含NaN的行（只要任一一列为NaN就算）df1.dropna(how='any')# 用5填充df1里的缺失值df1.fillna(value=5)# 判断df1中的值是否为缺失数据，返回True/Falsepd.isnull(df1) 操作统计此类操作默认排除缺失数据 12345678910111213141516171819202122232425262728293031323334353637383940414243# 求平均值print(df.mean())&gt;A -0.190821B -0.050040C -0.203207D 5.000000F 3.000000dtype: float64 # 指定求平均值的轴print(df.mean(1))&gt;2013-01-01 1.2647492013-01-02 1.0497482013-01-03 1.5780672013-01-04 1.0356392013-01-05 1.8557542013-01-06 1.936110Freq: D, dtype: float64 # 创建Series对象s，以dates为索引并平移2个位置s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)print(s)&gt;2013-01-01 NaN2013-01-02 NaN2013-01-03 1.02013-01-04 3.02013-01-05 5.02013-01-06 NaNFreq: D, dtype: float64# 从df中逐列减去s（若有NaN则得NaN）print(df.sub(s, axis='index'))&gt; A B C D F2013-01-01 NaN NaN NaN NaN NaN2013-01-02 NaN NaN NaN NaN NaN2013-01-03 -1.572312 0.570952 -1.108305 4.0 1.02013-01-04 -3.401496 -4.304842 -4.115467 2.0 0.02013-01-05 -4.955735 -5.576715 -4.188780 0.0 -1.02013-01-06 NaN NaN NaN NaN NaN 应用对DataFrame中的数据应用函数。 1234567891011121314151617181920# 逐行累加print(df.apply(np.cumsum))&gt; A B C D F2013-01-01 0.000000 0.000000 0.058997 5 NaN2013-01-02 0.277465 -0.161767 -0.807960 10 1.02013-01-03 -0.294847 1.409186 -0.916265 15 3.02013-01-04 -0.696343 0.104344 -2.031732 20 6.02013-01-05 -0.652078 -0.472372 -1.220512 25 10.02013-01-06 -1.144929 -0.300242 -1.219241 30 15.0# 每列的最大值减最小值print(df.apply(lambda x: x.max() - x.min()))&gt;A 0.849776B 2.875794C 1.926687D 0.000000F 4.000000dtype: float64 字符Series对象的str属性具有一系列字符处理方法，可以很轻松地操作数组的每个元素。 1234567891011121314# 字符串变为小写s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])print(s.str.lower())&gt;0 a1 b2 c3 aaba4 baca5 NaN6 caba7 dog8 catdtype: object 合并连接Pandas在join/merge两中情境下提供了支持多种方式，基于逻辑/集合运算和代数运算来连接Series，DataFrame和Panel对象。 concat()方法连接数组 123456789df = pd.DataFrame(np.random.randn(10, 4))print(df)print(\"------\")# 拆分成块pieces = [df[:3], df[3:7], df[7:]]# 重新连接，可得初始数组print(pd.concat(pieces)) 增补给DataFrame增补行 1234567df = pd.DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])print(df)print(\"------\")# 将索引为3的行增补到整个DataFrame最后s = df.iloc[3]print(df.append(s, ignore_index=True)) 组合“组合”包含以下步骤： 基于一定标准将数据分组 对每个部分分别应用函数 把结果汇合到数据结构中 12345678910111213141516171819202122232425262728293031323334# 新建DataFrame对象dfdf = pd.DataFrame(&#123;'A' : ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'], 'B' : ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'], 'C' : np.random.randn(8), 'D' : np.random.randn(8)&#125;)print(df)&gt; A B C D0 foo one 0.298545 -0.1018931 bar one 1.080680 -0.7172762 foo two 1.365395 0.9394823 bar three 0.783108 -0.5759954 foo two -1.089990 -0.0338265 bar two 0.442084 1.5331466 foo one 0.041715 0.1906137 foo three 0.529231 0.380723# 对'A'列进行合并并应用.sum()函数print(df.groupby('A').sum())&gt; C DA bar 2.305871 0.239875foo 1.144897 1.375099# 对'A', 'B'两列分别合并形成层级结构，再应用.sum()函数print(df.groupby(['A','B']).sum())&gt; C DA B bar one 1.080680 -0.717276 three 0.783108 -0.575995 two 0.442084 1.533146foo one 0.340260 0.088720 three 0.529231 0.380723 two 0.275406 0.905656 重塑层次化12345678910111213141516171819202122232425262728293031tuples = list(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]))# 多重索引index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B'])df2 = df[:4]print(df2)&gt; A Bfirst second bar one -1.144920 -0.823033 two 0.250615 -1.423107baz one 0.291435 -1.580619 two -0.574831 -0.742291# .stack()方法将DataFrame的列“压缩”了一级stacked = df2.stack()print(stacked)&gt;first second bar one A -1.144920 B -0.823033 two A 0.250615 B -1.423107baz one A 0.291435 B -1.580619 two A -0.574831 B -0.742291dtype: float64 对于已经层次化的，具有多重索引的DataFrame或Series，stack()的逆操作是unstack()——默认将最后一级“去层次化”。 1234567891011121314151617181920212223242526print(stacked.unstack())&gt; A Bfirst second bar one -1.144920 -0.823033 two 0.250615 -1.423107baz one 0.291435 -1.580619 two -0.574831 -0.742291print(stacked.unstack(1))&gt;second one twofirst bar A -1.144920 0.250615 B -0.823033 -1.423107baz A 0.291435 -0.574831 B -1.580619 -0.742291 print(stacked.unstack(0))&gt;first bar bazsecond one A -1.144920 0.291435 B -0.823033 -1.580619two A 0.250615 -0.574831 B -1.423107 -0.742291 数据透视表1234567891011121314151617181920212223242526272829303132df = pd.DataFrame(&#123;'A' : ['one', 'one', 'two', 'three'] * 3, 'B' : ['A', 'B', 'C'] * 4, 'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2, 'D' : np.random.randn(12), 'E' : np.random.randn(12)&#125;)print(df)&gt; A B C D E0 one A foo -0.411674 0.2845231 one B foo -1.217944 1.5192932 two C foo 0.502824 -0.1678983 three A bar 0.565186 0.2268604 one B bar 0.626023 0.4015295 one C bar -0.437217 0.8328816 two A foo -0.825128 0.3463037 three B foo 0.069236 0.7287298 one C foo 1.647690 -0.5310919 one A bar -0.881553 0.07071810 two B bar 0.203672 1.60176111 three C bar 1.334214 -0.778639# 生成数据透视表print(pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C']))&gt;C bar fooA B one A -0.881553 -0.411674 B 0.626023 -1.217944 C -0.437217 1.647690three A 0.565186 NaN B NaN 0.069236 C 1.334214 NaNtwo A NaN -0.825128 B 0.203672 NaN C NaN 0.502824 时间序列Pandas提供了简单、强力且有效的工具，可以在频率转换中进行重采样（比如从秒级数据中提取5分钟一更新的数据）。这在金融工程中应用甚广，当然也不仅限于金融领域。 时区表示 12345678910111213141516171819202122232425262728293031rng = pd.date_range('3/6/2012 00:00', periods=5, freq='D')ts = pd.Series(np.random.randn(len(rng)), rng)print(ts)&gt;2012-03-06 -0.6051792012-03-07 0.9612522012-03-08 1.3094062012-03-09 1.1843132012-03-10 0.249745Freq: D, dtype: float64# 设定国际时区标准ts_utc = ts.tz_localize('UTC')print(ts_utc)&gt;2012-03-06 00:00:00+00:00 -0.6051792012-03-07 00:00:00+00:00 0.9612522012-03-08 00:00:00+00:00 1.3094062012-03-09 00:00:00+00:00 1.1843132012-03-10 00:00:00+00:00 0.249745Freq: D, dtype: float64# 切换时区print(ts_utc.tz_convert('US/Eastern'))&gt;2012-03-05 19:00:00-05:00 -0.6051792012-03-06 19:00:00-05:00 0.9612522012-03-07 19:00:00-05:00 1.3094062012-03-08 19:00:00-05:00 1.1843132012-03-09 19:00:00-05:00 0.249745Freq: D, dtype: float64 周期和时间戳之间的转换让我们可以方便的使用一些算术运算。比如下面的例子，我们把一个以季度为单位的时间序列转换为按日期表示。 1234567prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')ts = pd.Series(np.random.randn(len(prng)), prng)print(ts.head())ts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9print(ts.head()) 分类12345df = pd.DataFrame(&#123;\"id\":[1,2,3,4,5,6], \"raw_grade\":['a', 'b', 'b', 'a', 'a', 'e']&#125;)# 将原始记录转换为分类类型df[\"grade\"] = df[\"raw_grade\"].astype(\"category\")print(df[\"grade\"]) 将类别重命名为更有意义的字样 123456789101112df[\"grade\"].cat.categories = [\"very good\", \"good\", \"very bad\"]# 重排类别同时添加上缺失的类别名称df[\"grade\"] = df[\"grade\"].cat.set_categories([\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"])print(df[\"grade\"])# 排序在各分类中分别进行print(df.sort_values(by=\"grade\"))# 对类别列分组可以显示空类print(df.groupby(\"grade\").size()) 绘图随机累加序列 12345678# 生成一串随机序列ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))# 求累加和ts = ts.cumsum()# 输出图象ts.plot() 绘制带标签的列 12345# 生成一个4列的DataFrame，每列1000行，并逐列累加df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=['A', 'B', 'C', 'D']) df = df.cumsum()df.plot(); plt.legend(loc='best') 获取数据输入/输出CSVdf是一个DataFrame 12345# 输出至.csv文件df.to_csv('haha.csv')# 从.csv文件中读取数据pd.read_csv('haha.csv') Exceldf是一个DataFrame 123456# 输出至.xlsx文件df.to_excel('haha.xlsx', sheet_name='Sheet1')# 从.xlsx文件中读取数据pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])('haha.csv')","tags":[{"name":"Wiki","slug":"Wiki","permalink":"https://charlesliuyx.github.io/tags/Wiki/"},{"name":"Library","slug":"Library","permalink":"https://charlesliuyx.github.io/tags/Library/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"}]},{"title":"区块链（比特币）与金融","date":"2017-09-25T20:59:14.000Z","path":"2017/09/25/区块链（比特币）与金融/","text":"【阅读时间】11min - 17min - 4187 words【内容简介】其中讨论了区块链技术（比特币）对金融体系的影响，并解读了最近很火的热点问题：ICO相关的知识 三个关键问题英格兰银行副行长，布罗德本特提出了有关区块链金融的三个关键问题十分犀利，也为宋老师推崇，这里列出来大家共同思考 第一个问题 比特币它并不是创造了一种新的货币，而是创造了一种新的清算方式 清算权清算问题始终是中央银行诞生的最重要的动力，是金融体系中相当核心的问题，而货币发行权的基础在于清算权 17世纪，荷兰的阿姆斯特丹银行，进化成了中央银行，首创用金银币的账户来进行清算 19世纪，上海汇丰银行。控制了大清国的货币清算功能。担当清算中心的职责，整个清朝内部的资金流动，都要通过汇丰银行 到现在，美国对某某国家采取金融制裁，因为美国掌握着美元的清算权 支付宝和微信支付争的是网络交易的清算权 谁能控制清算权，就占据了战略的制高点，有俯瞰全局的作用，控制战局。清算权意味着游戏规则的制订权。 这种新的清算方式，成本低。英格兰银行希望主导这种潮流，也就是RSCoin的核心 英国的数字货币，依赖大账本进行清算 但是它要控制这个大账本 比特币清算的致命缺陷 互联人没有信用体系，用消耗运算力的方式来保持整个记账体系的安全 浪费巨大的资源，导致记账效率低下 会出现记账效率的瓶颈。比如，比特币，每秒只能完成7笔交易，而全世界体系要求的记账效率非常高，比如，VISA卡，每秒2000 - 7000笔 RSCoin的解决办法记账效率低下，是由于竞争记账。既然银行控制大账本，没有必要搞竞争，指定少数几家银行，负责记账。双层记账体系，树形结构优化，RSCoin使用30个授权节点，一秒可以达到2000笔 我的观点有关英国人弄得RSCoin来讲，个人认为已经脱离的了区块链技术的核心Idea：贪婪=信任。既然还是有中心化的控制，那么就等同于只使用了区块链的数据结构，本质来说，也就不是加密货币。 但是，有一些博弈和妥协的想法是非常值得借鉴的，比如，流水账本身就有透明的好处，其实是从另一方面利用区块链密码学加密技术实现了一种变种的信息透明 第二个问题 如果数字货币是一个独立的货币，未来它将会跟传统的银行，争夺储蓄 举个例子来说，共享单车想当于数字货币。出现之前，单车控制权都是银行。骑车，只能从一家银行停车场到另一家银行的停车场。自行车的控制权，由银行转向了最终客户。只要能解开密码锁，你就可以随便骑，随意停。久而久之，共享单车数量越来越大，传统的自行车就会越来越小 银行的商业模式是：吸纳储蓄➜放贷➜吃存款和贷款之间的利差。数字货币的量越大，银行体系中的储蓄减少，金融体系面临挤兑的风险，数字货币很可能成为传统银行这种商业模式的掘墓人，如何解决这个问题，布罗德本特提出了第三个问题 第三个问题 新的数字货币如何去倒逼传统银行商业模式的改革 （胖银行➜瘦银行） 胖银行的基本运行模式 货币金融学中的概念：部分准备金：允许银行用一块钱的准备金进行10倍的放大 ，即用1块钱创造10块钱 银行体系因为这个制度有了巨大的竞争优势。开银行等于印钞 其中的前提条件或基础就是一定要获得大量的储蓄然后才能进行放贷 存在问题 储蓄是别人的钱，是短期的，放贷的资产是长期的 在金融危机中，资产会迅速贬值，使得银行体系变得不稳定 危机爆发，银行倒闭，银行倒闭，社会流动性紧缩，钱荒，更多银行倒闭，工厂关门，工人下岗，消费力消失，导致金融危机和经济危机 特权化是社会动荡的罪魁祸首 瘦银行亚当·斯密，李嘉图等主张搞瘦银行（不赞同部分准备金制度） ，比如有名的芝加哥计划 周期性金融危机的爆发，就跟银行不断放大这种杠杆就直接的关系。其中的关键问题是：如果银行由胖变瘦，使银行总的资产负债表收缩，社会的流动性紧缩，发生钱荒，经济就会停滞不前 解决方式 央行给全体国民每个人开账户 传统意义上，只有金融机构可以在中央银行开账户，目的也是相互清算。以前做不到，主要原因是技术水平不够，效率太低，成本太高 但在数字货币时代，这个问题很简单。意味着央行向人民开放资产负债表，人人可以在央行有账户。应对金融危机的方式变成，中央银行给每个国民的账户上打钱 重资产商业银行现在是重资产 持有大量的贷款，长期贷款。承当巨大的风险：比如利率，汇率不稳定；经济周期；地缘政治冲突；国际油价。进而引发贷款出现违约 在日益动荡的社会中，重资产的运行模式，风险大，利差小 轻资产转型问题，就涉及到轻资产，这里可以用万达的转型之路来举例 最开始重资产：从拿地，融资，投资，建设到经营，自己搞定。造成的问题是人多事杂；押占大量资金；销售也有压力 转型成轻资产模式：出品牌，设计广场，钱由社会上投资人来投 ➜负责运营，利润分成➜赚取管理费用 银行也是类似的轻资产模式 银行出品牌，出技术团队：因为银行的人对全国各种项目比较了解，知道哪个贷款比较可靠，风险低 重点工作是：设计贷款产品；设计完，投资人来出钱，银行负责风控，运营，挣的钱也是服务费 关于三个问题的思考这么一套总结下来，的确想象中十分美好，但是我们不能忽略国家间的政治博弈，利益冲突。银行转型，也是涉及到了众多的问题和利益方面的权衡和Trade-off，但我认为，区块链比特币的出现，的确给了银行一定的压力，这样就和布罗德本特先生的真知灼见切合了，是一种倒逼，是一种时代潮流倾轧的必然产物 什么是ICO最近中国强势出手，关停ICO，手段雷厉风行，很多ICO项目都变成了一场庞氏骗局，让人唏嘘，这里针对ICO做一些思考，出自借鉴科普文章 赌场例子 有一个人开了一间赌场，每个赌徒要来赌场玩，必须先换一些筹码，才能参与赌场内的赌局。赌场内所有赌局都是实时利用筹码结算的。 这间赌场服务特别好，赌具特别好，入场费收得也非常低，总之一切都特别好，于是越来越多的赌徒都慕名而来，跑到这间赌场来玩，赌局额度也越来越大。自然，这就需要更多的筹码来确保赌场的顺利运营。 但是，这间赌场的筹码是用一种特殊的金属、特殊的工艺制造的，这些赌具只能用这种筹码玩。赌场一开始的时候造了一万个，以后再也造不出来了。怎么办？ 参与赌局的人越来越多，赌局额度越来越大，原本1个筹码1美元，同时100个人在玩，一次赢1美元，赌场运行得很顺利，除了在赌徒间流通的筹码外，赌场手里还有一些筹码，可以卖给刚进场的赌徒。现在来了1000个人，每次要赢100美元，依然只有10000个筹码。 为了玩得尽兴，于是赌徒之间互相约定，我们1个筹码100美元，这样就能玩得尽兴了。新进场的赌徒从老赌徒手里高价买回了筹码，老赌徒大赚了一笔，新赌徒玩的尽兴。赌场呢？手里剩的筹码也可以100美元的价格卖出去了。 由于赌场运营得越来越好，来的赌徒越来越多，赌局越来越大，筹码的价格就一路上涨。 对于赌场而言，由于一开始有大量的筹码以1美元的价格卖给了老赌徒，如果按照100美元的价格承兑，赌场就亏大了。于是，赌场就宣布，找我承兑筹码可以，但是只能按照1美元的价格。 但是赌场在卖出筹码的时候，依然按照市场价100美元出售，这也成为赌场盈利的一种手段。于是大家都不会把赢来的筹码拿去找赌场退钱，而是留在手里等着卖给新的赌徒。 赌场例子背后的经济学常识随着生产力越来越高，市场越来越大，市场上流通的商品（总价值）越来越多，如果货币的流通速度不变，理论上就需要更多的货币。从赌场故事中，可以非常容易地看出，由于赌场固定了筹码的总额，加上很多人持币等待升值造成筹码流通速度变化不大，于是筹码价格只能上涨。 进行一个赌场ICO 又有一个人，看到了赌场的生意这么好，眼红了，于是他也想开一间赌场。可是他没有足够的本钱，怎么办呢？于是他公开宣布：我要开一间赌场，赌场服务非常好，赌局非常好，手续费非常低。我的赌场只有10000个筹码，以后绝不增加。 现在筹码先造出来了，我把其中5000个筹码拿出来，任何人都可以来买。这5000个筹码卖完了以后，我就用卖掉的钱作为本钱开赌场。等赌场开业了，你们可以把手里的筹码卖给赌徒，或者自己来赌都行。 由于开始赌场还没开业，所以出售的筹码价格也是很低的，0.5美元1个。等赌场开业了，至少1美元1个，甚至更高。由于很多人都看好这个生意，于是5000个筹码很快就卖光了。这个人筹集到了2500美元，造了一些赌具，租了间房子，赌场就开始营业了。 由于他的服务真的很好，很快又聚集了大量的赌徒，于是筹码的价格一路飞涨，很快就到了100美元。在赌场开业前以0.5美元的价格买了筹码的人，筹码增值了200倍。 我们看到，其中加粗的环节都是预先设计或者是预想的。并且这个案例和加密货币（区块链）的特别不谋而合 ICO英文全称是Initial Coin Offering，翻译成中文是“初始货币供应”。 ICO的基本原理 公司创造了一种商业模式，在这种商业模式里，大家只能使用公司自己发行的“代币”（虚拟货币）进行交易。公司事先宣布，这种代币总额是固定，或者增发的方式是固定，也就是说，任何人都不能更改代币增发的规则（区块链技术从本质上保证了这一点）。 如果大家认为我们的商业模式非常有前途，我们的代币就会增值。现在公司拿出一定比例的代币进行发售，用筹得的经费作为本钱来运营这种商业模式。 这种依靠出售日后商业模式中的某种公司产品（如果代币可以视为公司产品的话）的方式来筹集资金的金融行为，就被称为ICO ICO的特点 1、所发行的代币必须是在未来的商业模式中有使用价值，并且不可替代。 2、ICO的商业模式中，代币的发行方式必须事先固定规则，并且不可更改。 3、ICO虽然是一种商业融资的方式，但是ICO模式并不出让股权或者负债，也就是说，运营该商业的公司，未来仍可以继续出让股权或者举债 ICO与区块链如果你之前已经对区块链最大的核心思想了有了了解，你就知道ICO里面有一个C coin，本身就是看中了区块链没有任何一个中心能够控制这个系统，数据一旦产生便不可更改，并通过工作量证明，产生的强大信任。 但是，这个加密货币的实体在未来到底能不能变现，必须要落到实际问题中，真真正正可以解决人们生活中的实际问题。 加密货币直接能解决问题是金融体系的一些问题，这在布罗德本特的三个问题中已经进行了详细的阐述。正因为金融体系的局限性，这种货币不可能同时出现很多并且都具有那么高的价值。 那就需要以太坊这种本质上的服务类落地的想法来晚上这一落地过程，这部分的一些创新和面临的技术难题，在如何评估竞争币的价值与智能合约中我进行了一切探讨 总结我作为一个学习CS专业，学习机器学习的人来说，对于金融的理解十分浅显，希望各位前辈能多多指教，共同探讨！ 以上！鞠躬！","tags":[{"name":"BlockChain","slug":"BlockChain","permalink":"https://charlesliuyx.github.io/tags/BlockChain/"},{"name":"FinTech","slug":"FinTech","permalink":"https://charlesliuyx.github.io/tags/FinTech/"}]},{"title":"现代区块链与新技术","date":"2017-09-25T18:54:40.000Z","path":"2017/09/25/现代区块链与新技术/","text":"【阅读时间】不断更新百科类【内容简介】总结市面上不同的币种，关注区块链技术的最新进展，整理总结各种新名词，新概念，新技术相关概念和文章 竞争币和竞争块链因为比特币的完全开源，所以基于比特币，有创新的，无创新仅仅修改一些参数的，出现非常多的竞争币和竞争块链（与比特币和区块链的关系一样，两者联系紧密） 竞争币区别与比特币主要是以下三点： 货币策略不同 基于工作量证明的一致性机制不同 一些特殊的功能，比如更强的匿名性等 如果想关注所有竞争币（机密货币）的信息，可以访问：MaoOfCoin，打开会有所有你想知道的信息 评估竞争币的价值我们可以从问几个最基本的问题开始入手 这款竞争币有没有引入重大创新？ 如果有，那么这项创新是不是足够有吸引使用比特币的用户转移 这款竞争币是不是致力于某一细分领域或引用？ 这款竞争币可以吸引到足够的矿工来抵御一致性攻击吗？ 更多的，关于财务和市场的问题： 这款竞争币的市场总值是多少？（一般最初来源于ICO） 整个系统的用户/钱包规模大概是多少？ 接受其支付的商家有多少？ 整个系统每日的交易数是多少？ 交易总量是多少？ 元币平台 Meta Coin元币和元区块链是比特币之上实现的软件层，也可以认为是覆盖在比特币系统之上的平台/协议，或者是一个币中币的实现。 这些功能扩展了核心比特币的协议，使得比特币交易和比特币地址附加信息称为可能 彩色币 Colored Coin通过仔细跟踪一些特定比特币的来龙去脉，可以将它们与其他的比特币区分开来，这些特定比特币就叫作彩色币。 它们具有一些特殊的属性，比如支持代理或聚集点 ，从而具有与比特币面值无关的价值。 彩色币可以用作替代货币、商品证书、智能财产以及其他金融工具，如股票和债券等。 彩色币本身就是比特币，存储和转移不需要第三方，可以利用已经存在的比特币的基础，因此彩色币可以为现实世界中难以通过传统方法去中心化的事物铺平道路。 万事达币 MasterCoinMasterCoin是比特币协议的应用层协议，类似HTTP协议是TCP协议的应用层一样 货币属性不同于比特币的竞争币比特币自身所具有的一些设计规则使得它成为了一个总额固定并且不通胀的虚拟货币，有一些竞争币通过对这些货币属性的微调，来达到实现不同的货币政策的目的。 莱特币 LiteCoin它是最早的一批竞争币的一员，自2011年发布至今，已经成为继比特币之后的第二成功的电子货币。它的主要创新是两点 使用了Scrypt作为工作量证明的算法（这种算法比SHA256来说，主要的特点就是内存消耗更多，难度更大，使用ASCII或者GPU矿机更加难以计算） 更快的货币参数 狗狗币 DogeCoin它是基于莱特币的一款竞争币，与2013年12月发布。之所以值得一提还是因为它飞快的出块速度和惊人的货币总量。现在已经一文不值 Freicoin于2012年7月发布。它是一种滞留性通货，可以理解为存在钱包中的货币的利率为负数，只有不断交易和消费才能保证它不变少。它的特点是它的货币政策正好和比特币的通货紧缩政策相反 货币属性总结表 货币名称 出块速度 货币总量 一致性算法 市场总值（到2017/9/25） 24小时交易量 莱特币 2分半 2014年8400万 Scrypt $2,716,920,950 $214,787,000 狗狗币 60秒 2015年达1000亿 Scrypt $123,105,955 $11,777,800 Freicoin 10分钟 2014年1亿 SHA256 $108,614 $1 一致性机制创新的竞争币在时代的发展中，除了SHA256找0的方式，还衍生出了不同的算法来实现工作量证明的一致性机制。包括Scrypt；Scrypt-N，Skein，Grosetl，SHA3，X11，Blake。这些算法都在一定程度上组织的ASIC矿机的泛滥 而在2013年，出现的一种替代方式：权益证明（或股权证明 Proof of Stake PoS），称为现代竞争币的基础 在权益证明系统中，货币的所有人可以将自己的货币做利息抵押。类似于存款证明，参与者可以保有他们货币的一部分，通过利息和矿工费的方式获取回报 Peercoin与2012年8月发布，首款工作量证明和权益证明混用的竞争币 Myriad与2014年2月发布，它同时使用了5中工作量证明算法（HA256d；Scrypt；Qubit；Skein；Myriad-Groestl），根据矿工的情况动态选择。这也是为了让系统不受集中化ASIC矿机的影响，也加强了其抵御一致性攻击的能力 黑币 BlackCoin与2014年2月发布，使用的是权益证明的一致性机制。同时，它引入的可以根据收益自动切换到不同竞争币的“多矿池”机制也值得关注 VeriCoin与2014年5月发布，它使用权益证明机制，并用市场供需关系动态调整利率。它是首款可以直接在钱包中兑换比特币支付的竞争币 NXT发音同Next，一种纯粹的权益证明竞争币，根本不采用工作量证明的挖矿机制。 它是一款完全自己实现的加密货币，并非衍生品。NXT具有很多先进的功能，包括名字注册、去中心化资产交易、集成的去中心化加密信息的权益委托。NXT也被称为加密货币2.0 货币属性总结表 货币名称 出块速度 货币总量 一致性算法 市场总值（到2017/9/25） 24小时交易量 Peercoin 10分钟 没有上限 工作量证明和权益证明混用 $31,009,448 $395,757 Myriad 30秒 2024年到20亿 多重算法 $3,758,612 $59,805 Blackcoin 1分钟 没有上限 权益证明机制 $13,817,132 $1,180,000 VeriCoin 1分钟 没有上限 权益证明机制 $10,103,100 $180,508 NXT 1分钟 1亿 权益证明机制 $63,052,381 $2,330,510 多目的挖矿创新比特币的工作量证明机制的目的是：维护比特币系统的安全，构建去中心化的信任。很多人认为挖矿这一行为是一种浪费。（这个观点个人持保留态度） 多目的挖矿算法就是为了解决工作量证明导致的“浪费”问题而出现的。多目的挖矿的在为货币系统的安全加入额外需求的同时，也为该系统供需关系加入了额外的变量 PrimeCoin与2013年7月发布，它的工作量证明算法可以搜索质数，计算孪生素数表。很有意思的，随着PrimeCoin新区块的不断产生，会不断的发现新的素数，构造一个科学成果：素数表 CureCoin与2013年5月发布。它把SHA256工作量证明算法和蛋白质褶皱结构的研究结合起来。蛋白质褶皱研究需要对蛋白质进行生化反应的模拟，用于发现治愈疾病的新药，但这一过程需要大量的计算资源 GridCoin与2013年10月发布。它结合了Scrpy为基础的工作量证明算法和参与BOINC计算项目的补贴机制。BONIC是伯克利发开的网络计算系统，算力是提供给这个平台的 货币属性总结表 货币名称 出块速度 货币总量 一致性算法 市场总值（到2017/9/25） 24小时交易量 PrimeCoin 1分钟 没有上限 计算素数 $2,750,679 $691,788 CureCoin 10分钟 没有上限 蛋白质研究 $5,078,597 $80,012 GridCoin 150秒 没有上限 BONIC $12,920,406 $91,290 致力于匿名性的竞争币其实比特币的地址和显示中真是个人的关系还是比较容易通过大数据的分析手段计算出来的，所以有一些加密货币希望能在这方面有突破 ZeroCoin/Zerocash还在开发当中 CryptoNote它提供一种以电子货币为基础的匿名性的参考实现，与2013年10月发布。它可以被克隆继而衍生出其他的实现，并内建了一个周期性的重置机制使其不能作为货币，很多竞争币基于它实现：入ByteCoin，Aeon，Boolberry，DuckNote，FantomCoin，Monero，MonetaVerde和Quazarcoin ByteCoin与2012年发布，是CryptoNote的第一个实现，之前还有一个同名的ByteCOin电子货币，BTE，这个为BCN。 ByteCoin使用了基于CryptNote的工作量证明机制，每个实例至少2MB的内存，是的GPU和ASIC矿机无法在Bytecoin中运行，它继承了CryptoNote的环签名、不可链接交易和块链抗分析匿名性等机制 Monero货币区县比ByteCoin平缓，在系统运行最开始的四年发型80%的货币 货币属性总结表 货币名称 出块速度 货币总量 一致性算法 市场总值（到2017/9/25） 24小时交易量 ByteCoin 2分钟 1840亿 基于CryptoNote $237,302,332 $1,501,490 Monero 1分钟 1840万 基于CryptoNote $1,395,218,943 $28,579,800 非货币型竞争区块链这些区块链设计模式有着自己独特的设计模式，并不主要作为货币使用。当然不少这种区块链也含有货币，但只不过它们的货币仅是一种象征，用于分配其他东西，比如一种资源或者一份合约。 域名币 NameCoin它是一种使用区块链的去中心化平台，用来注册和转让键-值对。也就是域名注册。现在，.bit的替代性域名服务（DNS）就是使用这个系统来完成。 用它注册的域名空间不受限制，人和人都可以以任意方式使用任意的命名空间 Bitmessage它是一种去中心化安全消息服务的比特币竞争区块链。其本质是一个无服务器的加密电子邮件系统。 Bitmessage可以让用户通过一个Bitmessage地址来编写和发送消息。这些消息的运作方式与比特币交易大致相同，区别在于，消息的保存是有时间时间限制的，如果两天还没被送到目的地，就会消失。 Bitmessage的好处是可以抵御全面监视，除非网络偷听者破坏了接收方的谁被，否则无法截取邮件信息 以太坊以太坊是一种图灵完备的平台，基于区块链账本，用于合约的处理和执行。它不是比特币的一个克隆，而是完全独立设计和实现的。币用来付合约执行的花费。 以太坊区块链记录的东西叫做合约，所谓合约，就是一种低级二进制码。本质上，合约是运行在以太坊系统中各个节点上的程序。这些程序可以存储数据、支付及收取、存储币以及执行无穷范围的计算行为，在系统中充当去中心化的自制软件代理 货币属性总结表 货币名称 出块速度 货币总量 一致性算法 市场总值（到2017/9/25） 24小时交易量 NameCoin 10分钟 2140刀2100万 SHA256 $20,536,647 $125,689 Ethereum 14秒 1亿 转POS中 $27,458,433,693 $459,104,000 权益证明什么是权益证明权益证明（又称股权证明），英文单词 Proof of Stake，缩写PoS，与之平级的概念是工作量证明，Proof of Work，这个我们应该很熟悉了 PoS会根据你持有货币的量和时间，给你发利息。持有货币的时间，被称之为“币龄”，每个币每天产生1币龄。 例如，你持有100个币，总共持有30天，你的币龄就是3000，此时，若你发现了一个PoS算法支持的区块（直观来说就是你拥有币的数量和时间越长，你新建区块的几率越大），你的币龄就会被置0。你每被清空365币龄，你将会从区块中获得0.05个币的利息（相当于年利率5%），那么在这个案例中，利息 = 3000 * 5% / 365 = 0.41个币。 这个利息的数量也是不同的币种自己来设定的，变为一种货币属性 也就是说，你的“挖矿”收益将正比于你的币龄，与算力无关 这种新的一致性算法不要求证明完成一定数量的计算工作，而是要求证明者对某些数量的钱展示其所有权。中本聪没有这么做的原因其实是因为当年还有一个去中心化的能展示个人财产的方式，而现如今，有一个展示财产的方式就是比特币本身。 为什么要设计PoS设计权益证明的初衷其实是为了解决比特币本身规则的几个痛点，这也是PoS的设计者，或者很多人公认的一些原因的概括，个人观点上并不完全认同 挖矿动机衰减比特币每过240000个区块，区块奖励（Coinbase）就会减半。很多人对此表示担忧，他们认为在未来因为奖励的减少导致大家挖矿的动力越来越低，整个比特币网络就会陷入瘫痪（一种情况是大家都减少运行比特币客户端的时间，P2P可用链接节点就越来越少） 【解决方案】 在PoS体系中，只有打开钱包客户端程序，才能发现PoS区块，才能获得利息，这促使很多不想挖矿的人，也会打开钱包客户端（为了获得利息） 共识攻击按照第一条的逻辑，随着矿工的减少，比特币很有可能被一些高算力的人进行51%攻击，导致整个比特币网络攻击。个人观点是，只要比特币还有价值（甚至的对应电费的价值），世界上的人类贪婪的本性不会变，只有有利可图，或者利益足够吸引人，马上会有足够多人的去挖矿，所以会维持一种诡异的平衡。这个问题在各种PoS的说明书上被反复提到，个人观点是：不应该设想一种情况的发生，而是详细的从用户动机的角度来论证和思考，你很自然的通过现象得出一个结论，这种做法是很武断的 【解决方案】 当然，在PoS体系中，利息产生的货币是保存在PoS算法的区块中，直观结果是，这会要求攻击者还需要持有超过全球51%的货币量才可以，从侧面来说提高了攻击的难度 通货紧缩体系比特币因为货币总量和丢失等问题，会导致通货紧缩 【解决方案】 对于PoS体系来说，可以通过调整年利率的方法来调控整体的紧缩和通胀状态。但是从侧面来讲，通货紧缩和通货膨胀都是经济学问题，说白了，是一种经济体（国家）运行状态，其中涉及到十分复杂的学问，对于加密货币本身来说，还远远没有到需要讨论紧缩和通胀的时候 智能合约自2017年以来，随着IBM，微软两大巨头的加入，以及各大银行的支持，区块链+智能合约的解决方案越来越受到大家的关注和重视。 什么是智能合约合约合约的概念即合同，一个承诺，规定一个规则，大家必须遵守，并设定违反惩罚机制，这就是合约。但我们知道，合约的执行必须要有权威中心机构背书，在现代区块链实现点对点信任的基础上，合约的执行过程从某种程度来说，可以去中心化。从另一方面来说，提高了很多办事情的核心效率。 为什么这样说呢？因为现实生活中，我们对交易对象，合作伙伴，甚至婚姻都是不能说完全信任的，一方面因为人的善变特质，另一方面因为事物随时间进程推进的不确定性（比如车祸，或者不可预知的特殊情况）合约在当今社会是一个巨大的市场，没有合约，寸步难行，这么说，一点也不为过。而市场巨大的价值就是其提供的信任，而所谓智能合约，智能而字除了自动执行外，更加包含了一种由区块链本身原理带来的信任，这也是其本质价值所在 智能合约这个名词或者说概念，提出的了时间非常早，是由尼克萨博于1996年提出的，定义为： 一个智能合约是一套以数字形式定义的约定，包括合约参与方可以在上面执行这些约定的协议，智能合约的基本思想是，各种各样的合约条款，可以嵌入到我们所使用的硬件和软件中，从而使得攻击者需要很大的代价去攻击 你可以想象，最简单的智能合约是就是一台自动售货机。合约内容是，你给他钱，他出商品，非常简单的内容。 复杂的合约，可以想象，现在Uber的所有打车过程都是由Uber的服务器完成，并且使用第三方支付系统来付款，如果使用智能合约变成“去中心化的Uber”，场景就变为，每一次乘车，以一种没有破绽的逻辑来触发乘车交易这一智能合约的内容，那么就可以完全省去付款的部分，合约自动打钱（当然这里打的钱只是区块链的上加密货币）。当然这只是强行设想的一种非常复杂的智能合约形式，在现实中，基本上是完全无法实现的。 智能合约（用于实现事务的业务规则结合后的产物）实际上是一种过程调用（Procedure call），可在网络中多个节点上运行，运行后输出的结果通过合意（Consensus）过程被所有网络成员认可 个人观点是：区块链+智能合约必须对应特定场景，特定需求，才可能有应用价值，比如对信任十分需求的场景，而传统解决方案中为了信任本身需要花费很大的代价或效率十分低下。 从最近各大银行的动态，还有新闻来看，银行贷款，保险，供应链管理，合规领域都是很有潜力的 但是智能合约+区块链面临的问题也很多，截止今天，也有很多项目或技术在为了落地它而努力 比如执行速度，执行成本，可扩展性，匿名性，和现实世界的隔离性，还有政策和法律的监管等等问题。 以太坊技术栈Solidity是一门需要学习的开发语言 在应用层来说，还有对应网络部分的web3.js作为一个和网络部分交互的JavaScript API 还有一些编译器（Solc），机器层面的（EVM）的工具，社区Library（Zepplin）等 热点名词或概念总结这个版块不定期更新，主要作为新技术的一个总结窗口 RootStockRootStock 是一个建立在比特币区块链上的智能合约分布式平台。它的目标是，将复杂的智能合约实施为一个侧链，为核心比特币网络增加价值和功能。RootStock实现了以太坊虚拟机的一个改进版本，它将作为比特币的一个侧链，使用了一种可转换为比特币的代币作为智能合约的“燃料”。 侧链 Sidechain楔入式侧链技术（ pegged sidechains），它将实现比特币和其他数字资产在多个区块链间的转移，这就意味着用户们在使用他们已有资产的情况下，就可以访问新的加密货币系统。目前，侧链技术主要是由Blockstream公司负责开发。 闪电网络 Lightning Network闪电网络的目的是实现安全地进行链下交易，其本质上是使用了哈希时间锁定智能合约来安全地进行0确认交易的一种机制，通过设置巧妙的‘智能合约’，使得用户在闪电网络上进行未确认的交易和黄金一样安全（或者和比特币一样安全）。 超级账本 Hyperledger超级账本（hyperledger）是Linux基金会于2015年发起的推进区块链数字技术和交易验证的开源项目，加入成员包括：荷兰银行（ABN AMRO）、埃森哲（Accenture）等十几个不同利益体，目标是让成员共同合作，共建开放平台，满足来自多个不同行业各种用户案例，并简化业务流程。 由于点对点网络的特性，分布式账本技术是完全共享、透明和去中心化的，故非常适合于在金融行业的应用，以及其他的例如制造、银行、保险、物联网等无数个其他行业。 通过创建分布式账本的公开标准，实现虚拟和数字形式的价值交换，例如资产合约、能源交易、结婚证书、能够安全和高效低成本的进行追踪和交易。 如果想进一步具体案例，这篇文章可能可以帮到你，使用Hyperledger Composer十分钟搭建区块链概念验证环境 面向商业的区块链基础设施平台现在最常见四大区块链技术开源平台，Ethereum、来自IBM的Fabric、Corda和BCOS 在四个开源技术平台中，Ethereum代表的是公有链技术，Fabric、Corda和BCOS代表的是联盟链，即多个机构联合创建，需要身份验证的半公开“受控”系统。 在公有链、私有链还是联盟链中选型，取决于开发者和应用场景的需求。对于“安全”有特殊需求的金融机构和企业级应用来说，联盟链的低风险与高可控，最有利于说服法律部门和监管者 OpenBazzarOpenBazzar是一个结合了ebay与BitTorrentt特点的去中心化商品交易市场，使用比特币进行交易，既没有费用，也不用担心受到审查。 因此相对于易趣与亚马逊这些提供中心化服务的电子商务平台，通过OpenBazz不需要支付高额费用、不需要担心平台收集个人信息致使个人信息泄露或被转卖用作其他用途。 2015年，获得了由科技行业的两大风投公司Andreessen Horowitz和Union Square Ventures 投资。 关于如何使用OpenBazzar建立新的交易推荐文章：什么是OpenBazzar","tags":[{"name":"BlockChain","slug":"BlockChain","permalink":"https://charlesliuyx.github.io/tags/BlockChain/"},{"name":"Ethereum","slug":"Ethereum","permalink":"https://charlesliuyx.github.io/tags/Ethereum/"}]},{"title":"一文弄懂区块链-以比特币为例","date":"2017-09-24T22:09:42.000Z","path":"2017/09/24/一文弄懂区块链-以比特币为例/","text":"【阅读时间】29min - 35min - 11348 words【内容简介】此文潜在面向群体是对区块链或比特币的运行原理完全不了解的人。所以会从用户需求的角度出发，一步一步发明区块链（或者说比特币，因为两者互相依存），在此之后的内容有关比特币与金融，ICO，竞争币等，可选择性阅读。 如果有帮助求知乎点个赞，感谢！ 因为贪婪，所以信任 加密货币在一步一步发明发明比特币之前，解释几个直观的认知： 我们常说的比特币，是加密货币（Cryptocurrency）的一种，而加密货币实现去中心化的最关键的技术是区块链 有些地方可能把加密货币又称为数字货币（或称电子货币），但实际上，加密货币是数字货币的子集，同为子集的还有虚拟货币（如Q币），加密货币的称谓要更加专业 加密货币一定具有下列三个特点 去中心的清算 分布式的记账 离散化的支付 为了实现这些特点，需要使用到区块链技术。这里的区块链技术是一个很广义的范畴，它包含了密码学，算法等很多不同的内容，其中最精彩的点子，可算是工作量证明 = 共识信用了 一步一步发明比特币第一个用户需求 - 账本和电子签名的由来 第一个用户需求描述了中心化清算系统几个关键内容的由来，只对区块链感兴趣的读者可以跳过 经济体的蓬勃发展离不开交易。在交易过程中，人们早已发现使用一般等价物（如金银）十分麻烦，发明了纸币（最早的来自中国，北宋时代四川地区的纸币交子的清算体系，是生产力发展的必然产物，最终的目标是提高生产效率），现如今，人们发现，携带现金也很麻烦 这是第一个基本用户需求：摆脱现金进行交易带来的不便 【解决办法】几个用户使用公共账本记录转账记录，月底结算，账本公开，每个人都可以修改，也就是说可以在上面添加新行（一笔交易），如小明转账给小红10块钱 产生的问题1：身份问题在这个账本条目上我们无法确认交易双方小明和小红是否是本人，可能出现伪造（逍遥法外电影中的伪造支票） 【解决办法】使用电子签名，即公钥 - 私钥对 记住，电子签名被发明的核心目的是希望在电子文档也能有一个类似与现实中个人笔迹的签名，目的一定是：确认写这个签名的人是本人，即身份确认（验证） 私钥顾名思义，也叫做密钥，是你本人需要需要妥善保管和保存的$$Sign(\\text{信息},私钥) = \\text{电子签名}$$Sign在这里是一个函数，可以理解为一连串计算（变换），这一连串计算有一个特点，就是输入值只要改变一点点，输出就会完全改变。信息和私钥一起，可以得到一个电子签名。并且这个电子签名不能被轻易的复制到其他信息里，原因是因为每一个电子签名都和这一段信息有关联。$$Verify(\\text{信息},\\text{电子签名},\\text{公钥}) = \\text{真/假}$$在进行验证的时候，Verify也是一个函数，输入值是信息，电子签名，公钥，输出是一个True or False，来判断这个电子签名是真的还是假的。 这个时候可能有人就要问了，这个电子签名我难道不能试出来吗？很不幸，这是一个有256bit的1/0字符串，可能性是2的256次方，2的10次方是1024，我只能告诉你这么多了。 解释完电子签名，我们来看看实例。小明使用自己的私钥加上小明转给小红10块钱这段话通过Sign函数生成一个签名（256位），把签名放在这条转账信息的后面，通过之前的讲解，这个签名就能保证小明已经过目了，并且说：“这真的是我小明，不用怀疑了！肯定是我” 直观结果是，我们可以利用密码学的手段，只要有对应人的数字签名，我们保证小明和小红的身份能被100%确认真实 但是这个解决方案有一个小漏洞：可以复制同一行信息来伪造交易记录，解决的办法是添加一个这笔交易独有的信息（比如时间戳） 产生的问题2：欠债跑路问题如果小明在此时账户上已经没有足够的余额进行支付，就会出现超支问题 【解决办法】添加余额记录，此时就不可避免的需要一个中间担保人（国家？信誉机构？银行？）为小明进行余额担保 一个大家都遵守的协议此时，现代金融体系的框架基本建立完毕，协议内容是 任何人都可以在账本上添加新行 固定时间间隔时用真金白银进行清算 只有有签名的交易是有效的 中间担保的人保证不可超支 此时发现一个很有趣的发现，这个比较严谨的协议有一个特点：如果所有人都按照这个协议来办事，我们可以用任何形式的东西来代替人民币了，换句话说，就是我根本不关心你在账本上添加的新行的交易内容是什么，可以是任何东西 利用这个提出需求再解决问题的过程，强化一个认知：货币 = 交易记录（账本），即货币的本质是交易记录，在这背后，有一个前提是，货币的另一个本质是一种共识，我们都信任它有价值的共识 第二个用户需求：账本放在哪里？传统的（现在的）解决方案当时是，使用中心代理-银行，来存放账本 既然是第二个用户需求，那肯定就是因为现在的解决方案大家都不满意 核心需求：去中心化中心化的痛点大致可以说几点 银行效率低下，一笔跨国转账的等待时间较长 胖银行金融体系因部分准备金制度等等方便的规则，能抬升杠杆，产生金融泡沫，进一步诱发金融危机 私有财产神圣不可侵犯是精英与平民，剥削与被剥削者几个世纪以来博弈的风暴中心 当然还有很多没有提到（比如好处，控制经济发展速度，调控供需平衡等），总之，是一种一直饱受诟病的清算方式，此时，中本聪在2009年横空出世，他提出了一种全新的清算方法，并且真正解决了陌生人间信用的问题！接下来就是真正的一步一步的发明比特币了 如何实现分布记账（去中心化）为了去中心化，我们可以反其道而行之：每个用户保存账本，分布记账。用户产生一笔交易就将这笔交易广播到到网络上所有的节点上，这样不就完美的去中心化了？ 广播模式分布式账本示意图 只要是明眼人都能发现，太天真的，这个方法行不通。若行不通，那就把行不通的原因总结出来 遇到问题，总结不可行的原因，寻找解决方案。这是整个人类不断前进的核心最小单位 问题核心如何让所有人都同意这个新账本？如何保持这些账本同步？有一笔交易发生时，如何让其他人都听到并相信这一笔交易呢？ 这些问题才是真正的核心：是否能在协议（办法，规则）中添加几行，找到办法，来决定是否接受交易，并确定交易顺序，使你可以放心的相信，世界上遵守同一协议的所有人手上的账本都和你的一模一样呢？（问题描述值得品读，只有抽象出问题才能更好的去寻找解决方案） ☆解决方案解决的思路是：哪个账本的计算工作量大，就信任哪个账本。换个角度来说是【让交易欺诈和账本不一的情形的计算力成本高到不能接受甚至完全不可行】 1、密码学：哈希函数哈希函数，输入可以是任意信息或者文件，输出是固定长度的比特串。例如256bit的1/0串，这个输出叫做这个信息的“哈希值”或者“摘要”（digest）。SHA256就是一个哈希函数 密码哈希函数的特点 密码哈希函数有几个特点 特点是输入值稍微变化后，结果就会有很大的不同，完全无法预测不同输入间的规律 逆向计算不可行，只能使用试错法（穷举法），解空间 $2^{256}$ 在每一个账本后添加一个特殊数字，对整个列表使用SHA256，我们要求这个特殊数字可以使得输出值的开头有30个零（关于如何确定0的个数问题，在后面部分有详细的说明） 寻找能使得输出前30位为0的特殊数字 根据之前说过SHA256的性质：输入变化输出不可预测，找到这个特殊数字唯一的办法就是穷举。换言之，你很容易就证明了他们进行了海量的计算。而这个特殊数字就叫做工作量证明（proof of work） 这就意味着，所有的工作量证明就对应了交易列表（账本 Ledger），如果你修改了一个交易，哪怕只是其中一个字符，就会完全改变哈希值，就得重做工作量证明，直观动图如下 修改后的重新计算 2、区块链 - 信任与共识的基石每一个小账本被称为区块，每一个不同的区块链协议（产生不同的加密货币）都会规定每一个区块的大小（最初比特币为1M） 账本组成区块，区块构成链表，区块的头包含前一块的哈希值，这就是区块链 区块链的诞生 如此一来，任何人就不能随意修改其中的内容，或者交换顺序。如果你这么做，意味着你需要重新计算所有的特殊数字 修改任何部分都以为着重新计算 规定，允许世界上的每一个人建造区块。每一个新建区块的人（找到了这个特殊数字 - SHA256值有30个零）都能获得奖励，对于新建区块的这部分人（矿工）来说 没有发送者信息，不需要签名 每一个新区块都会给整个币种增加新的虚拟（加密）货币 新建区块的过程又被称为“挖矿”：需要大量工作量并且可以向整个经济体注入新的货币 挖矿的工作是：接受交易信息，建造区块，把区块广播出去，然后得到新的钱作为奖励 对每个矿工来说，每个区块就像一个小彩票，所有人都在拼命快速猜数字，直到有一个幸运儿找到了一个特殊数字，使得整个区块的哈希值开头有许多个零，就能得到奖励。我记得有一个知乎答主给了一个形象的比喻，区块链就像一个拥有貌美如花女儿（区块）的国王，有很多的青年翘首以盼，而国王的方法是出了一道很难得题目让所有的青年计算（学习改变人生），谁算的快（在计算哈希值过程也可能是运气好）就能抱得美人归 对于想用这个系统来收付款的用户来说，他们不需要收听所有的交易，而只要收听矿工们广播出来的区块，然后更新到自己保存的区块链中就可以了 3、51%算力-共识攻击这里有一个小漏洞，因为网络的延迟或者有人在篡改区块链等因素，你作为一个收听网络广播的用户，如果同时接受到两条不同的区块链怎么办？其中的交易信息发生了冲突 注：区块链本身就是最终的大账本，发生交易的唯一方法就是把你的交易加入到大账本上。具体来说，就是让矿工把你的交易记录加入它新挖到的区块中，并把这个区块链接到区块链上。链表的纽带，当然就是工作量证明 对于上面的问题，用户的解决方案也比较简单：即，只保留最长的，也就是包含的工作量最大的那一条 用户保留最长的区块链 这里有一个Trick，即所谓信任工作量最大不仅仅是出【一道难题】，还通过等待多个区块的产生引入世界上所有矿工之间的博弈（吃瓜群众，坐看大戏，谁厉害我选谁，你们尽管斗） 个人观点：区块链的Idea最核心的创新就是从技术上把信任和贪婪画了等号。因为贪婪（希望去竞争建立区块的建立和交易费）所以信任（全网算力越大，用户越放心），这句话甚至带上了些许哲学和传奇的色彩 对于用户来说，是这样一种情景 如何更新本地区块链 其中的原因是，你可以假设Alice希望篡改一个交易信息，那么就意味着Alice需要不断的通过计算维护这个区块链了。也就是说每一次有新的区块链产生，Alice都需要不断的抢到这个彩票，理论上来说，他至少必须拥有全网51%以上的算力才能做到这一点，更多的，随着用户等待区块的增加，这个难度，幂次上升，在7-8个区块链产生后，概率上来讲，就是绝对信任 无穷大的篡改成本 此时 我们用数字签名保证了不能伪造交易记录 用区块链及工作量证明保证了不能篡改其中的信息 这两点，就完成了：证明区块链的每一条交易记录都是可信的这一终极目标 总结 - 系统可行性分析只需给出一个命题来思考：我们如何才能在这个系统下骗人呢？ 如果你想篡改一笔不存在的交易记录，那么你必须比所有人都算的快，赢得这个彩票 但所有用户会继续收听其他矿工的广播 所以为了让所有用户继续相信这个伪造的区块 你必须投入自己所有的工作量，不断给篡改后的区块链分叉增添新的区块 记住：根据协议，所有用户会一直信任他所知道的最长的链 是的，你持续的竞争过世界上所有的矿工的概率或者说代价，实在太大了，得不偿失（其实法律也是一样的道理，它强制给违法的人给予惩罚，让违法者付出他们不能承受的代价了保证公平和社会稳定运行） 注意，作为一个用户，你不能立马相信你所听到的最新区块，而是应该等待多几个区块被创建过后，再确认这的确是世界所有人都在使用的区块链 发明过程中的关键点 电子签名 Digital Signatures 公共账本就是货币 The Ledger is the currency 去中心化 Decentralize 工作量证明 Proof of work 区块链 Block Chain 比特币技术到这里，已经发明了比特币，解决了去中心化的信任这一难题。只对比特币和区块链是什么这个问题感兴趣的读者，可以停在这里了，希望大家可以在我的叙述中解决一些困惑！鞠躬！ 针对比特币的一些实现的内在细节，继续在探索和学习的道路上披荆斩棘。新技术，新点子，要拥抱它，使用它，判断它，必须先追根究底了解它。 我们知道区块链中记载的核心内容，对于比特币（加密货币）来说就是转账记录。但是，一个概念真正落地成大众可以用的服务，有很多技术上，协议上的细节。接下里的部分主要探讨一些比特币具体实现方面的细节，如网络节点构成，比特币的计算难度系数，比特币总量的由来，比特币一笔交易发生的内部细节等 比特币网络节点的构成比特币网络是一种点对点的数字现金系统（P2P），网络节点中每台机器都彼此对等。P2P网络不存在任何服务端、层级关系或者中心化服务。 节点类型与分工 一个全功能节点包含上述4个模块【钱包Wallet】【矿工Miner】【完整区块链full Block-chain database】【网络路由节点Network routing】 【网络路由节点】使得节点具有参与验证并传播交易与区块信息，发现监听并维持点对点的链接的能力 【完整区块链】具有此模块的节点被称为：全节点。它能够独自自主的校验所有交易，不需要任何其他信息。 【钱包】比特币的所有权是通过数字密钥、比特币地址和数字签名来确定的，数字密钥实际上并不是存储在网络中，而是由用户生成并存储在一个文件或简单的数据库中，称为钱包。有些节点仅仅保留区块链的一部分，通过一种”简易支付验证“（SPV Simplified Payment Verification）的方法来完成交易 【矿工】挖矿节点以相互竞争的方式创造新的区块。有一些挖矿节点也是全节点，可以独立挖矿；还有一些参与矿池挖矿的节点是轻量级节点，必须依赖矿池服务器维护全节点进行工作 拥有全部四个模块被称之为核心客户端（Bitcoin Core），除了这些主要节点类型外，还有一些服务器及节点运行其他协议，如特殊矿池挖矿协议、轻量级客户端访问协议。 下表为扩展比特币网络的不同节点类型 图示 名称 说明 独立矿工 具有完整区块链副本 完整区块链节点 此种节点有时有中继作用，不断收听网络广播，维护完整区块链 轻量(SPV)钱包 移动端，或者不想太过于笨重的桌面端，只需要进行交易广播操作 矿池协议服务器 将运行其他协议的节点，连接至P2P网络的网关路由器 挖矿节点 不具有区块链，但具备Stratum协议的节点或其他矿池挖矿协议的网络节点 轻量 Stratum 钱包 不具有区块链的钱包、运行Stratum协议的网络节点 扩展比特币网络要在全世界的网络中完成整个的交易，下图描述了一个扩展比特币网络，它包含了多重类型的节点、网关服务器、边缘路由器、钱包客户端以及它们互相连接所需要的各类协议，比特币互相连接的接口一般使用8333端口 可以参看这个文章了解Stratum协议，Stratum协议详解 扩展比特币网络 如何控制区块产生速度恒定难度系数我们在发明比特币的过程已经详细说明了工作量证明寻找一个特殊数字使得SHA256函数的输出字符串的前n位是零 对于每一种不同的加密货币来说，都有一个值需要在建立货币的时候时候被定义，即每一个新区块在当前全网算力的条件被发现的【平均时间】，这也是难度系数的由来 比特币10分钟；以太坊15秒；瑞波币3.5秒；莱特币2.5分钟 抛开代码算法层面来说，实现方法就是通过找前n位是0的方法。从概率角度来说，n值越大，意味找到这个这个数的解范围越小。 随着需求0的数目一个一个增加，需要的计算时间将会程指数增长。 那么肯定会问，这个难度值如何动态调整？由谁调整？ 难度调整方式难度的调整实在每个完整节点中自动发生的。如果网络发现区块产生速率比10分钟要快时会增加难度。如果发现比10分钟慢时则降低难度。 例如比特币中的是这样定义的：每2016个区块后计算生成它们花费的时长，比上20160（14天）调整一次。有人可能会问，如果在这十四天内计算能力暴涨怎么办，其实这个10分钟的区块新建间隔的规定也只是一个估计要求，真实情况下，这个时间会偏离10分钟这个设定值很多，但是这种偏差并不会对整个区块链的运行产生影响 比特币总量的由来我们已经知道，矿工没新建一个区块就可以得到一定数量的比特币作为奖励，最开始，一个区块可以得到50BTC的奖励，之后每210000个区块，奖励减半，直到2140年，所有的比特币将会发放完毕，可以得到公式$$Total = 210000 \\times(50 + 25 + 12.5 + \\ldots) = 20999999980 \\approx \\text{2100万}$$而这个规则不同的竞争币种都可以自由设置。但是因为交易费的存在，挖矿的人还是会有收益，否则无法建立新的区块，那么整个比特币网络就瘫痪了 比特币的交易处理能力现在比特币区块链的区块信息我现在直接从BLOCKCHAIN上，在我写下这句话的时候，最新的区块是情况 区块高度 存在时间 交易数量 交易额 创建人(矿池) 大小(KB) 重量(kWU) 486883 3分钟 2126 25992.38 BTC.TOP 999.34 3917.54 486882 23分钟 2422 33926.89 BTCC Pool 1034.39 3996.58 486881 51分钟 358 4480.22 BTC.TOP 191.92 718.14 486880 53分钟 352 3770.26 BTC.TOP 197.27 715.72 其中的重量是指的实际存储的大小，这个值和交易协议有关，其实可以忽略。非常幸运的是，这几个区块放佛是专门为讲解这一节而出现的，这可能是天意吧 另外插一句，你会发现平均区块建立间隔时间，的确和10分钟这个设计值差距很大吧 区块容量比特币从被创建时，或者说源代码中规定了，区块容量是1M。最初设计成1M的原因一方面，防止DOS攻击。另一方面，当年中本聪在创建区块链的时候的容量是32M，但是他通过一个说明为”Clear up“这样毫不起眼的Commit把区块容量改成了1M，为防止区块链体积增长过快，为区块容量这个问题添加了些神秘色彩。好吧，我承认，中本聪就已经非常具有神秘色彩了，是在神秘色彩上添上了些故事 通过上表我们知道，1M的容量意味着比特币最大的处理交易数量在约2400（486882区块1034.39的大小很接近了），在给出一张时间和每秒交易数量的关系图表(交互表格点击链接) 每秒比特币交易数量 这是一张对数图，纵坐标是每秒交易数量，横坐标是时间。其中，蓝色圆圈的大小代表的是比特币内存池（mempool）的大小（交易在等待矿工处理之前都会暂时存在这里）。 一句话总结，这是一个拥堵的网络，已经重负不堪。 再来看一张比特币交易费和区块使用率之间的关系图(交互表格可以点击链接) Bitcoin Fees VS BlockSize 蓝色的圈大小是Mempool的大小。横坐标是区块容量的使用情况，纵坐标是每一个区块的可以得到的交易费用。 手续费随区块使用率开始增长，甚至出现了4BTC一个区块2400笔交易的情况，意味着挖到这个区块的人通过交易费得到的汇报接近了本身建立区块的回报 有一个结论是，扩容后，因为每一个区块的交易承载量增加，矿工的交易费收入肯定会减少。因为，通过上表可以发现，只有当区块使用程度接近95%时候，交易费才有明显的增长 再看一张用户执行交易需要等待的时长和区块使用比例间关系的图表(交互图表点击链接) Bitcoin Median Confirmation Time VS Bloacksize 蓝色的圈大小是Mempool的大小。横坐标是区块容量的使用情况，纵坐标是用户平均需要等待的时间，单位是分钟。 通过上面三张表我们可以知道，矿工的计算力是整个区块链信用的基石（记住贪婪=信任），所以对矿工的激励不能少，而对于用户来说，当然希望自己的交易越快速完成越好。 对于矿工来说，区块使用率超过95%是一个很好的信号，那意味着我可以拿到更多的奖励。奖励太低，在区块建立奖励越来越少的情况下，安全性（信任）就慢慢的得不到保障。这么说来，这也就变成了一个Trade-off博弈过程 分析下来，类似门罗币（menero）实现的根据网络负载来动态调整区块容量的设计似乎很合理 比特币扩容之争这是一场复杂的博弈斗争，使用隔离见证增长区块容量，并出现了比特币现金这个新的币种。 如果想要了解这里面的很多技术，英文是必须过硬的，因为比特币代码开源，可以随意fork，只要英文功底过硬，阅读白皮书，文档等，这些不同技术的处理方法都是能够学到的 比特币的一笔交易过程中到底发生了什么我们可以确认的是，每一笔都将记录在大账本中，那么我们需要研究的内容，就是区块中交易内容内的具体数据结构 交易结构每一个交易块包含的内容如下表所示 大小 字段 描述 4 bytes 版本 明确这笔交易参照的规则 1 - 9 bytes 输入数量 输入值的数量 不定 输入 一个或多个交易输入 1 - 9 bytes 输出数量 输出值的数量 不定 输出 一个或多个交易输出 4 bytes 时钟时间 UNIX时间戳或区块号 最后这个值是解锁时间，定义了能被加到区块链里的最早交易时间。大多是时候设为0，表示立马执行。 一笔比特币交易是一个含有输入值和输出值的数据结构。该数据结构包含了将一笔资金从初始点（输入值）转移至目标地址（输出值）的代码信息。比特币交易的输入值和输出值与账号或者身份信息无关。可以把它理解为一种被特定秘密信息锁定的一定数量的比特币。只有拥有者或者知道这个秘密信息的人可以解锁 交易的输入和输出比特币交易的基本单位是未经使用的一个交易输出，简称UTXO（unspent transaction outputs） 可以把UTXO类比为我们使用的人民币1，5，10，20，50，100的面值，对于UTXO来说，它的面值可以是一”聪“的任意倍数（1BTC等于一亿聪）但是这个有着任意面值的”人民币“不能随意打开，还被加上一道类似红包支付口令的密码，只有拥有这个密码的人才可以使用这个UTXO，UTXO包含，币值+一段代码（锁，只有有钥匙的人才能打开） 被交易消耗的UTXO被称为交易输入，由交易创建的UTXO被称为交易输出 交易输出不同面值的UTXO是由交易输出来提供的。你可以想象你需要购买一个3.1BTC的物品，你并不能从你的钱包中找到几个UTXO来得到3.1BTC，但是你刚好拥有一个4BTC的UTXO，你使用这个UTXO作为付款，那么你需要自己手动构建一个0.9的UTXO返还给你自己。 一个交易输出包含两个部分 一定量的比特币。被命名为“聪”（satoshis） 一个锁定脚本。给这个UTXO上锁，保证只有收款人地址的私钥才可以打开 交易输入每个交易输入是在构造的一笔交易（使用UTXO），比如你需要支付0.015BTC，钱包会寻找一个0.01BTC和0.005BTC的UTXO来组成这一笔交易。交易输入中还会包含一个解锁脚本，这是一个签名，可以类比成支付宝红包密码的口令 交易费交易费 = 求和（所有输入） - 求和（所有输出） 这里有一个比较有意思的地方，就是因为找零的输出UTXO是交易的发起这自己构建的，如果很不幸，你忘记了自己构建找零的UTXO，那么这些多余的BTC就会变成矿工的劳务费 例如，我需要和小明进行交易，需要购买一个商品，花费0.8BTC，为了确保这笔交易能被更快的处理（添加到大账本上），我要在其中添加一笔交易费，假设0.01BTC（忽略人傻钱多），那就意味着这笔交易会需要我从钱包中找到几个UTXO能组成0.81BTC。但如果我的钱包内找不出这样的UTXO，只有一个1BTC的UTXO可用，那么我就需要构建一个0.19BTC的UTXO作为找零回到自己的钱包 交易费只和交易字段使用的字节大小有关，与参与交易的比特币币值无关。UTXO是有尺寸的，比如某人想支付一笔很大的BTC交易，但是他的钱包中有很多小尺寸的UTXO，如果加入了很多个UTXO，就以为这他的交易会变复杂且尺寸大。 解锁和锁定脚本在实际实现的时候，这个“支付宝红包口令”被称为脚本，是一种基于逆波兰表示法的基于堆栈的执行语言。具体细节感兴趣的读者可以去比特币的Github研究代码。关于脚本有很多细节上的定义和实现方法，这里限于篇幅不展开描述了 矿工费和优先级我们知道，每一笔交易都是广播到区块链上，由矿工决定是不是加入到新区块上的。那么这里就会涉及到一个问题，谁的交易的优先级更高，是先来后到？还是谁给前多谁就能加入到新区块中？ 在区块容量一节中，有一张图表直观的展示了现在网络中一笔交易的等待时间，其中最长的，也就是30分钟，如果你不是一个超级急性子，很多时候还是可以接受的（毕竟跨国转账1-2个工作日） 优先级 = 输入值块龄 * 输出值块龄 / 交易总长度 一个交易想成为“较高优先级”，需满足条件：优先值大于57600000，等价于1个BTC，年龄为1天，交易的大小为250字节 区块中前50KB的字节是保留给“较高优先级”的，其实这一机制也保证了一笔交易不会等待时间无现长。但是我们要知道，内存池（存放待处理交易的位置）中的交易，如果在没有处理后消失，所以钱包必须拥有不断重新广播未被处理交易的功能 创币交易 - Coinbase每一个新建立的区块，都会有新的比特币作为奖励被产生，这个交易是一个特殊交易，被称为创币交易（Coinbase奖励） 创币交易中不存在解锁脚本（也叫ScriptSig字段），被Coinbase的数据取代，长度最小2字节，最大100字节，除了开始的几个自己以外，矿工可以任意使用Coinbase的其他部分。比如创世区块中，Coinbase的输入中的字段是：The Times 03/Jan/2009 Chancellor on brink of second bailout for banks，是泰晤士日报当天的头版文章标题：财政大臣将再次对银行施以援手。 Merkle树每个区块中的所有交易，都是用Merkle树来表示的。换句话说，交易的存储数据结构是，Merkle树 什么是Merkle树Merkle树是一种哈希二叉树，它可以用来进行快速查找和检验大规模数据完整性。对于比特币网络来说，使用Merkle树来存储交易信息的目的是为了高效的查找和校验某笔交易的信息是否存在 当N个数据元素经过加密（使用两次SHA256算法，也称double-SHA256），至多计算 $2log_2(N)$ 次就能检查出任意某元素是否在树中 构造Merkle树假设我们有A B C D四笔交易字段，首先需要把这四个数据Hash化。然后把这些哈细化的数据通过串联相邻叶子节点的哈希值然后哈希化。基本过程如下图所示 Merkle树的构造过程 叶子节点必须是偶数（平衡树），如果遇到奇数的情况，把最后一个节点自身复制一个，凑偶 Merkle树的效率下表显示了证明区块中存在某笔交易所需转化为Merkle路径的数据量 交易数量 区块的近似大小 路径大小（哈希数量） 路径大小（字节） 16 4KB 4个哈希 128 bytes 512 128KB 9个哈希 288 bytes 2048 512KB 11个哈希 352 bytes 65535 16MB 16个哈希 512 bytes 可以发现，即使区块容量达到16MB规模，为证明交易存在的Merkle路径长度增长也极其缓慢（幂指数增长取对数变为线性增长） Merkle应用 - 简单支付验证节点（SPV）我们知道，每当一笔新的交易产生的时候，我们必须验证这笔交易是否真的存在，在SPV节点中，不保存区块链，仅仅保存区块头。使用认证路径或者Merkle路径来验证交易是否存在于区块中 例如，一个SPV节点需要处理一笔支付，它需要验证这笔交易在某个区块中是否存在，才能决定是不是把这笔交易添加到这个区块中，那么它只需要接收少于1KB大小的，有关区块头和Merkle路径的信息，比接收完整区块（大约1MB）大小少了1千倍。简单来说，可以想象，Merkle树类似一个数组（这也是哈希表的最简单表示），下标是区块字段，下标对应数组存储的内容是这笔交易是否存在的值（True or False） 区块链（比特币）与金融因为比特币具有价值，那就必须谈到它和金融的关系。 限于篇幅（太长了太可怕了，一个博客写2万字莫不是有病），这部分另起一篇：链接（直接点不会打开新标签），如果对ICO和金融方面感兴趣的读者欢迎移步讨论 竞争币和其他技术创新所谓竞争币当然是利用区块链技术为即使，仿照比特币的基本协议架构进行的创新后的新币种，或者是新的区块链实现模式。这篇文章围绕什么是区块链展开，这部分的内容请移步（持续更新）如何评估竞争币的价值与新技术创新（直接点不会打开新标签） 其中谈到了工作量证明的其他替代手段；到底什么是智能合约；以太坊开发技术栈等 总结感谢您看到这里，写这篇文章的目的一方面也是回答区块链（比特币）到底是什么这个困扰了自己很久的问题，另一方面，也是因为最近区块链技术非常火，需要一些接地气的科普文 比如最近最新的消息称一家保险公司，使用区块链技术来赔偿航班晚点2小时，基于以太坊智能合约第一款落地应用。 如果你已经对区块链的实现原理有了初步的认知，就明白这些应用利用了区块链的分布式特点。说到底，并没有贪婪=信任工作量证明核心，只是一种基于云的新型运用，也很有想法，但是和比特币之类就没什么关系了（当然，这是我的个人看法，最近区块链方面的有很多突破性技术，比如侧链，闪电网络等等，太多的新概念，新名词，新技术，对此，也只能不断学习）。 但你只需牢记，贪婪=信任，以太坊也是利用以太币这个媒介来实现了合约价值，中本聪用人内核的贪婪来给陌生人之间加上了信任的纽带，这个代价是永远不会变的。换句话说，如何抵抗共识攻击和安全漏洞是一个永远不会消失的议题。 一句话来说，万变不离其宗，道生一，一生二，二生三，三生万物。中本聪给了道，是个妙人，但是万物依旧有无穷可能。信任作为一个人类社会一直以来的重要问题（痛点），为了解决它，出现了权威机构进行信任背书（中心化）。建立信任，一定要付出代价，天底下没有免费的午餐，最终这些技术都会回归于一个投入产出的博弈过程（Trade-off），梳理主干，抓住要点，才能游刃有余！ 那么如何才能梳理主干，抓住要点，提升学习能力呢？见谅加一个软广告 幕布是一款笔记本软件，博主参与了部分研发工作，如果你喜爱沉浸式层次化输入，并喜欢思维导图，还喜爱记录总结整理各种书籍或文章，幕布完美切合这三类人群的需求：一键生成思维导图，极简输入界面，快捷键操作，全平台支持！Organize your brain by mubu 点我一键微信注册，一个月9块钱，良心商家，这是一篇自己写的有关幕布的介绍文章 如果感觉看完有帮助求知乎点个赞，感谢！ 以上！鞠躬！ 除了江卓尔 知乎回答等优秀的知乎答主的回答，附参考文献出处：【1】文章中引用多个Gif的出处：比特币原理-3B1B，这也是让我真正弄懂比特币的一个视频，不得不说，外国人在让门外汉入行这件事上，领先了很多【2】一本入门教材。包含代码和实现，以及很多数据结构，具体实现方式的细节，如果想成为加密货币（区块链）开发者，这本书5星推荐：精通比特币【3】宋老师的鸿观125期 （需要优酷会员）【4】只能膜拜之的创世区块作者的论文：比特币白皮书【5】ICO科普文章中的例子引用","tags":[{"name":"BlockChain","slug":"BlockChain","permalink":"https://charlesliuyx.github.io/tags/BlockChain/"},{"name":"BTCoin","slug":"BTCoin","permalink":"https://charlesliuyx.github.io/tags/BTCoin/"}]},{"title":"【直观详解】拉格朗日乘法和KKT条件","date":"2017-09-20T17:49:49.000Z","path":"2017/09/20/拉格朗日乘法和KKT条件/","text":"【阅读时间】8min - 10mun【内容简介】直观的解读了什么是拉格朗日乘子法，以及如何求解拉格朗日方程，并且给出几个直观的例子，针对不等式约束解读了KKT条件的必要条件和充分条件 What &amp; Why拉格朗日乘法（Lagrange multiplier）是一种在最优化的问题中寻找多元函数在其变量受到一个或多个条件的相等约束时的求局部极值的方法。这种方法可以将一个有 n 个变量和 k 个约束条件的最优化问题转换为一个解有 n+k 个变量的方程组的解的问题 考虑一个最优化问题 $$ \\operatorname*{max}_{x,y} f(x,y) \\qquad s.t.\\;\\; g(x,y)=c $$ 为了求 $x$ 和 $y$ ，引入一个新的变量 $\\lambda$ 称为拉格朗日乘数，再引入朗格朗日函数的极值$$\\mathcal{L}(x,y,\\lambda)=f(x,y)-\\lambda \\cdot \\bigl( g(x,y) - c\\bigl) \\tag 1$$ 红线表示 $g(x,y) = c$ ，蓝线是 $f(x,y)$ 的等高线，所有箭头表示梯度下降最快的方向。图中红线与等高线相切的位置就是待求的极大值 How那么如何求这个极值点呢？ 单约束对(1)式直接求微分，并令其为零，计算出鞍点 $$ \\nabla_{x,y,\\lambda} \\mathcal{L}(x,y,\\lambda) = 0 $$ 有三个未知数，所以需要3个方程。求 $\\lambda$ 的偏微分有 $\\nabla_{\\lambda} \\mathcal{L}(x,y,\\lambda) = 0 \\implies g(x,y)=0$，则总结得 $$ \\nabla_{x,y,\\lambda} \\mathcal{L}(x,y,\\lambda) = 0 \\iff \\begin{cases} \\nabla_{x,y} f(x,y) = \\lambda \\nabla_{x,y} g(x,y) \\\\ g(x,y)=0 \\end{cases} $$ 例子1设一个具体的例子，我们需要求下列问题 $$ \\operatorname*{max}_{x,y} f(x,y) = x^2y \\qquad s.t.\\;\\; g(x,y): x^2+y^2-3=0 $$ 只有一个约束，使用一个乘子，设为 $\\lambda$，列出拉格朗日函数 $$ \\mathcal{L}(x,y,\\lambda)=f(x,y)-\\lambda \\cdot \\bigl( g(x,y) - c\\bigl) = x^2y + \\lambda(x^2+y^2-3) $$ 接下来求解上式，分别对三个待求量偏微分 $$ \\begin{align} \\nabla_{x,y,\\lambda} \\mathcal{L}(x,y,\\lambda) & = \\left( \\frac{\\partial \\mathcal{L}}{\\partial x},\\frac{\\partial \\mathcal{L}}{\\partial y},\\frac{\\partial \\mathcal{L}}{\\partial \\lambda}\\right)\\\\ & = (2xy + 2\\lambda x, x^2 + 2\\lambda y, x^2 + y^2 - 3) \\end{align} $$ 令偏微分分别等于0，得到 $$ \\nabla_{x,y,\\lambda} \\mathcal{L}(x,y,\\lambda) = 0 \\iff \\begin{cases} 2xy+2\\lambda x = 0 \\\\ x^2 + 2\\lambda y = 0 \\\\ x^2 + y^2 - 3 = 0 \\end{cases} \\iff \\begin{cases} x(y + \\lambda) = 0 & (i)\\\\ x^2 = -2\\lambda y & (ii)\\\\ x^2 +y^2 = 3 & (iii) \\end{cases} $$ 根据上式，我们可以解得 $\\mathcal{L}$: $$ (\\pm \\sqrt{2},1,-1 ); (\\pm \\sqrt{2},-1,1 );(0,\\pm \\sqrt{3},0) $$ 根据几个不同的解带入 $f(x,y)$ 得到，2，-2，0，也就是我们需要的最大值，最小值，对应的直观图像解释如下图所示（非常直观的展现约束和等高线的含义） 例子2关于拉格朗日乘子法的应用，有一个十分著名的：求离散概率分布 $p_1,p_2,\\cdots,p_n$ 的最大信息熵 $$ f(p1,p2,\\cdots,p_n) = - \\sum_{j=1}^n p_j log_2{p_j} \\\\ s.t. \\quad g(p1,p2,\\cdots,p_n) = \\sum_{k=1}^n p_k = 1 \\text{（概率和为1）} $$ 单约束问题，引入一个乘子 $\\lambda$ ，对于 $k \\in [1,n]$ ，要求 $$ \\frac{\\partial}{\\partial p_k} (f + \\lambda(g - 1)) = 0 $$ 将 $f$ 和 $g$ 带入有 $$ \\frac{\\partial}{\\partial p_k} \\left( -\\sum_{k=1}^np_klog_2{p_k} + \\lambda (\\sum_{k=1}^n p_k - 1)\\right) = 0 $$ 计算这 n 个等式的偏微分，我们可以得到： $$ -\\left( \\frac{1}{\\ln(2)} + log_2p_k \\right) + \\lambda = 0 $$ 这说明所有的 $p_i$ 都相等，所以得到 $p_k = \\frac{1}{n}$ 我们可以得到一个结论是：均匀分布的信息熵是最大的 多约束既然可以解决单约束，继续思考一下多约束情况的直观表现，假设我们的约束是两条线，如下图所示 和单约束的解决方法类似，我们画出等高线图，目的就是在约束线上找到一个点可以和等高线相切，所得的值实在约束范围内的最大值或者最小值，直观表示如下图 解算方法是讲单约束的扩展到多约束的情况，较为类似，可举一反三 KKT条件已经解决的在等式约束条件下的求函数极值的问题，那不等式约束条件下，应该如何解决呢？ 这就需要引出KKT条件（Karush-Kuhn-Tucker Conditions），它是在满足一些有规则的条件下，一个非线性规划问题能有最优化解法的一个必要和充分条件 考虑以下非线性最优化问题，含有 $m$ 个不等式约束，$l$ 个等式约束$$\\operatorname*{min}_{x}f(x) \\qquad s.t. \\; g_i(x) \\leqslant 0,\\; h_j(x) =0$$ 必要条件假设 $f,g_i,h_j$ 三个函数为实数集映射，再者，他们都在 $x^$ 这点连续可微，如果 $x^$ 是一个局部极值，那么将会存在一组称为乘子的常数 $\\lambda \\geqslant 0,\\mu_i \\geqslant0, \\nu_j$ 令 $$ \\lambda + \\sum_{i=1}^m \\mu_i + \\sum_{j=1}^l |\\nu_i| \\gt 0, \\\\ \\lambda \\nabla f(x^*) + \\sum_{i=1}^m \\mu_i \\nabla g_i(x^*) + \\sum_{j=1}^l \\nu_i \\nabla h_j(x^*) = 0, \\\\ \\mu_i g_i(x^*) =0 \\; \\text{for all} \\; i=1,\\ldots,m $$ 这里有一些正则性条件或约束规范能保证解法不是退化的（比如$\\lambda$为0），详见 充分条件假设 $f,g_i$ 为凸函数，$h_j$ 函数是仿射函数（平移变换），假设有一个可行点 $x^*$，如果有常数 $\\mu_i \\geqslant 0$ 及 $\\nu_j$ 满足 $$ \\nabla f(x^*) + \\sum_{i=1}^m \\mu_i \\nabla g_i(x^*) + \\sum_{j=1}^l \\nu_i \\nabla h_j(x^*) = 0 \\\\ \\mu_i g_i(x^*) =0 \\; \\text{for all} \\; i=1,\\ldots,m $$ 那么 $x^*$ 就是全局极小值 总结总的来说，拉格朗日乘子法是一个工具（手段或方法），来解决在有约束情况的求函数极值的问题","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"}]},{"title":"【直观详解】支持向量机SVM","date":"2017-09-20T01:23:39.000Z","path":"2017/09/19/支持向量机SVM学习笔记/","text":"【阅读时间】13min - 19min【内容简介】详解解读什么是支持向量机，如何解支持向量以及涉及的拉普拉斯乘子法，还有核方法的解读 什么是支持向量机-SVM支持向量机-SVM(Support Vector Machine)从本质来说是一种：用一条线（方程）分类两种事物 SVM的任务是找到这条分界线使得它到两边的margin都最大，注意，这里的横坐标是 $x_1$ 纵坐标为 $x_2$，如下图所示 margin 有了直观的感知，在定义这一节在做一些深入的思考，分解名词（Support Vector Machine）并尝试解释： Machine - Classification Machine 说明它的本质是一个分类器 Support Vector - 如上图所示，在Maximum Margin上的这些点就是支持向量，具体说即最终分类器表达式中只含有这些支持向量的信息，而与其他数据点无关。在下面的公式中，只有支持向量的系数 $\\alpha_i$ 不等于0。说人话，上图中两个红色的点，一个蓝色的点，合起来就是支持向量 $$ \\mathbf w \\cdot \\varphi (\\mathbf x) = \\sum_i \\lambda_i y_i k(\\mathbf x_i,\\mathbf x) $$ 公式中每一个符号的含义在后文有说明 如何求解支持向量机对于我们需要求解的这个超平面（直线）来说，我们知道 它离两边一样远（待分类的两个部分的样本点） 最近的距离就是到支持向量中的点的距离 根据这两点，抽象SVM的直接表达（Directly Representation） 注：$arg \\operatorname*{max}_{x} f(x)$ 表示当 $f(x)$ 取最大值时，x的取值 $$ arg \\operatorname*{max}_{boundary} margin(boundary) \\\\ \\text{所有正确归类的两类到boundary的距离} \\ge margin \\tag{1} $$ 其实这个公式是一点也不抽象，需要更进一步的用符号来表达。 我们知道在准确描述世界运行的规律这件事上，数学比文字要准确并且无歧义的多，文字（例子）直观啰嗦，数学（公式）准确简介 硬间隔 SVM支持向量机 注：公式中加粗或者带有向量箭头的都表达一个向量 假设这些数据线性可分，也可称为硬间隔（Hard Margin） 首先定义超平面：$\\mathbf w^T \\vec x_i + b = 0$，接下来为了方便，设 $\\vec x = (x_1,x_2)$ 即一条直线 任意点 $\\vec x_i$ 到该直线的距离为 $\\frac{1}{\\lVert \\mathbf w \\lVert} (\\mathbf w^T \\vec x_i + b)$ 对于空间内所有训练点的坐标记为 $(\\vec x_i,y_i)$，其中 $y_i$ = 1 or -1， 表示点 $\\vec x_i$ 所属的类 如果这些训练数据是线性可分的，选出两条直线（上图中的虚线），使得他们的距离尽可能的大，这两条直线的中央就是待求的超平面（直线） 为了表达直观，我们定义这两个超平面（直线）分别为 $\\mathbf w^T \\vec x_i + b = 1$ 和 $\\mathbf w^T \\vec x_i + b = -1$，两个超平面（直线）之间的距离为 $\\gamma = \\frac{2}{\\lVert \\mathbf w \\lVert}$ 注：选择1的好处是，w 和b进行尺缩变换（kw和kb）不改变距离，方便计算 为了使得所有样本数据都在间隔区（两条虚线）以外，我们需要保证对于所有的 $i$ 满足下列的条件 $\\mathbf w^T \\vec x_i + b \\geqslant 1$ 若 $y_i = 1$ $\\mathbf w^T \\vec x_i + b \\leqslant -1$ 若 $y_i = -1$ 上述两个条件可以写作 $y_i(\\mathbf w^T \\vec x_i + b) \\geqslant 1, \\;\\text{for all 1}\\; 1\\leqslant i \\leqslant n$ 这里的n指样本点的数量 上面的表达（Directly Representation）可以被写成 $$ arg \\operatorname*{max}_{\\mathbf w,b} \\left\\{ {\\frac{1}{\\lVert \\mathbf w \\lVert} \\operatorname*{min}_{n} [y_i(\\mathbf w^T\\vec x_i}+b)]\\right\\} \\tag{2} $$ 最终目的是找到具有“最大间隔”（Maximum Margin）的划分超平面（直线），找到参数 $\\mathbf w$ 和 $b$ 使得 $\\gamma$ 最大 则可以对(2)式进行形式变换，得到 canonical representation $$ arg \\operatorname*{max}_{\\mathbf w,b} \\frac{2}{\\lVert \\mathbf w \\lVert} \\implies arg \\operatorname*{min}_{\\mathbf w,b} \\frac{1}{2}\\lVert \\mathbf w \\lVert ^2 \\\\ s.t.\\; y_i(\\mathbf w^T\\vec x_i+b) \\geqslant1,\\;i = 1,2,\\ldots,m \\tag{3} $$ 注：s.t. ：subject to 表示约束条件，表达的意思等价于：为了使得所有样本数据都在间隔区（两条虚线）以外 为了解(3)式，需要用到拉格朗日乘子法（Method of lagrange multiplier），它是用来求解在约束条件目标函数的极值的，详细直观详解 注：以下解算过程希望完全看懂强烈建议理解阅读详细直观详解，很多地方推导过程只写必要过程及结论 根据约束的形式，我们引入m个拉格朗日乗法子，记为 $\\boldsymbol \\lambda = (\\lambda_1,\\ldots,\\lambda_m)^T$ ，原因是，有m个约束，所以需要m个拉格朗日乗法子。可以得出拉格朗日方程如下： $$ \\mathcal{L}(\\mathbf w,b,\\boldsymbol \\lambda) = \\frac{1}{2}\\lVert \\mathbf w \\lVert ^2 - \\sum_{i=1}^m \\lambda_i \\{ y_i(\\mathbf w^T\\vec x_i+b) -1 \\} \\tag{4} $$ 解这个拉格朗日方程，对 $\\mathbf w$ 和 $b$ 求偏导数，可以得到以下两个条件 $$ \\mathbf w = \\sum_{i=1}^m \\lambda_i y_i \\vec x_i \\\\ 0 = \\sum_{i=1}^m \\lambda_i y_i $$ 将这两个条件带回公式(4)，可以得到对偶形式（dual representaiton），我们的目的也变为最大化 $\\mathcal{L}(\\boldsymbol \\lambda)$，表达式如下 $$ arg \\operatorname*{max}_{\\boldsymbol \\lambda}\\mathcal{L}(\\boldsymbol \\lambda) = \\sum_{i=1}^m \\lambda_i - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\lambda_i \\lambda_j \\vec x_i \\vec x_j \\mathbf x_i^T \\mathbf x_j \\\\ s.t. \\qquad \\lambda_i \\geqslant 0, \\forall i\\;;\\quad \\sum_{i=1}^m \\lambda_i y_i = 0 \\tag{5} $$ 以上表达式可以通过二次规划算法解出 $\\boldsymbol \\lambda$ 后，带回，求出$\\mathbf w$ 和 $b$，即可得到模型 $$ f(\\mathbf x) = \\mathbf w^T\\mathbf x + b = \\sum_{i=1}^m \\lambda_i y_i \\mathbf x_i^T \\mathbf x + b \\tag{6} $$ 补充一些关于二次规划算法的相关，(3)式的约束是一个不等式约束，所以我们可以使用KKT条件得到三个条件： $$ \\lambda_i \\geqslant0 ;\\quad y_i f(\\mathbf x_i)-1 \\geqslant0; \\quad \\lambda_i\\{ y_i f(\\mathbf x_i)-1 \\}=0 $$ 使用这些条件，可以构建高效算法来解这个方程，比如SMO（Sequential Minimal Optimization）就是其中一个比较著名的。至于SMO是如何做的，考虑到现代很多SVM的Pakage都是直接拿来用，秉承着前人付出了努力造了轮子就不重复造的核心精神，直接调用就好 软间隔已经说明了如何求得方程，以上的推导形式都是建立在样本数据线性可分的基础上，如果样本数据你中有我我中有你（线性不可分），应该如何处理呢？这里就需要引入软间隔（Soft Margin），意味着，允许支持向量机在一定程度上出错 由上一节我们得知，约束为： $y_i(\\mathbf w^T\\vec x_i+b) \\geqslant1,\\;i = 1,2,\\ldots,m$ ，目标是使目标函数可以在一定程度不满足这个约束条件，我们引入常数 $C$ 和 损失函数 $\\ell_{0/1}(z)$ 为0/1损失函数，当z小于0函数值为1，否则函数值为0 $$ \\operatorname*{min}_{\\mathbf w,b} \\frac{1}{2}\\lVert w \\lVert^2 + C \\sum_{i=1}^m \\ell_{0/1}(y_i(\\mathbf w^T\\vec x_i+b) -1) \\tag {7} $$ 对于(7)式来说 $C \\geqslant 0$ 是个常数，当C无穷大时，迫使所有样本均满足约束；当C取有限值时，允许一些样本不满足约束 但 $\\ell_{0/1}(z)$ 损失函数非凸、非连续，数学性质不好，不易直接求解，我们用其他一些函数来代替它，叫做替代损失函数（surrogate loss） $$ \\begin{align} & \\text{hinge损失:} \\ell_{hinge}(z) = max(0,1-z)\\\\ & \\text{指数损失:} \\ell_{exp}(z) = e^{-z}\\\\ & \\text{对数损失:} \\ell_{log}(z) = log(1+e^{-z})\\\\ \\end{align} $$ 三种常见损失函数如下图 为了书写方便，我们引入松弛变量（slack variables）: $\\xi_i \\geqslant 0$，可将(7)式重写为 $$ \\operatorname*{min}_{\\mathbf w,b,\\xi_i} \\frac{1}{2}\\lVert w \\lVert^2 + C \\sum_{i=1}^m \\xi_i \\\\ s.t. \\quad y_i(\\mathbf w^T\\vec x_i+b) \\geqslant 1 - \\xi_i ;\\; \\xi_i \\geqslant 0,\\; i = 1,2,\\ldots,m \\tag{8} $$ (8)式就是常见的软间隔支持向量机，其中，每一个样本都有一个对应的松弛变量，用以表征该样本不满足约束的程度，求解的方法同理硬间隔支持向量机 支持向量机扩展核方法以上我们求解的支持向量机都是在线性情况下的，那么非线性情况下如何处理？这里就引入：核方法 对于这样的问题，可以将样本从原始空间映射到一个更高为的特征空间，使得样本在这个特征空间内线性可分，直观可视化解释 为了完成这个目的，令 $\\phi(\\mathbf x)$ 表示将 $\\mathbf x$ 映射后的特征向量，于是，在特征空间划分超平面所对应的模型可表示为： $$ f(\\mathbf x) = \\mathbf w^T \\phi(\\mathbf x) + b $$ 同理上文中引入拉格朗日乘子，求解整个方程后可得 $$ \\begin{align} f(\\mathbf x) &= \\mathbf w^T \\phi(\\mathbf x) + b \\\\ &= \\sum_{i=1}^m \\lambda_i y_i \\phi(\\mathbf x_i)^T \\phi(\\mathbf x) + b \\\\ &= \\sum_{i=1}^m \\lambda_i y_i k(\\mathbf x,\\mathbf x_i)+ b \\end{align} $$ 这里的函数 $k(\\cdot,\\cdot)$ 就是核函数（kernel function），常见的核函数见下表 名称 表达式 参数 线性核 $\\boldsymbol x_i^T \\boldsymbol x_j$ 无 多项式核 $(\\boldsymbol x_i^T \\boldsymbol x_j)^d$ $d \\geqslant 1$ 多项式次数 高斯核 $exp(-\\frac{\\lVert\\boldsymbol x_i - \\boldsymbol x_j \\lVert^2}{2\\sigma^2})$ $\\sigma&gt;0$ 高斯核带宽 拉普拉斯核 $exp(-\\frac{\\lVert\\boldsymbol x_i - \\boldsymbol x_j \\lVert^2}{\\sigma})$ $\\sigma&gt;0$ Sigmoid核 $tanh(\\beta \\boldsymbol x_i^T\\boldsymbol x_j + \\theta)$ $\\beta&gt;0$ $\\theta&gt;0$ 也可以通过函数组合得到这些值 多类问题多类问题可以使用两两做支持向量机，再由所有的支持向量机投票选出这个类别的归属，被称为one-versus-one approace。 Reference知乎各类回答Wiki百科PRML周志华-机器学习","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"}]},{"title":"Dota2伤害类型详解","date":"2017-09-18T21:31:12.000Z","path":"2017/09/18/Dota2伤害类型详解/","text":"【阅读时间】5min 百科类【内容简介】有关Dota2所有伤害来源的总结和互相作用总结，方便查阅 伤害来源攻击伤害主要来自于普通物理攻击 - 平A 具体计算方式 技能伤害详情参见 包含了所有来自技能的伤害，三种类型：魔法，物理和纯粹 伤害类型互相作用机制表格 游戏机制 物理攻击 物理技能 魔法伤害 纯粹伤害 护甲 降低 降低 正常 正常 伤害格挡 降低 正常^1 正常 正常 魔法抗性 正常 正常 降低 正常 虚无 没有效果 美国效果 降低 正常 闪避 可能落空 正常 正常 降低 致盲 可能落空 正常 正常 正常 伤害加深^2 加深 加深 加深 加深 伤害减免^2 降低 降低 降低 降低 伤害无效化^2 没有效果 没有效果 没有效果 没有效果 魔法伤害护盾 正常 正常 没有效果 正常 无敌 没有效果 没有效果 没有效果 没有效果 技能免疫 正常 不定 不定 不定 物理与护甲和伤害格挡有关，虚无和一些技能可以造成物理免疫，召唤单位和守卫的平A也是物理攻击 物理免疫技能炼金术士：酸性喷雾 炼金术士：不稳定化合物 敌法师：法力损毁 兽王：野性飞斧 赏金猎人：暗影步 钢背兽：针刺扫射 人马：反击 克林克次：灼热之箭 戴泽：剧毒之触 戴泽：暗影波 死亡先知：驱使恶灵 龙骑士：古龙形态溅射 上古巨神：裂地沟壑 上古巨神：回音重踏 灰烬之灵：无影拳 主宰：无敌斩 昆卡：潮汐使者 拉席克：恶魔赦令 噬魂鬼：盛宴 露娜：月刃 马格纳斯：加强力量溅射 司夜刺客：复仇 剃刀：风暴之眼 力丸：背刺 斯拉达：鱼人碎击 斯拉达：重击 狙击手：爆头 熊灵：缠绕之爪 斯温：巨力挥舞 圣堂刺客：隐匿 潮汐猎人：锚机 熊战士：怒意狂击 冥界亚龙：幽冥剧毒 编织者：虫群 魔法大多数技能都是魔法伤害，虚无状态会承受更多伤害 在纯粹和物理技能中未提及的都是魔法伤害 纯粹纯粹伤害能作用与技能免疫单位，不能作用于无敌单位 斧王-反击螺旋 祸乱之源：蚀脑 祸乱之源：噩梦 嗜血狂魔：血之祭祀 嗜血狂魔：割裂 陈：忠诚考验 末日使者：末日 魅惑魔女：推进 谜团：午夜凋零 谜团：黑洞 祈求者：炎阳冲击 杰奇洛：A烈焰焚身 莉娜：A神灭斩 美杜莎：（石化）秘术异蛇 司夜刺客：尖刺外壳 全能骑士：洗礼 殁境神蚀者：奥术天球 帕吉：肉钩 痛苦女王：超声冲击波 沉默术士：智慧之刃 幽鬼：荒芜 圣堂刺客：灵能之刃 伐木机：锯齿飞轮 伐木机：伐木锯链 伐木机：死亡旋风 修补匠：激光 骨灰 标记是一种特殊标记，为的是与其他技能区分开来 生命移除标记某些生命移除标记的技能可以立即杀死幻想 干扰者：恶念瞥视 莱恩：妖术 莱恩：法力抽取 美杜莎：石化凝视 帕格纳：生命吸取 羊刀：变羊 暗影萨满：变羊 有些技能利用生命移除来制造生命消耗效果，通常都是非致命伤害，伤害类型也是纯粹，被标记为生命移除 臂章：扣血 哈斯卡：沸血之矛对自身 艾欧：过载 凤凰：凤凰冲击 凤凰：烈火精灵 凤凰：烈日炙烤 魂戒：献身 工程师：自爆起飞 不朽尸王：噬魂 不反弹标记不反弹标记会使得一些受到伤害事件不会与带有不反弹标记的伤害相互作用，这防止了无限伤害循环( 刃甲：反弹伤害 司夜刺客：尖刺外壳 冥界亚龙：腐蚀皮肤 术士：致命链接","tags":[{"name":"Dota2","slug":"Dota2","permalink":"https://charlesliuyx.github.io/tags/Dota2/"},{"name":"Wiki","slug":"Wiki","permalink":"https://charlesliuyx.github.io/tags/Wiki/"}]},{"title":"【直观详解】机器学习分类器性能指标详解","date":"2017-09-12T20:05:32.000Z","path":"2017/09/12/机器学习分类器性能指标详解/","text":"【阅读时间】16 - 26 min【内容简介】系统详解分类器性能指标，什么是准确率 - Accuracy、精确率 - Precision、召回率 - Recall、F1值、ROC曲线、AUC曲线、误差 - Error、偏差 - Bias、方差 - Variance及Bias-Variance Tradeoff 在任何领域，评估（Evaluation）都是一项很重要的工作。在Machine Learning领域，定义了许多概念并有很多手段进行评估工作 混淆矩阵 - Confusion Matrix准确率定义：对于给定的测试数据集，分类器正确分类的样本数与总样本数的之比 通过准确率，的确可以在一些场合，从某种意义上得到一个分类器是否有效，但它并不总是能有效的评价一个分类器的工作。一个例子，Google抓取了100个特殊页面，它的索引中有10000000页面。随机抽取一个页面，这是不是特殊页面呢？如果我们的分类器确定一个分类规则：“只要来一个页面就判断为【不是特殊页面】”，这么做的效率非常高，如果计算按照准确率的定义来计算的话，是(9,999,900/10,000,000) = 99.999%。虽然高，但是这不是我们并不是我们真正需要的值，就需要新的定义标准了 对于一个二分类问题来说，将实例分为正类（Positive/+）或负类（Negative/-），但在使用分类器进行分类时会有四种情况 一个实例是正类，并被预测为正类，记为真正类（True Positive TP/T+） 一个实例是正类，但被预测为负类，记为假负类（False Negative FN/F-） 一个实例是负类，但被预测为正类，记为假正类（False Positive FP/F+） 一个实例是负类，但被预测为负类，记为真负类（True Negative TN/F-） TP和TN中的真表示分类正确，同理FN和FP表示分类错误的 为了全面的表达所有二分问题中的指标参数，下列矩阵叫做混淆矩阵 - Confusion Matrix，目的就是看懂它，搞清楚它，所有模型评价参数就很清晰了 DiagnosticTesting Diagram Relationships between various measures of diagnostic testing by CMG Lee. In the SVG image, hover over a block or relation to highlight it. #mainsvg { font-family:Helvetica,Arial,sans-serif; font-size:6px; text-anchor:middle; stroke-linejoin:round; stroke-linecap:round; stroke-width:0.7; fill:none; stroke-opacity:1; fill-opacity:1; } #mainsvg:hover { stroke-opacity:0.25; fill-opacity:0.25; } .active:hover { stroke-opacity:1; fill-opacity:1; } text { fill:#000000; cursor:default; } .label { stroke:none; fill:#000000 } .op { stroke-width:0.15; font-size:5px; font-weight:bold; } + &#247; &#247; &#247; + &#247; &#247; +,&#247; &#247; F1 s.F1 score = 2 / (1 / Recall + 1 / Precision) ACCAccuracy = (&#931; True positive + &#931; True negative) / &#931; Total population DORDiagnostic odds ratio = Positive likelihood ratio / Negative likelihood ratio LR+Positive likelihood ratio = True positive rate / False positive rate LR&#8722;Negative likelihood ratio = False negative rate / True negative rate FDRFalse discovery rate = &#931; False positive / &#931; Predicted condition positive PPVPositive predictive value, Precision = &#931; True positive / &#931; Predicted condition positive NPVNegative predictive value = &#931; True negative / &#931; Predicted condition negative FORFalse omission rate = &#931; False negative / &#931; Predicted condition negative TPRTrue positive rate, Recall, Sensitivity, probability of detection = &#931; True positive / &#931; Condition positive FNRFalse negative rate, Miss rate = &#931; False negative / &#931; Condition positive FPRFalse positive rate, Fall-out, probability of false alarm = &#931; False positive / &#931; Condition negative TNRTrue negative rate, Specificity = &#931; True negative / &#931; Condition negative prev.Prevalence = &#931; Condition positive / &#931; Total population pop.Total population = Condition positive + Condition negative = Predicted condition positive + Predicted condition negative 样本空间 = 正类 + 负类 = 预测结果正类 + 预测结果负类 Pc&#8722;Predicted condition negative = False negative + True negative Pc+Predicted condition positive = True positive + False positive C+Condition positive = True positive + False negative C&#8722;Condition negative = False positive + True negative T&#8722;负类中预测正确的部分 F&#8722;负类中预测错误的部分 F+正类中预测错误的部分 T+正类中预测正确的部分 通过上面的的讨论已经有T+:TP F+:FP T-:TN F-:FN C+:样本正类 C-:样本负类 Pc+:预测正类 Pc-:预测负类 用样本中的正类和负类进行计算的定义 缩写 全称 等价称呼 计算公式 TPR True Positive Rate 真正类率 Recall Sensitivity $ \\frac {\\sum T+}{\\sum C+}$ FNR False Negative Rate 假负类率Miss rate Type rs error $ \\frac {\\sum F-}{\\sum C+}$ FPR False Positive Rate 假正类率fall-out Type 1 error $ \\frac {\\sum F+}{\\sum C-}$ TNR Tre Negative Rate 真负类率Specificity $ \\frac {\\sum T-}{\\sum C-}$ 用预测结果的正类和负类进行计算的定义 缩写 全称 等价称呼 计算公式 PPV Positive Predictive Value 正类预测率Precision $ \\frac {\\sum T+}{\\sum Pc+}$ FOR False Omission Rata 假错误率 $ \\frac {\\sum F-}{\\sum Pc-}$ FDR False Discovery Rate 假发现率 $ \\frac {\\sum F+}{\\sum Pc+}$ NPV Negative Predictive Value 负类预测率 $ \\frac {\\sum T-}{\\sum Pc-}$ 其他定义概念 缩写 全称 等价称呼 计算公式 ACC Accuracy 准确率 $ \\frac {\\sum (T+) + \\sum {T-}}{样本空间}$ LR+ Positive Likelihood Ratio 正类似然比 $ \\frac {TPR}{FPR}$ LR- Negative likelihood ratio 负类似然比 $ \\frac {FNR}{TNR}$ DOR Diagnostic odds ratio 诊断胜算比 $ \\frac {LR+}{LR-}$ F1 score $F_1$ test measure F1值 $\\frac{2}{\\frac{1}{recall}+\\frac{1}{precision}}$ MCC Matthews Correlation coefficient 马修斯相关性系数 $\\frac{TP \\times TN - FP \\times FN}{\\sqrt {(TP + FP)(TP + FN)(TN + FP)(TN +FN)}}$ LR+/-指的是似然比，LR+ 越大表示模型对正类的分类越好，LR-越大表示模型对负类的分类效果越好 F1值是精确值和召回率的调和均值，其实原公式是 $F_\\beta = (1 + \\beta^2)\\frac{precision \\times recall}{(\\beta^2recall)+recall}$，这里的β表示：召回率的权重是准确率的β倍。即F值是一种精确率和召回率的综合指标，权重由β决定 MCC值在[-1,1]之间，靠近1表示完全预测正确，靠近-1表示完全悖论，0表示随机预测 最终为了不那么麻烦，说人话，还是一图胜千言 Precision - Recall 图片详解： 左边暗一些部分的点都是真正的正类，右边亮一些部分的点都是真正的负类 中间的一个圆圈就是我们的正类分类器：注意，这个圈是的预测结果都是正类，也就是说在这个分类器看来，它选择的这些元素都是它所认为的正类，对应的，当然是圈以外的部分，也就是预测结果是负类的部分 底下的Precision和Recall示意图也相当的直观，看一下就能明白 ROC CurveROC - Receiver Operating Characteristic Curve，接受者操作特征曲线，ROC曲线 这个曲线乍看下为啥名称那么奇怪呢，原来这个曲线最早是由二战中的电子工程师和雷达工程师发明的，用来侦测战场上的敌军飞机，舰艇等，是一种信号检测理论，还被应用到心理学领域做知觉检测。 什么是ROC曲线ROC曲线和混淆矩阵息息相关，上一部分已经详细解释了相关内容，这里直接说明ROC曲线的横坐标和纵坐标分别是什么 横坐标：FPR假正类率，纵坐标：TPR真正类率 初看之下你不懂一个曲线表示的什么意思，那么看几个特征点或特殊曲线是一个非常好的方法。按照这种方法来分析ROC曲线： 第一个点：(0,1)，FPR=0 TPR=1 ，这意味着所有的正类全部分类正确，或者说这是一个完美的分类器，将所有的样本都分类正确了 第二个点：(1,0)， FPR=1 TPR=0 ，和第一个点比较，这是第一个点的完全反面，意味着是个最糟糕的分类器，将所有的样本都分类错误了（但其实可以直接取反，就是最好的模型，因为是二分类问题） 第三个点：(0,0)，FPR=0 TPR=0 也就是原点，这个点表示的意思是，分类器预测所有的样本都为负类 第四个点：(1,1)，FPR=1 TPR=1，和第三个点对应，表示分类器预测所有的样本都为正类 一条线：y=x。这条对角线上的点实际上就是一个采用随机猜测策略的分类器的结果 总结来说，ROC曲线的面积越大，模型的效果越好；ROC曲线光滑以为着Overfitting越少 还是一图胜千言 ROC曲线解释 $TPR = \\frac{TP}{TP+FN}$ $FPR = \\frac{FP}{FP+TN}$ 蓝色图像是正类分类器的概率分布，红色图像负类分类器的概率分布，竖直的黑线是阈值（Threshold），二分类分类器的输出就是一个取值在[0,1]间的值（概率），我们将黑线从0移动到1，就能得出一条曲线，这条线就是ROC曲线 如果问这个分类器画成的图像为何是一个类似帽子的形状，例子是最佳的说明方法，我们就来算一个ROC曲线看看，下图是20个测试样本的结果，“Class”一栏表示每个测试样本真正的标签（p表示正类，n表示负类），“Score”表示每个测试样本属于正样本的概率，Inst#是序号数 example-data 接下来，我们从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值（和你的测试样本的数量有关），将它们画在ROC曲线的结果如下图： example-roc-curve 当然我们也可以曲很多个阈值画曲线，不一定非要从测试样本的结果中取20个 为什么使用ROC曲线ROC曲线有一个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线的对比： ROC-PrecisionRecall 在上图中，(a)和(c)为ROC曲线，(b)和(d)为Precision-Recall曲线。 (a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大，记住这个结论即可 PRC Curve在上面提到了一个指标，PRC - Precision-Recall 曲线，画法和ROC很相似，但是使用值是Precision和Recall AUC ValueAUC - Area Under Curve被定义为ROC曲线下的面积 AUC在[0.5,1]之间，这是因为ROC曲线一般都处于y=x这条直线的上方（否则这个做分类器的人连简单的取非都不会真可以去死了） AUC值越大，证明这个模型越好 Bias-Variance Tradeoff三个名词，Error误差 Bisa偏差 Variance方差 三个名词表示了什么再来一次，一图胜千言 准：Bias 描述的是根据样本训练的模型的输出预测结果的期望与样本真实结果的差距，说人话，这个模型对样本拟合的好不好。想在Bias上表现好，降低Bias，就是复杂化模型，增加模型的参数，但这样容易过拟合（Overfitting）Low Bias对应的就是点都在靶心附近，所以瞄的都是准的，但手不一定稳 确：Variance 描述的是根据样本训练的模型在测试集上的表现（泛化能力） ，想在Variance上表现好，降低Variance，需要简化模型，减少模型的参数，这样做容易欠拟合，对应上图的High Bias，点偏离的中心。Low Variance对应的是点打的都很集中，但不一定在靶心附近，手很稳，但是瞄的不准 要准确表达这两个定义的含义必须要使用公式化的语言，不得不感叹，在准确描述世界运行的规律这件事上，数学比文字要准确并且无歧义的多，文字（例子）直观啰嗦，数学（公式）准确简介 我们假设有这样的一个函数，$y=f(x) + \\epsilon$ ，其中噪声 $\\epsilon$ 均值为0，方差为 $\\sigma^2$ 我们的目的是去找到一个函数 $\\hat {f}(x)$ 尽可能接近 $f(x)$ ，我们可以用均方误差（MSE）或者交叉熵，或者DL散度来表示这个接近程度，我们希望 $(y - \\hat f(x) )^2$ 对样本空间内的所有样本和测试集中的所有样本都最小 机器学习核心就是用各种不同的算法去找这个 $\\hat f$，希望最小，那就使用一个公式来表征这个值得大小，即期望，也称Total Error（误差），在机器学习的训练中，这个值是评判模型好坏最重要：$$E[(y - \\hat f(x))^2] = Bias[\\hat f(x)]^2 + Var[\\hat f(x)] + \\sigma^2$$ 其中 $Bias[\\hat f(x)] = E[\\hat f(x) - f(x)]$，且 $Var[\\hat f(x)] = E[\\hat f(x)^2] - E[\\hat f(x)]^2$ Bias-Variance Tradeoff作为机器学习一个核心训练的观点或者说概念，推导觉得还是十分重要，整理如下 推导过程为了公式简介，把 $f(x)$ 与 $\\hat f(x)$ 简写为 $f$ 与 $\\hat f$ ，记随机变量为 $X$，有$$Var[X] = E[X^2] - E[X]^2 \\implies E[X^2] = Var[X] + E[X]^2$$ 因为 $f$ 是一个已经确定的函数，所以 $E[f] = f$ 成立 根据 $y = f + \\epsilon$ 和 $E[\\epsilon] = 0$ 有$$E[y] = E[f + \\epsilon] = E[f] = f$$噪声的方差 $ Var[\\epsilon] = \\sigma^2$ $$ Var[y] = E[(y-E[y])^2] = E[(y - f)^2] = E[(f + \\epsilon - f)^2] = E[\\epsilon^2] = Var[\\epsilon] + E[\\epsilon]^2 = \\sigma^2 $$ 由于 $\\epsilon$ 和 $\\hat f$ 互相独立 $$ \\begin{align} E[(y - \\hat f)^2] & = E[y^2 + \\hat f^2 - 2y\\hat f] \\\\ & = E[y^2] + E[\\hat f^2] - E[2y\\hat f] \\\\ & = Var[y] + E[y]^2 + Var[\\hat f] + E[\\hat f]^2 - 2fE[\\hat f] \\\\ & = Var[y] + Var[\\hat f] + (f^2 - 2fE[\\hat f] + E[\\hat f]^2) \\\\ & = Var[y] + Var[\\hat f] + (f - E[\\hat f])^2 \\\\ & = \\sigma^2 + Var[\\hat f] + Bias[\\hat f]^2 \\end{align} $$ 总结感觉在实际使用中，你不需要去自己写代码来画这些曲线，只要是框架是一定整合了这些值得结果，但是知其然知其所以然，越了解它是如何画的，越能处理奇怪的特殊情况 常见的处理方式是记下来所有指标的结果，即这些指标怎么变，表示了模型的那些方面好或者坏的结论，但是如果在特殊的问题出现了不在你看的结果中的情况可能还是会捉襟见肘，还是脚踏实地，能看见更大的世界！","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"}]},{"title":"【直观详解】信息熵、交叉熵和相对熵","date":"2017-09-11T07:25:49.000Z","path":"2017/09/11/什么是信息熵、交叉熵和相对熵/","text":"【阅读时间】10min - 13min【内容简介】使用一个现实中直观的例子详解信息熵、交叉熵及相对熵的核心概念，读完后，希望能帮助你建立起这三个概念的固有直觉，不再疑惑 要完成题目的最终解释，必须从熵这个神奇的概念开始讲起 什么是熵 - Entropy词源 - 最初来源于热力学Entropy来源于希腊语，原意：内向，即：一个系统不受外部干扰时往内部稳定状态发展的特性。定义的其实是一个热力学的系统变化的趋势 $$\\Delta S = \\frac{Q}{T} = \\frac{热量}{温度} \\tag{1-1}$$1923年，德国科学家普朗克来中国讲学用到entropy这个词，胡刚复教授看到这个公式，创造了“熵”字，因为“火”和热量有关，定义式又是热量比温度，相当自洽 信息论信息论中，熵是接受的每条消息中包含的信息的平均值。又被称为信息熵、信源熵、平均自信息量。可以被理解为不确定性的度量，熵越大，信源的分布越随机 1948年，由克劳德·爱尔伍德·香农将热力学中的熵引入信息论，所以也叫做：香农熵 生态学在生态学中，熵表示生物多样性的指标 广义的定义熵是描述一个系统的无序程度的变量；同样的表述还有，熵是系统混乱度的度量，一切自发的不可逆过程都是从有序到无序的变化过程，向熵增的方向进行 我们接下来要讨论的信息熵 交叉熵 相对熵 更多的着眼于信息论的角度，换句话说，更加关注概率和不确定性 什么是信息熵、交叉熵、相对熵可以将对熵的理解从简单到复杂依次分解成三个层次来理解 如何衡量不确定事物的发生？数学是一种工具，使用数学来描述现实中的各种事物是一个数学家本质的工作目标。而现实中不确定性，或者说不太确定是否会发生的事件必须要找到一种抽象的、符号化和公式化的手段去表示。 比如天气情况，假设可能有【阴、晴、雨、雪】四种情况，使用概率符号表示 $\\mathbf P = [p_1,p_2,p_3,p_4]$，接下来自然而然的思考：那么，什么条件（情况）会影响这些值呢？ 假设有一下三种描述，或者说条件 今天是晴天，所以明天可能也是晴天 天气预报说明天下雨 9月12日苹果公司举行发布会 那么这三个描述中，很明显，第二条的信息量更大，因为它可以使得不确定事件发生在 $p_3$ 的概率更大。类似的，第三条对判断毫无帮助，信息量为0。注意，信息量不等于信息熵，如果是这样，那么直接用概率来衡量就可以了，不需要在重新定义一个概念 其实信息熵是信息量的期望（均值），它不是针对每条信息，而是针对整个不确定性结果集而言，信息熵越大，事件不确定性就越大。单条信息只能从某种程度上影响结果集概率的分布 考虑到信息冗余，信息量存储下来究竟需要多大空间？我们已经有了 $\\mathbf P = [p_1,p_2,p_3,p_4]$ 来表示天气情况，那么用计算机来存储每天的天气，那该如何编码呢？ 常见的做法是，4个不同的信息，只需要2bit就能做到，00 01 11 10，假设我们在南方城市，肯定要把00编码成雨天，这样可以节省存储空间，至于为什么能节省存储空间，这就要讨论编码方式。可以简单的理解为，如果一串信息一串0很多，可以通过编码压缩这一群0来节省空间 使用一个公式来计算记录n天数据需要的存储空间：Sn $$ S_n = n \\times \\sum_{i = 1}^4{\\left(P_i \\times F(P_i) \\right) } \\tag{2-1} $$ $P_i$ 表示第i个事件发生的概率；$F(P_i)$ 表示存储空间的存储因子 如何确定这个函数 $F(P_i)$ 的形式？考虑这个函数需要满足条件：概率大的事件对应小的存储空间，说人话，就是成反比，你的数学功底不错的话，脑海中第一反应出来满足这个条件最直观是反比例函数，说人话， $\\frac{1}{P_i}$ 。 之后我们发现这个公式中有个除法非常讨厌，我们想着去掉它，脑海中第一反应出来的满足这个条件的一定是取对数，至于为什么取对数，那说道就很多，取对数是指数的逆操作， 对数操作可以让原本不符合正态分布的模型符合正态分布，比如随着模型自变量的增加，因变量的方差也增大的模型取对数后会更加稳定 取对数操作可以rescale（原谅我，这里思前想后还是感觉一个英文单词更加生动）其实本质来说都是因为第一点。说人话版本，人不喜欢乘法，对数可以把乘法变加法 那么我们结束清楚之后，就很容易就可以定义出$$F(P_i) = \\log_a ({\\frac{1}{P_i}}) \\tag{2-2}$$ a作为底数，可以取2（处理2bit数据），10（万金油），e（处理正态分布相关的数据） 结合对信息熵的定义（第一节最后的粗体字）然后把（2-2）带入（2-1），就会发现，哦！看着有点眼熟啊$$H(P) = \\sum_i {P(i)log_a {\\frac{1}{P(i)}}} = - \\sum_i {P(i)log_a {P(i)}} \\tag{2-3}$$这这这，就是信息熵的定义式吧？总结就发现，信息熵其实从某种意义上反映了信息量存储下来需要多少存储空间 总结为：根据真实分布，我们能够找到一个最优策略，以最小的代价消除系统的不确定性（比如编码），而这个代价的大小就是信息熵 理解基于信息熵的交叉熵和相对熵因为是我们用2bit模式存储，为了计算方便，这里取a = 2 先计算刚刚有关天气问题 $\\mathbf P = [p_1,p_2,p_3,p_4]$ ：【阴、晴、雨、雪】的信息熵，假设我们对天气的概率一无所知，那么四种天气的发生概率为等概率（服从平均分布），即 $\\mathbf P = [\\frac {1}{4},\\frac {1}{4},\\frac {1}{4},\\frac {1}{4}]$ ，带入公式2-3，得到 $H(P) = 2$ ，存储信息需要的空间 $S_n = 2n$ 继续思考，假设我们考虑天气的城市是一个地处中国南方雨季的城市，那么阴天和雨天的概率从经验角度（先验概率）来看大于晴天雪天，把这种分布记为 $\\mathbf Q = [\\frac{1}{4},\\frac{1}{8},\\frac{1}{2},\\frac{1}{8}]$，带入公式2-3，信息熵 $H(Q) = 1.75$，存储信息需要的空间 $S_n = 1.75n$ 直观的来考虑上面不同的两种情况，明显当事件的不确定性变小时候，我们可以改变存储策略（00 雨天 01 阴天），再通过编码，节省存储空间。信息熵的大小就是用来度量这个不确定大小的 关于编码的方式，这里提一下，哈夫曼树与哈夫曼编码 ，有兴趣的读者可以去研究一下 交叉熵的由来我们把这个问题再扩展一下 天气【阴、晴、雨、雪】 信息熵 $\\mathbf P = [\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4}]$ $H(P) = 2$ $\\mathbf Q = [\\frac{1}{4},\\frac{1}{8},\\frac{1}{2},\\frac{1}{8}]$ $H(Q) = 1.75$ $\\mathbf Z = [\\frac{1}{8},\\frac{1}{16},\\frac{3}{4},\\frac{1}{16}]$ $H(Z) = \\frac{7}{8}+\\log_2 {\\frac{4}{3}} = 1.29$ $\\mathbf W = [0,0,1,0]$ $H(W) = 0$ 接下来，假定在确定性更大的概率分布情况下，用更不确定的存储策略来计算，比如使用 $\\mathbf P$ 的概率乘上 $\\mathbf Q$ 的存储因子，套用公式2-3$$H(\\mathbf P,\\mathbf Q) = \\sum_i {P(i) \\log_a {\\frac{1}{Q(i)}}} \\tag{3-1}$$顾名思义，看公式3-1的形式，就不难发现，这就是所谓的交叉熵，计算可得 交叉熵 P Q Z W P $H(P,P) = 2$ $H(P,Q) = 2.25$ $H(P,Z) = \\frac{11}{4}+\\frac{1}{4}\\log_2 {\\frac{4}{3}} = 2.85$ +inf Q $H(Q,P) = 2$ $H(Q,Q) = 1.75$ $H(Q,Z) = \\frac{7}{4}+\\frac{1}{2}\\log_2 {\\frac{4}{3}} = 1.96$ +inf Z $H(Z,P) = 2$ $H(Z,Q) = 1.375$ $H(Z,Z) = \\frac{7}{8}+\\log_2 {\\frac{4}{3}} = 1.29$ +inf W $H(W,P) = 2$ $H(W,Q) = 1$ $H(W,Z) = \\log_2 {\\frac{4}{3}} = 0.415$ $H(W,W) = 0$ 上表直观的展现的交叉熵的数值表现，PQZW依次不确定性越来越低，极端情况的W不确定性为0，即是确定的 交叉熵，用来高衡量在给定的真实分布下，使用非真实分布指定的策略消除系统的不确定性所需要付出努力的大小 总的来说，我们的目的是：让熵尽可能小，即存储空间小（消除系统的不确定的努力小）。（不要问为什么想要存储空间小，这都是钱更是效率和时间） 通过上表我们发现一个规律，为了让熵小，解决方案是：是用确定性更大的概率乘以确定性更小的存储因子，比如不确定性越大的概率分布，如P概率分布，其信息熵越大；基于同一真实（确定性）分布的情况下，套用不确定性更大的存储因子，如P的存储因子，得出的交叉熵越大 在机器学习中，即用测试结果集（样本结果集）的概率乘以训练出来的结果集存储因子，而在不断的训练过程中，我们要做的就是通过不断调整参数，降低这个值，使得模型更加的稳定，不确定性越来越小，即突出需要表征的数值的特点（白话文也就是分类的效果更好） 相对熵的由来有了信息熵和交叉熵后，相对熵是用来衡量两个概率分布之间的差异，记为 $D(P||Q) = H(P,Q) - H(P)$，也称之为KL散度$$D_{KL}(P||Q) = \\sum_i{P(i) \\log_a {\\frac{P(i)}{Q(i)}}}$$当 $P(i) = Q(i)$ 的时候，该值为0，深度学习过程也是一个降低该值的过程，该值越低，训练出来的概率Q越接近样本集概率P，即越准确，或者可以理解为相对熵一把标尺，用来衡量两个函数是否相似，一样就是0，当然，这种解释十分牵强，但是更直观 关于底数 $a$ 的选择问题，其实和概率分布的情况是分不开的。比如使用2进制编码，那么所能表示的不同情况的数量，$\\sum_{i=0}^N 2^i$，我们知道，指数函数变化率变化很大，不好分析，稳定性差。对数操作可以乘法变加法，指数放下来，是十分好用的数学工具（其实是一种变换域的思想，这种思想在整个信息论，统计学中处处可见） 比如使用 $ln()$ 的时候，对应的分布，其实是正态分布，很好理解，正太分布的底数是 $e$","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"}]},{"title":"Dota2机制总结","date":"2017-09-06T06:50:56.000Z","path":"2017/09/05/Dota2机制总结/","text":"【阅读时间】百科类型文章【内容简介】这是一份有关Dota2游戏机制的总结，核心目的是为了方便查阅，计算公式。针对人群是对数据和游戏机制有很大兴趣的高玩，从中你可能能了解如何通过击杀或得更多的经济，哪些操作可以躲避技能等等 版本信息：更新到7.06f 金钱击杀英雄奖励获得金钱 = 110 + 连杀奖励 + （被击杀者等级 * 8） 连杀奖励 = 60 * （连杀数-2）[大于0] 助攻奖励 助攻英雄数 获得金钱 1 [140 + (5 × 被击杀者等级) + (0.0375 × 被击杀者净收入 × 净收入因子) + (100 × 团队净收入之差 / 4000)] 2 [70 + (4 × 被击杀者等级) + (0.0375 × 被击杀者净收入 × 净收入因子) + (75 × 团队净收入之差 / 4000)] 3 [35 + (3 × 被击杀者等级) + (0.0375 × 被击杀者净收入 × 净收入因子) + (50 × 团队净收入之差 / 4000)] 4 [25 + (2 × 被击杀者等级) + (0.03 × 被击杀者净收入 × 净收入因子) + (35 × 团队净收入之差 / 4000)] 5 [20 + (1 × 被击杀者等级) + (0.0225 × 被击杀者净收入 × 净收入因子) + (25 × 团队净收入之差 / 4000)] 净收入因子 = (敌方队伍净收入 / 友方队伍净收入) - 1，最小值为0，最大值为1 还需要 × [1.2 - 0.1 × (被击杀者净收入排名 -1)] × [净收入排名因子] Roshan200 团队奖励 150 - 400 击杀奖励 死亡掉钱损失不可靠金钱 = 50 + 财产总和 ÷ 40 5千经济 ➜ 175；1万经济 ➜ 290；2万经济 ➜ 550 复活复活时间 每级增加2秒 每到6级的倍数增加10秒 18级后每级增加4秒 买活花费买活金钱 = 100 + (英雄等级 英雄等级 1.5) + (游戏时间(s) * 0.25 ) 放弃比赛掉线超过5分钟后，所有金钱被队友平分 物理攻击伤害最终攻击伤害 $$ 最终攻击伤害 = \\\\ \\{ [\\text {MD} × (1 + \\sum \\text {PBD}) + \\text {FBD}] \\times \\text {CSM} - \\text {BD} \\}\\\\ \\times \\text {AVM} \\times \\text {ATM} \\times \\text {GDM} $$ MD(Main Damage) 主要攻击力【白字攻击力】主要攻击力 = 基础攻击力 + 主属性 除此之外，所有加成的攻击力都是【绿字攻击力】 PBD(Percentage bonus damage) 百分比攻击力加成这个加成是加法叠加 Tips: 圣者遗物可以增加60攻击力，一个出支配带头狼的VS，36%+30% = 66%，60 / 0.66 = 90，也就是说，白字攻击力达到90就等于这个英雄出了一个圣者遗物，相当的可怕。 技能 加成数值 头狼光环 30% 强化图腾 100%/200%/300%/400% 野性驱使 15%/26%/37%/48%（只影响狼人控制的单位） 授予力量 20%/30%/40%/50% (天赋 30%/40%/50%/60%) 神之力量 80%/120%/160% (友军A帐: 75%/100%/125%) 复仇光环 12%/20%/28%/36% (天赋 32%/40%/48%/56%) 祭品光环 15% FBD(Flat Bonus Damage) 固定值百分比加成 技能 加成数值 臂章-邪恶之力 31 嗜血渴望 最高攻击力加成/英雄: 16/24/32/40 战意 最高叠加层数: 5/7/9 每层攻击力加成: 18/24/30 极度饥渴 60/100/140 死亡契约 基于目标最大生命值的攻击力加成: 5%/7%/9% 精准光环 敏捷 20%/26%/32%/38% (天赋26%/32%/38%/44%) 星体游魂 小兵6/9/12/15 英雄 12/24/36/48 (天赋92/104/116/128) 灵动迅捷 10/25/40/55/70/85/100 A帐115 卡尔-火 4/8/12/16/20/24/28 * 3 决斗 10/14/18 (天赋50/54/58) 战斗嚎叫 70/100/130 月之祝福 14/22/30/38 嚎叫 英雄 10/15/20/25 非英雄 4/6/8/10 夜晚翻倍 静电连接 每秒偷取 7/14/21/28 (天赋21/28/35/42) 支配死灵 最大灵魂数: 18/24/30/36 (A帐 22/30/38/46) 折光 攻击次数: 3/4/5/6 (天赋6/7/8/9)攻击力加成: 20/40/60/80 魔化 20/40/60/80 长大 50/100/150 加主要攻击力 衰退光环 英雄死亡 30/35/40/45 小兵死亡 5 CSM(Critical Strike Multiplier) 致命一击倍数 致命一击来源 几率% 伤害% DPS期望% 头狼 - 致命一击 20 200 20 血棘 - 致命一击 20 175 15 酒仙 - 醉拳 10/15/20/25 230 13/19.5/26/32.5 混沌一击 12 125/175/225/275 3/9/15/21 水晶剑 - 致命一击 20 175 15 大炮 - 致命一击 30 235 40.5 剑舞 20/25/30/35 200% 20/25/30/35 狼人 - 变身 40 160/180/200 24/32/40 恩赐解脱 15 230/340/450 19.5/36/52.5 殊死一搏 15 英雄 150/200/250/300 7.5/15/22.5/30 血棘 - 灵魂撕裂 100 140 140 忍术 100 150/175/200/225 12/10/8/6 天赋 -5 棒击大地 100 150/175/200/225 天赋+100 触发条件 暗杀 100 A帐 280 距离内所有敌人 海象神拳 100 350/A帐500 冷却 36/24/12 游戏中出现的红字代表的是减少前的物理伤害 BD(Blocked Damage) 被格挡伤害 伤害格挡来源 几率% 格挡伤害 圆盾 50 近战16 远程8 穷鬼盾 英雄100 非英雄50 近战20 远程10 先锋盾 50 近战70 远程35 赤红甲 - 坚盾 100 60 海妖外壳 100 12/24/36/48 伤害格挡不格挡物理伤害技能，守卫的攻击也不格挡 AVM(Armor Value Multiplier) 护甲值倍数见护甲 ATM(Armor Type Multiplier) 护甲类型倍数见攻击类型，英雄打英雄100% GDM(General Damage Multipliers) 一般伤害倍数见伤害调整 攻击类型基础打英雄护甲75%伤害 穿刺打英雄护甲50%伤害，基础护甲（小兵护甲）150%伤害 伤害调整伤害减免和加深除了回光返照，幽灵船，魔法护盾之外，伤害减免和加深的叠加为加法叠加 技能 来源 数值% 血之狂暴 加深接受输出 25/30/35/40 远处 减半 赎罪 加深接受 18/24/30/36 灵魂猎手 加深接受 20/30/40/50 守卫冲刺 加深接受 15 血肉傀儡 加深接受 20/25/30 200范围内最高 回光返照 减免接受 0 全免 4/5/6 A帐+1 刚毛后背 减免接受 背后 16/24/32/40 侧面 减半 奔袭冲撞 减免接受 A帐 40 持续4秒 过载 减免接受 5/10/15/20 幽灵船 减免接受 40/45/50 持续10秒 决斗 减免接受 A帐 100 持续6/7/8秒 魔法护盾 减免接受 60 1.6/1.9/2.2/2.5 钻地 减免接受 40 折射 减免接受 10/14/18/22 激怒 减免接受 80 持续4秒 陵卫斗篷 减免接受 4层 8/12/16/20 冷却6/5/4/3 寒冬诅咒 减免接受 100 3.25/4/4.75 战斗饥渴 降低输出 A帐 30 持续10秒 白银之锋 降低输出 50 持续5秒 伤害无效化伤害实例仍然存在，如果一些与伤害触发相关的事件并且没有低于伤害阈值的伤害，仍然会触发伤害事件 技能或物品名称 说明 无天光盾 110/140/170/200 天赋 +300 15s持续时间 回光返照 3/4/5(A 5/6/7) 伤害转化为治疗 凝魂之类 5次 大于50点的伤害抵挡120点 尖刺外壳 2.25s持续时间 无效化每个玩家的第一次伤害 守护天使 6/7/8 (A 8/9/10) 物理伤害无效化 虚妄芝诺 7/8/9(天赋 +2) 持续时间结束受到被无效化的伤害 折光 次数 3/4/5/6(天赋 +3) 忽略低于5点的伤害 活体护甲 所有类型伤害无效化 20/40/60/80 次数 4/5/6/7(天赋 +4) 持续15s 低于5伤害忽略 极寒之拥 持续时间4s 无效物理伤害 攻击速度基础攻击间隔 BAT英雄在没有额外攻速加成的情况下每两次攻击间的时间间隔 攻击速度 ISA 面板中英雄增加的攻击速度 由装备获得的攻击速度加成 每个英雄基础100点基础攻速 由Debuff造成的攻速减低 攻击速度计算公式 $$ 每秒攻击的次数 = \\frac{(100 + IAS) × 0.01} {BAT} $$ $$ 每次攻击的时间 = \\frac{1}{每秒攻击的次数} $$ 攻击速度 效果 -80 五分之一BAT时间来攻击 -75 四分之一BAT时间来攻击 -66 三分之一BAT时间来攻击 -50 二分之一BAT时间来攻击 +00 正常状态 +100 * n （1+n）倍攻击速度 根据表格我们可以知道减攻速的技能在基础攻速很高的情况下基本没有什么效果，但是越接近0速度，减速效果越明显 增加攻击速度技能列表 技能 增加数值 持续时间s 魔霭诅咒 10/20/30/40 4.5 雷肤兽 - 暴怒 75 8 雷肤兽 - 战鼓光环 15 光环范围 900 天穹守望者 - 磁场 50/60/70/80 3.5/4.5/5.5/6.5 淘汰之刃 30 6 A帐10 成功淘汰 野性之心 15/25/35/45 光环范围 900 扫射 130 天赋 +70 4/6/8/10 熊怪 - 迅捷光环 15 光环范围 900 飓风之力 100 5 狂战士之血 220/260/300/340 剩下10%生命值最高 灵动迅捷 10/25/40/55/70/85/100/A115 9 卡尔 - 雷 2/4/6/8/10/12/14 * 3 开关 过载 40/50/60/70 开关 强攻 65/90/115/140 5 狂暴 50/60/70/80 3/4/5/6 炽魂 每层40/55/70/85(天赋 75/90/105/120) 10 最高3层 德鲁伊 - 狂猛 10/20/30/40 18/22/26/30 跳跃 16/32/48/64 (天赋 +100) 5 死灵射手光环 5/7/9 光环范围 900 暗夜猎影 45/60/75/90 夜晚 嗜血术 30/40/50/60 (天赋 +40) 30 幻影突袭 130 4s or 4次攻击 战斗专注 60/120/180 5 热血战魂 15/20/25/30 (105/140/175/210) 每次攻击同个目标 超强力量 400 15 or 3/4/5/6次攻击 黄泉颤抖 64 3/4/5/6 集中火力 500 20 寒冬诅咒 70 3.25/4/4.75 降低攻击速度技能比较有效果的降低攻速的技能 烈火精灵：80/100/120/140 不可侵犯：40/70/100/130，蝮蛇突袭：40/60/80 重生：75 黄泉颤抖：64 小狼-致残：60 冰封魔印：30/40/50/60 雷霆一击：25/35/45/55 原始咆哮：50 冰霜新星：20/30/40/50 液态火：20/30/40/50 石化凝视：50 夜魔虚空：50 冰眼：45 豪猪：10/20/30/40 冰火交加：28/32/36/40 毒龙法球：10/20/30/40 全能光环：10/18/26/34 技能攻击伤害技能伤害计算魔法伤害受到魔法抗性影响，技能伤害可以由智力获得增强 $$ 增强数值 = [初始智力 + (当前等级 - 1) \\times 智力成长] / 14 / 100 + 技能增强天赋 $$ $$ 技能最终伤害 = 技能伤害数值 \\times (1 + 增强数值) \\times \\\\ \\prod_{i=1}^n{(1 - 魔法抗性增加_i)} \\times\\prod_{i=1}^n{(1 + 魔法抗性降低_i)} $$ 技能增强天赋远古冰魄10：8% 蝙蝠骑士15：5% 人马20：10% 死亡先知10：5% 干扰者20：10% 大地之灵20：15% 灰烬之灵10：8% 矮人直升机10：6% 杰奇洛10：8% 拉西克20：5% 莉娜20：6% 莱恩20：8% 马格纳斯10：15% 米拉娜15：5% 食人魔魔法师25：15% 殁境神蚀者25：8% 凤凰20：8% 帕克20：10% 拉比克20：8% 暗影恶魔15：8% 影魔15：6% 风暴之灵25：10% 伐木机20：5% 修补匠15：4% 孽主15：12% 维萨吉25：20% 风行者20：15% 技能伤害类型分为：魔法伤害，物理伤害，纯粹伤害 大部分伤害为魔法伤害 物理伤害技能炼金术士：酸性喷雾 炼金术士：不稳定化合物 敌法师：法力损毁 斧王：反击螺旋 兽王：野性飞斧 赏金猎人：暗影步 钢背兽：针刺扫射 人马：反击 克林克兹：灼热之箭 戴泽：剧毒之触 戴泽：暗影波 死亡先知：驱使恶灵 主宰：无敌斩 昆卡：潮汐使者 拉西克：恶魔的赦令 噬魂鬼：盛宴 剃刀：风暴之眼 斯拉达：鱼人碎击 斯拉达：深海重击 狙击手：爆头 工程师：感应地雷 工程师：爆破起飞 潮汐猎人：锚机 熊战士：怒意狂击 冥界亚龙：幽冥剧毒 编织者：虫群 纯粹伤害技能祸乱之源：蚀脑 祸乱之源：噩梦 刃甲：反弹伤害 嗜血狂魔：血之祭祀 嗜血狂魔：割裂 陈：忠诚考验 死亡先知：吸魂巫术 末日使者：末日 魅惑魔女：推进 谜团：午夜凋零 祈求者：电磁脉冲 祈求者：阳炎冲击 莉娜：神灭斩A帐 美杜莎：石化后秘术异蛇 司夜刺客：尖刺外壳 全能骑士：洗礼 殁境神蚀者：奥术天球 帕吉：肉狗 痛苦女王：超声波冲击 沉默术士：智慧之刃 幽鬼：荒芜 圣堂刺客：灵能之刃 伐木机：锯齿飞轮 伐木机：伐木锯链 伐木机：带树木死亡旋风 修补匠：激光 骨灰 护甲白字护甲$$敏捷 = 基础敏捷 + (等级 - 1) * 敏捷成长$$ $$白字护甲 = 基础护甲 + ( \\frac{敏捷}{7})$$ 护甲值倍数$$护甲值倍数 = 1 - \\frac{0.06 \\times 护甲值}{1 + 0.06 \\times |护甲值| }$$ 护甲值倍数倍数和护甲值的相关曲线 相关曲线 纵坐标是护甲值倍数，横坐标是现在英雄的护甲，不同颜色的线是此时减少的护甲（越上面的线减的越多） 有效生命值 （EHP）有效生命值 = 总生命值 ÷ 护甲值倍数 $$ 实时有效物理生命值 = 当前生命值 \\div (1 - \\frac{0.06 \\times 当前总护甲值}{1 + 0.06 \\times |当前总护甲值| }) $$ $$ 实时有效魔法生命值 = 当前生命值 \\div (0.75 \\times (1 - 装备提供抗性_1) \\times \\ldots \\times (1 - 装备提供抗性_n)) $$ 护甲调整增加护甲的技能 技能 加成数值 持续时间s 黑龙 - 龙肤光环 3 光环范围 900 狂战士怒吼 40 2/2.4/2.8/3.2 编织 0.75/1.0/1.25 每秒 18/24/30 24 龙族血统 3/6/9/12(天赋 翻倍) 永久 霜冻护甲 3/5/7/9 40 战斗嚎叫 10/15/20 6 变形术 4/6/8 变形状态 寒冰盔甲 8 45 战吼 5/10/15/20 8 活性护甲 5/10/15/20 每层 1/1.2/1.4/1.6 10/13/16/19 崎岖外表 3/4/5/6 永久 巨魔 - 狂战士之怒 6 切换 减低护甲的技能 技能 降低数值 持续时间s 酸性喷雾 4/5/6/7 (天赋 +4) 16 远古 - 亵渎 50% 6 粘稠鼻液 1/1.4/1.8/2.2 最高层数4(8) 英雄5 小兵10 实相裂隙 3/4/5/6 8 编织 0.75/1/1.25每秒 (18/24/30) 24 自然秩序 基础护甲：40%/60%/80%/100% 光环范围 275 火人 - 攻击 每次1点 上限10 5 击中刷新时间 激流 2/3/4/5 8 范围 320 风暴之眼 0.7/0.6/0.5 (天赋 -0.1) 打击1次1点 30 魔王降临 3/4/5/6 光环范围 900 侵蚀雾霭 10/15/20 18 隐匿 2/4/6/8 10 巨浪 3/4/5/6 (天赋 +5) 4 死亡旋风 敏捷损失 * 0.14 14 恐怖波动 3/4/5/6 1400距离 300范围 15 虫群 1.4/1.25/1.1/0.95 攻击一次1点 16 护甲相关装备强袭 +5 玄冥盾牌系列 +2 勋章 +7 天鹰 +2 炎阳纹章 +10 祭品 +4 黯灭 -7 勋章 -7 炎阳纹章 -10 强袭 -5 枯萎之石 -2 疯脸 -5 闪避机制闪避与致盲都会在攻击完成（弹道击中）时有一定几率触发 叠加与计算多个闪避来源乘法叠加 上下坡落空几率如果攻击者处于比目标更低的位置时，远程攻击会有25%的几率落空。 攻击者和目标之间的地形的高低差异实在击中目标时决定的，中路对线过程中，可以使用弹道飞行过程位移来保持和目标的同样地形高度保证必中 飞行单位无上下坡落空几率 计算公式$\\prod_{i=0}^n$ 的含义是把i=0到n所有的项相乘 $$ 落空几率 = \\prod_{i=0}^n (1 - 闪避来源_i) \\times \\prod_{j=0}^n (1 - 致盲来源_j) \\times 上下坡落空几率 $$ $$ 命中几率 = 1 - \\prod_{i = 0}^n{(1 - 必中/克敌先机来源_i)} $$ $$ 最终命中几率 = 1 - 落空几率 \\times (1 - 命中几率) $$ $$ 最终落空几率 = 落空几率 \\times (1 - 命中几率) $$ 公式只是为了程序数值计算使用，是需要记住：每一次攻击要绕过所有的闪避成功命中，只有当所有的闪避都失败了，这次攻击才可以造成伤害。所以说，出很多个闪避装备，在一定程度上对物理核心非常克制，这时候物理核心必须出金箍棒 闪避来源 技能或物品名称 闪避几率% 敌法师 - 20级右天赋 15 磁场 100 3.5/4.5/5.5/6.5s 醉拳 10/15/20/25 一段时间的100%闪避 赏金猎人 - 25级右天赋 25 蝴蝶 35 人马 - 15级右天赋 10 克林克兹 - 20级右天赋 20 虚空 - 25级右天赋 20 黑暗贤者 - 10级右天赋 12 天堂之戟 25 噬魂鬼 - 20级右天赋 15 狼人 - 20级右天赋 15 美杜莎 - 15级左天赋 15 米波 - 20级右天赋 10 大圣 - 10级左天赋 12 模糊 20/30/40/50 猴子 - 20级左天赋 15 炎阳纹章 20 炎阳纹章- 队友使用 - 日耀 20 7s 斯温 - 20级左天赋 20 闪避护肤 20 圣堂刺客 - 15级左天赋 12 风行 100 3/4/5/6a 致盲来源 技能或物品名称 落空几率% 醉酒云雾 70 4s 麻痹之咬 30/40/50/60 2s 致盲之光 80 3/4/5s 伤残恐惧 白天10 3s 夜晚50 5/6/7/8s 辉耀 - 辉耀灼烧 17 烟雾 40/50/60/70 6s 激光 100 3/3.5/4/4.5s 小兵 6s 近战旋风飞斧 60 4/5/6/7s 克敌机先来源为一种攻击特效，防止该次攻击落空，用来反制闪避，致盲，以及远程单位上下坡的25%几率落空，也能够防止近战攻击由于目标在攻击之前超过了350距离而落空 但是攻击弹道依旧可以躲避 对建筑物无效 技能或物品名称 备注 不会落空为100% 强化图腾 带有Buff的一次攻击不会落空 棒击大地 不会落空的即时攻击 金箍棒 每次攻击带有克敌先机 复仇 破影一击不会落空 窒息之刃 不会落空的即时攻击 白银之锋 - 暗影步 破影一击不会落空 暗杀 需要A帐 自然庇护 破影一击不会落空 海象神拳！ 不会落空 死亡守卫 需要A帐 不会落空 必中来源必中防止一个单位受到的任何攻击落空 血棘的灵魂撕裂，岗哨守卫，炎阳纹章给敌方使用提供35%的必中效果 移动速度叠加相似的装备提供的移动速度不叠加，除了风帐 多个鞋类物品不叠加 夜叉 散夜对剑 幻影斧不叠加 多个战鼓或风灵之纹不叠加 风灵之纹和战鼓鞋类物品叠加 公式移动速度 = （基础移动速度 + 具体移动速度加成） * （1 + 百分比移动速度加成和减速的和） 魔法抗性魔法抗性除了米波35%，维萨吉10%魔法抗性外，其他英雄都为25%基础魔法抗性 魔法抗性乘法叠加，不同的提高魔法抗性的装备可以叠加 魔法抗性加成来源 技能或物品名称 加成数值%及备注 法术护盾 26/34/42/50 小马or小熊怪光环 英雄5 非英雄20 可叠加 魔抗斗篷 15 微光披风 15 被动 微光披风 - 微光 45 5s 0.6s渐隐时间 挑战头巾 25 狂战士之血 20/30/40/50 最大10%生命值 洞察烟斗 30 被动 洞察烟斗光环 10 腐肉堆积 6/8/10/12 失效力场 10/14/18/22 腐蚀皮肤 10/15/20/25 魔法抗性减少来源 技能或物品名称 减少数值% 备注 冰霜漩涡 15/20/25/30 16s 0.5s粘滞时间 自然秩序 40/60/80/100 光环范围350 1s粘滞时间 虚化冲击 40 敌方3s 友方4s 幽灵形态 40 4s 幽魂护罩 20 3/3.5/4/4.5 衰老 30/40/50/60 3.5 上古封印 30/35/40/45 3/4/5/6 纷争面纱 25 16 魔法抗性100%来源 技能或物品名称 备注 黑皇杖 10/9/8/7/6/5 牺牲 跳跃时间or持续5s 剑刃风暴 5s 狂暴 3/4/5/6 (天赋+1s) 石化凝视 3s 天赋5s 驱逐 4/5/6/7 命运赦令 3/3.5/4/4.5 魔法吸收护盾魔法吸收护盾计算是计算魔抗后的吸收数值，魔抗越高，护盾效果越好 任何类型魔法护盾无法叠加，同时吸收伤害 技能或物品名称 吸收数值 烈火罩 50/200/350/500 （天赋 +500） 挑战头巾 - 绝缘 325 持续12s 洞察烟斗 - 法术护盾 400 持续12s 物品被动效果叠加独立叠加 攻击力 属性加成 魔法值/生命值 生命恢复速率/魔法恢复速率（基础速率 * 加成倍数） 攻击速度加成 护甲加成 分裂区域 移动速度加成 乘法叠加出现边缘递减效应$$加成 = 1 - (1-x) \\times (1-y) \\times (1-z) \\times \\ldots$$其中 $x y z$ 都表示一个百分比 魔法抗性乘法叠加 一个100点魔法伤害的技能 英雄本身25%魔法抗性，伤害变为 100 * （1 - 25%） = 75 再装备挑战头巾，再降低30%，伤害变为 75 * （1 - 30%） = 52.5 躲避躲避是一种躲避弹道的行为，更确切的说，是使弹道完全失去跟踪目标能力的行为。白话文就是：秀操作，骚 躲避技能的方式技能以下技能在施法时能躲避弹道 炼金术士：化学狂暴 酒仙：元素分离 混沌骑士：混沌之军 噬魂鬼：感染``幻影斧：镜像 变体精灵：波浪形态 娜迦海妖：镜像 幻影长矛手：神行百变``凤凰：超新星 帕克：相位转移 力丸：绝杀秘技 风暴之灵：球状闪电 传送所有的真闪烁都能躲避弹道，躲避发生在使用技能移动时 敌法师：闪烁 闪烁匕首：闪烁 远行鞋：传送 艾欧：传送(只有艾欧传送过去时可以躲避) 先知：传送 帕克：灵动之翼 痛苦女王：闪烁 熊灵：回归 回城卷轴：传送 孽主：黑暗之门 编织者：时光倒流 陈：忠诚考验 光之守卫：召回 变体精灵：替换复制品 隐身所有能获得隐身状态的技能技能都能躲避弹道，除非敌人的在弹道到达之前使用了反隐，但是必须要注意不同技能的渐隐时间 隐藏变为临时性的隐藏不能躲避弹道。 躲避与变为隐藏无关，而是与技能本身有关。这意味着隐藏技能不一定都能躲避弹道， 但是，利用合适的时机，可阻止弹道或一般技能，击中施法者或目标。 隐藏来源有一下技能 酒仙：元素分离 混沌骑士：混沌之军 大地之灵：残炎魔咒 噬魂鬼：吸收 幻影斧：镜像 娜迦海妖：镜像 殁境神蚀者：星体禁锢 幻影长矛手：神行百变 凤凰：超新星 帕克：相位转移 力丸：绝杀秘技 暗影恶魔：崩裂禁锢 巨牙海民：雪球 无敌变为无敌不能躲避弹道，但是可以在击中时减轻或使其效果无效。 攻击伤害和技能伤害会被忽略。有一些技能可以影响无敌单位。 无敌来源 祸乱之源：噩梦 酒仙：元素分离 混沌骑士：混沌之军 大地之灵：残岩魔咒 灰烬之灵：无影拳 灰烬之灵：激活残焰 风帐：龙卷风 虚空假面：时间漫游 佣兽：石像形态 祈求者：强袭飓风 主宰：无敌斩 噬魂鬼：吸收 噬魂鬼：感染 幻影斧：镜像 变体精灵：波浪形态 娜迦海妖：镜像 娜迦海妖：海妖之歌 殁境神蚀者：星体禁锢 幻影长矛手：神行百变 凤凰：超新星 帕克：相位转移 力丸：绝杀秘技 暗影恶魔：崩裂禁锢 狂风：龙卷风 风暴之灵：球状闪电 巨牙海民：雪球 可以被躲避的弹道任何单位和英雄的所有物理攻击的弹道都可以躲避 可以被躲避的技能亚巴顿：迷雾缠绕 赏金猎人：投掷飞镖 酒仙：醉酒云雾 钢背兽：粘稠鼻涕 育母蜘蛛：孵化蜘蛛 混沌骑士：混乱之箭 陈：赎罪 戴泽：剧毒之触 龙骑士：神龙摆尾 大地：投掷巨石 撼地者：回音击 虚灵之刃：虚化冲击 变体精灵：变体攻击 泥土傀儡：投石 娜迦海妖：诱捕 食人魔魔法师：引燃 神谕者：气运之末 幻影刺客：窒息之刃 幻影长矛手：灵魂之矛 痛苦女王：暗影突袭 阿托斯：致残 天怒法师：震荡光弹 狙击手：暗杀 斯温：风暴之拳 潮汐猎人：巨浪 修补匠：导热飞弹 复仇之魂：魔法箭 冥界亚龙：蝮蛇突袭 维萨吉：灵魂超度 风行者：束缚击 寒冬飞龙：碎裂冲击 冥魂大帝：冥火暴击 不可以被躲避的技能炼金术士：不稳定化合物 天穹守望者：闪光幽魂 爱人直升机：追踪导弹 哈斯卡：牺牲 拉西克：闪电风暴 巫妖：连环霜冻 莉娜：神灭斩 莱恩：死亡一指 美杜莎：秘术异蛇 米拉娜：流星风暴 瘟疫法师：死亡脉冲 痛苦女王：痛苦尖叫 拉比克：技能窃取 天怒法师：奥法鹰隼 幽鬼：幽鬼之刃 小小：投掷 树精卫士：寄生种子 巨牙海民：雪球 寒冬飞龙：碎裂冲击弹射 巫医：麻痹药剂","tags":[{"name":"Dota2","slug":"Dota2","permalink":"https://charlesliuyx.github.io/tags/Dota2/"},{"name":"Wiki","slug":"Wiki","permalink":"https://charlesliuyx.github.io/tags/Wiki/"},{"name":"Data Analysis","slug":"Data-Analysis","permalink":"https://charlesliuyx.github.io/tags/Data-Analysis/"}]},{"title":"【直观详解】Logistic Regression","date":"2017-09-04T07:21:50.000Z","path":"2017/09/04/LogisticRegression学习笔记/","text":"【阅读时间】17min - 22min【内容简介】从不同角度解释为何使用Logistic回归模型，解读模型的现实意义，详细解读为何使用以及什么是交叉熵损失函数。并详细梳理符号表达，对公式不再恐惧 什么是【回归（Regression）】回归（Regression）是一项模拟技术，用来从一个或多个解释变量中预测输出变量的值 什么是及为什么【Logistic Regression】回归（Regression）是用来预测的，比如给你一组虫子的腿长和翅膀长数据，让你判断虫子是A类虫还是B类虫。 逻辑回归则是用来预测二进制输出变量取值（如：是/不是）的预测技术 即输出变量只有两个值得预测技术 下文中将会从不同的角度 概率论角度首先，需要回忆一下几个概念 【大数定理】 $$ \\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{i=1}^n {X_i} = \\mu $$ 不断的采样一个随机变量，得到n个值，当n趋向于正无穷的时候，这个平均值就收敛于随机变量的期望 【中心极限定理】 大量相互独立{条件1}的随机变量，其均值的分布以正态分布{结论}为极限{条件2} 【贝叶斯公式】 默认你已经对条件概率了若指掌（在某件事情已经发生的情况下另一件事发生的概率），关于贝叶斯方法的前世今生，这个链接或许可以帮到你。 那贝叶斯公式是如何推出来的？ 问题描述我们需要求的问题是：你在校园里面随机游走，遇到了N个穿长裤的人（但是可能因为你高度近视你无法看出他们的性别），问，这N个人里面有多少个女生，多少个男生，即，穿裤子的人里面有多少个女生 解决过程 $$ 穿裤子的人中的女生比例 = \\frac{穿长裤的女生人数}{穿长裤的总人数} =\\\\ \\frac {U\\times P(Girl)\\times P(Paints|Girl)}{U\\times P(Boy)\\times P(Paints|Boy) + U\\times P(Girl)\\times P(Paints|Girl)}\\tag{1-1} $$ 化简上式，可以发现其实分母合起来就是 $P(Paints)$ ，分子其实就是既穿裤子又是女孩，整理得 $$ P(Girl|Paints) = \\frac{P(Girl) \\times P(Paints|Girl)}{P(Paints)} $$ 再一般化，用A表示穿裤子的，B表示女生$$P(B|A) = \\frac{P(B)\\times P(A|B)}{P(A)} = \\frac{P(AB)}{P(A)}\\tag{1-2}$$上式就是贝叶斯公式的一般形式，我们在推导中发现，正常人类对频率的感知和理解速度要高于对概率的。 比如“穿长裤的女生人数”这个概念，用总人数乘以女人比例，得出女生人数，再用女生人数乘以女生中穿裤子人数的比例得到穿裤子的女生人数。这一串推导感觉毫无困难。但如果读成：在A发生条件下，发成B的概率，会让人乍看下，感到有一定的理解困难。 我们常说Sense，我觉得这就是一种敏感，对条件概率表达方式的敏感，在你看到的时候，抓住那个最关键的点，不存在任何的迷惑 那Logistic Function和贝叶斯公式有什么联系呢？ 如果我们把公式（1-1）也符号化，$B_1$ 表示女生，$B_2$表示男生，$A$ 表示穿裤子$$P(B_1|A) = \\frac {P(B_1)P(A|B_1)}{P(B_2)P(A|B_2) + P(B_1)P(A|B_1)}\\tag{1-3}$$右边同时除以 $P(B_1)\\times P(A|B_1)$ ，并定义 $a = \\ln{\\left( \\frac{P(B_1)P(A|B_1)}{P(B_2)P(A|B_2)}\\right)}$ 直接由公式(1-3)可得到$$f(a) = \\frac{1}{1 + e^{-a}} \\tag{1-4}$$很熟悉的形式，其实就是logistic函数的一般形式（对数几率函数），而这个函数的值就是 $f(a)$ ，很明显，是一个概率 另一个很重要超级重要的常识就是：正态分布的的累计分布函数（就是从负无穷到x积分）和概率分布函数长得样子很像Logistic累计分布函数和概率密度函数，可能看到这句话很多人就已经真相大白了，应给无论从中心极限定理出发，还是从统计学概率论角度来看，概率分布存在的价值是为了描述自然界（现实）中的随机事件，构造函数本身就十分重要，不同的规律需要不同的函数去拟合 正太分布概率密度函数（左）累计密度函数（右） Logistic函数概率密度函数（左）累计密度函数（右） 统计学角度动机 - 需要解决什么问题在现实生活中，有时候需要探究某一事件 $A$ 发生的概率 $P$ （0 - 1 之间的一个数）与某些因素 $\\mathbf X = (X_1, X_2, \\ldots, X_p)’$ 之间的关系。（其中1到p是各种不同的因素） ☆ 【核心问题】考虑到很多情况下，$P$ 对 $\\mathbf X$ 的变化并不敏感，即 $\\mathbf X$ 需要发生很大的变化才能引起 $P$ 的微弱改变 比如，农药的用量和杀死害虫的概率之间，在农药用量在很小的范围内增长的时候，因为药效不够，杀死害虫的概率增长很慢。 因此，我们要构造一个关于 $P$ 的函数 $\\theta(P)$ ，使得它在 $P = 0$ 或 $P = 1$ 附近，$P$ 的微小变化对应 $\\theta(P)$ 的较大改变，同时，$\\theta(P)$ 要尽可能的简单。于是，我们可以构造一个函数（注意：构造函数是数学中很有效的手段，我们需要什么特性就用什么方法来构造一个满足我们需求的函数）c$$\\frac {\\partial \\theta(P)}{\\partial P} =\\frac{1}{P} +\\frac{1}{1-P}$$根据上述公式可以解得$$\\theta(P) =\\ln\\left(\\frac{P}{1-P}\\right)$$ 可视化 这个 $\\theta(P)​$ 就是Logit变换，可以看到，这个函数很符合我们的要求： $P = 0​$ 或 $P = 1​$ 附近，$P​$ 的微小变化对应 $\\theta(P)​$ 的较大改变 方案 - 如何解决这个问题为了建立因变量 $P$ 与自变量 $\\mathbf X$ 之间的合理变动关系，一个很自然的假设就是线性关系，也就是：$$P = \\mathbf X’ \\boldsymbol{\\beta}$$其中 $\\boldsymbol \\beta = (\\beta_1,\\beta_1,\\ldots,\\beta_p)$ 表示每一个不同因素对最终概率 $P$ 产生的影响（这个也可以写作，权重weight） 由需求可知，在某些情况下，$P = 0$ 或 $P = 1$ 附近，$P$ 对 $\\mathbf X$ 的变化并不敏感，简单的线性关系不能反映这一特征。此时，构造的 $\\theta(P)$ 就派上用场了$$\\ln\\left(\\frac{P}{1-P}\\right) = \\mathbf X’ \\boldsymbol{\\beta}$$进行一系列的公式推导有$$\\ln\\left(\\frac{P}{1-P}\\right) = \\mathbf X^\\mathrm T \\boldsymbol{\\beta} \\implies \\frac{P}{1-P} = e^{\\mathbf X^\\mathrm T \\boldsymbol{\\beta}} \\implies P = \\frac{e^{\\mathbf X^\\mathrm T \\boldsymbol{\\beta}}}{1 + e^{\\mathbf X^\\mathrm T \\boldsymbol{\\beta}}}$$则上述最后推出的就是Logistic回归模型 机器学习角度周志华《机器学习》，3.3 对数几率回归笔记 和统计学角度相同，我们的目的是依旧是完成一个二分类任务，输出标记 $y \\in {0,1}$ ，而线性回归模型产生的预测值 $z = \\boldsymbol w^{T}\\boldsymbol x + b$ 是实值，于是，我们需要把 z 转换为0/1值，最理想的是单位阶跃函数（unit-step function z &gt; 0➜y=1，z&lt;0➜y=1） 单单位阶跃函数不连续，不能微分，积分，求逆，于是我们希望找到能在一定程度上近似单位阶跃函数的替代函数（surrogate function），并希望它单调可微，答案很明显，就是对数几率函数（logistic function）$$y = \\frac{1}{1+e^{-z}}$$ z 为预测值，y 为输出，对数几率函数是一种Sigmoid函数【一种形状类似S的函数】，将$z = \\boldsymbol w^{T}\\boldsymbol x + b$ 带入上面的公式 $$y = \\frac{1}{1+e^{-(\\boldsymbol w^{T}\\boldsymbol x + b)}} \\implies \\ln(\\frac{y}{1-y}) = \\boldsymbol w^{T}\\boldsymbol x + b$$如果将 $y$ 作为 $\\mathbf x$ 作为正例的可能性，$1-y$ 为其反例的可能性$$\\frac {y}{1-y}$$上面的式子成为“几率”(odds)：表示 $\\mathbf x$ 是正例的相对可能性，对odds取对数得到“几率对数”(log odds，也就做logit) 生态学角度可以换一个角度来解读这个问题的前世今生 1798年的时候一个叫Malthus的英国牧师发现人口的变化率和人口的数目成正比，需要用数学的手法建立一个公式来表征这个现象，则，使用 $N(t)$ 这个函数来表示t时刻某个地区的总人口数（根据成正比）$$\\frac{dN(t)}{dt} = {rN(t)}$$ 其中，r是常数，表示 $N(t)$ 的变化率 直接解出这个方程$$N(t) = N_0e^{rt}$$这很明显是一个指数增长函数，其实也是种群增长的函数表示 但是问题也是很明显的：种群因为环境容量的限制一定是不能无限增长的，即，这个模型非常不靠谱，需要重新设计模型来复合现实中的情况。Pierre-François Verhulst 在1838年提出，构造一个函数$$\\frac{dN(t)}{dt} = {rN(t)}\\left(1 - \\frac{N(t)}{K}\\right)$$ K是一个常数，表示系统的容量（capacity） 令 $f(t) = \\frac{N(t)}{K}$ ，在方程两边同时除以 $K$ ，上述方程变为：$$\\frac{df(t)}{dt} = rf(1 - f)$$这也是Logistic方程的一般形式 总结从不同的角度来研究问题就会发现，其实很多时候我们解决一个问题具有一个相似的模式，包括大数定律，贝叶斯全概率公式是一切的基石和解决问题的主要工具 一个模型的建立规则依据数据的分布特征，而这里依托的一个关键信息就是：在靠近输入0，1两点的时候，y随x的变化不明显，线性模型没法很好的反应这个特征，所以就构造了一个逻辑回归模型来表示这个特征 并且Logistic回归模型的本质是一个概率模型，因为在描述该分类时，我们其实是以概率来衡量的 重要概念均方误差 Mean Squre Error MSE指参数估计值与参数真值之差平方的期望值，是一种目标函数（Objective Function），常用于线性回归$$MSE = \\frac{1}{n} \\sum_{t = 1}^n{(observed_t - predicted_t)}^2$$ 交叉熵 Cross Entropy又称为logloss，是Objective function的一种，也称Loss function or Coss Function 什么是熵我觉得这个问题必须搞明白一件事就是：什么是熵 Entropy 广义的定义是：熵是描述一个系统的无序程度的变量；同样的表述还有，熵是系统混乱度的度量，一切自发的不可逆过程都是从有序到无序的变化过程，向熵增的方向进行 有一个很神奇的解释是：熵字为火字旁加商。当时有位姓胡的学者作为普朗克的防疫。S(entropy)定义为热量Q与温度的比值，所以造字：熵 至于信息论上熵的概念更有意思，有兴趣可以转到 要理解这个Cross Entropy，必须了解它是用来干啥的？ 延伸：信息熵 交叉熵 相对熵的理解，需要跳转到另一篇笔记：什么是信息熵、交叉熵和相对熵 简单来说Cross Entropy可以表示可以度量最终训练结果于测试集的差异程度，MSE也是同样的作用。 换种更具体的说法：我们用p表示真实标记（训练样本标记）的分布，q是训练后的模型的预测标记（输出值标记）的分布，而交叉熵损失函数可以衡量p与q的相似性。 似然函数定义：给定联合样本值 $x$ 关于（未知 - 因为也是一边的自变量）参数 $\\theta$ 的函数$$L(\\theta|x) = f(x;\\theta)$$ $x$ 指联合样本随机变量 $X$ 取到的值，比如天气取值 $X$ =【晴，阴，雨，雪】$x$ = 晴 $\\theta$ 指未知参数，属于参数空间，比如正态分布的均值，方差等 $f(x;\\theta)$ 是密度函数，表示 $\\theta$ 参数下联合样本值 $x$ 的联合密度函数（所以这里不用|符号，|符号表达的意思是条件概率或条件分布） 从定义上，似然函数和密度函数是完全不同的两个数学对象：前者是关于 $\\theta$ 的函数，后者是关于 $x$ 的函数。中间的等号理解成函数值形式相等 这个等式表示的是对于事件发生的两种角度的看法。左边表示概率，右边表示可能性。要表达的含义都是：给定一个样本 $x$ 后，我们去测度这个样本出现的可能性到底有多大。说人话，比如样本空间是 $X =【晴，阴，雨，雪】$，函数表达的就是样本 $x$ = 晴在这个样本空间下发生的概率或可能性 从统计学的角度来说，这个样本的出现一定是基于一个分布的（比如二项分布，只正态分布等等），那么我们假设这个分布为 $f(x;\\theta)$ ，对于不同的 $\\theta$ 样本的分布不一样。 $f(x;\\theta)$ 函数表示的就是在参数 $\\theta$ 下 $x$ 出现的概率有多大（可以带入天气例子思考） $L(\\theta|x)$ 表示在给定样本 $x$ ，哪个参数 $\\theta$ 使得 $x$ 出现的可能性有多大。说人话，我们已经知道天气是晴天，哪个参数（可能是 $\\theta_1$ $\\theta_2$）使得这个函数值最大 对于Logistic Regression 为什么要用LogLoss - Cross Entropy了解了熵，和似然函数，我们可以开始看看在Logistic Regression的条件下为什么要用LogLoss，换句话也就是说，它一定有它的优势，我们采用，那么它有什么优势？ Logistic Regression的本质还是一个二分类问题，即Y = 0，or Y = 1 令 $P(Y=1|x) = \\pi(x)$ $P(Y=1|x) = 1 - \\pi(x)$ $y_i$ 表示i次试验，取值就是0 or 1（二分类问题） $\\pi(x) = \\frac{1}{1 + e^{-wx}}$ 是Logistic Function的表现形式，其中w相当于似然函数一节提到的 $\\theta$ 是需要求的参数（加深理解，其实在二分类问题中，Logistic函数就是一种形式上的概率分布的表现形式） 所以使用基本概率方法可以求解二分类的问题的似然函数 $$ \\ell(w) = \\prod_{i = 1}^{N} [\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i} $$ 注解：说白就和算扔N次硬币，一个连续正反事件串的概率是多少一个含义 看到乘法和指数，第一反应取对数，得到对数似然函数$$L(w) = \\sum_{i=1}^N{[y_ilog_a\\pi(x_i) + (1-y_i)log_a(1-\\pi(x_i))]}$$ 如果跟随我的步伐走到这一步，你会发现，这个形式，前半部分是“正例成立”的交叉熵，后半部是“反例成立”的交叉熵，说实话，叫做交叉熵和二项分布，伯努利过程分不开联系。在上面不远的地方已经详细定义了这几个符号代表的意思 我们发现，$-\\frac{L(w)}{N}$ 就是我们一直使用的Objective function or Loss Function or Cost Function（加负号才是最终的形式）。总之，训练的目的就是要求能够使得这个函数达到最小的参数，最终的目的还是计算出模型参数，就是 $w$ ，这个参数在上方的统计学角度，和机器学习角度都进行的讨论，重复阅读可以链接这些知识点 至于LogLoss的好处，一是取对数之后，乘法边加法，指数放下来，是凸函数，方便可以寻找最优解。二是加快了收敛速度，这里有个形象的步长比喻，可以想象成去了对数后，缩小了尺度，可以让最快梯度下降法要走的距离变短","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"}]},{"title":"Xpath-Wiki","date":"2017-08-28T19:00:33.000Z","path":"2017/08/28/Xpath使用指南/","text":"【阅读时间】查阅类文档【内容简介】Xpath相关使用法法和例子文档，以供查阅（➜ 后是对应语句的输出output） XPath 相关例子Note例子1123456789101112131415from lxml import etreesample1 = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;My page&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;Welcome to my &lt;a href=\"#\" src=\"x\"&gt;page&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;This is the first paragraph.&lt;/p&gt; &lt;!-- this is the end --&gt; &lt;/body&gt;&lt;/html&gt;\"\"\"def getxpath(html): return etree.HTML(html)s1 = getxpath(sample1) //绝对路径 text() 获取内容中的文字信息1s1.xpath('//title/text()') ➜ ['My page'] / 相对路径1s1.xpath('/html/head/title/text()') ➜ ['My page'] 获取属性src的值1s1.xpath('//h2/a/@src') ➜ ['x'] 获取所有属性href的值1s1.xpath('//@href') ➜ ['#'] 获取网页中的所有文本123456789101112131415s1.xpath('//text()')➜['\\n ', '\\n ', 'My page', '\\n ', '\\n ', '\\n ', 'Welcome to my ', 'page', '\\n ', 'This is the first paragraph.', '\\n ', '\\n ', '\\n'] 获取网页中的所有注释1s1.xpath('//comment()') ➜ [&lt;!-- this is the end --&gt;] 例子212345678910111213sample2 = \"\"\"&lt;html&gt; &lt;body&gt; &lt;ul&gt; &lt;li&gt;Quote 1&lt;/li&gt; &lt;li&gt;Quote 2 with &lt;a href=\"...\"&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Quote 3 with &lt;a href=\"...\"&gt;another link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;h2&gt;Quote 4 title&lt;/h2&gt;Something here.&lt;/li&gt; &lt;/ul&gt; &lt;/body&gt;&lt;/html&gt;\"\"\"s2 = getxpath(sample2) 获取所有li中的文本1s2.xpath('//li/text()') ➜ ['Quote 1', 'Quote 2 with ', 'Quote 3 with ', 'Something here.'] 获取第一个 第二个li中的文本，两种写法均可1s2.xpath('//li[position() = 1]/text()') ➜ ['Quote 1'] 1s2.xpath('//li[1]/text()') ➜ ['Quote 1'] 1s2.xpath('//li[position() = 2]/text()') ➜ ['Quote 2 with '] 1s2.xpath('//li[2]/text()') ➜ ['Quote 2 with '] 奇数 偶数 最后一个1s2.xpath('//li[position() mod2 = 1]/text()') ➜ ['Quote 1', 'Quote 3 with '] 1s2.xpath('//li[position() mod2 = 0]/text()') ➜ ['Quote 2 with ', 'Something here.'] 1s2.xpath('//li[last()]/text()') ➜ ['Something here.'] li下面a中的文本1s2.xpath('//li[a]/text()') ➜ ['Quote 2 with ', 'Quote 3 with '] li下a或者h2的文本1s2.xpath('//li[a or h2]/text()') ➜ ['Quote 2 with ', 'Quote 3 with ', 'Something here.'] 使用 | 同时获取 a 和 h2 中的内容1s2.xpath('//a/text()|//h2/text()') ➜ ['link', 'another link', 'Quote 4 title'] 例子312345678910111213sample3 = \"\"\"&lt;html&gt; &lt;body&gt; &lt;ul&gt; &lt;li id=\"begin\"&gt;&lt;a href=\"https://scrapy.org\"&gt;Scrapy&lt;/a&gt;begin&lt;/li&gt; &lt;li&gt;&lt;a href=\"https://scrapinghub.com\"&gt;Scrapinghub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\"https://blog.scrapinghub.com\"&gt;Scrapinghub Blog&lt;/a&gt;&lt;/li&gt; &lt;li id=\"end\"&gt;&lt;a href=\"http://quotes.toscrape.com\"&gt;Quotes To Scrape&lt;/a&gt;end&lt;/li&gt; &lt;li data-xxxx=\"end\" abc=\"abc\"&gt;&lt;a href=\"http://quotes.toscrape.com\"&gt;Quotes To Scrape&lt;/a&gt;end&lt;/li&gt; &lt;/ul&gt; &lt;/body&gt;&lt;/html&gt;\"\"\"s3 = getxpath(sample3) 获取 a 标签下 href 以https开始的1s3.xpath('//a[starts-with(@href, \"https\")]/text()') ➜ ['Scrapy', 'Scrapinghub', 'Scrapinghub Blog'] 获取 href=https://scrapy.org1s3.xpath('//li/a[@href=\"https://scrapy.org\"]/text()') ➜ ['Scrapy'] 获取 id = begin1s3.xpath('//li[@id=\"begin\"]/text()') ➜ ['begin'] 获取text = Scrapinghub1s3.xpath('//li/a[text()=\"Scrapinghub\"]/text()') ➜ ['Scrapinghub'] 获取某个标签下 某个参数 = xx1s3.xpath('//li[@data-xxxx=\"end\"]/text()') ➜ ['end'] 1s3.xpath('//li[@abc=\"abc\"]/text()') ➜ ['end'] 例子41234567891011121314151617181920212223sample4 = u\"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;My page&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;Welcome to my &lt;a href=\"#\" src=\"x\"&gt;page&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;This is the first paragraph.&lt;/p&gt; &lt;p class=\"test\"&gt; 编程语言&lt;a href=\"#\"&gt;python&lt;/a&gt; &lt;img src=\"#\" alt=\"test\"/&gt;javascript &lt;a href=\"#\"&gt;&lt;strong&gt;C#&lt;/strong&gt;JAVA&lt;/a&gt; &lt;/p&gt; &lt;p class=\"content-a\"&gt;a&lt;/p&gt; &lt;p class=\"content-b\"&gt;b&lt;/p&gt; &lt;p class=\"content-c\"&gt;c&lt;/p&gt; &lt;p class=\"content-d\"&gt;d&lt;/p&gt; &lt;p class=\"econtent-e\"&gt;e&lt;/p&gt; &lt;!-- this is the end --&gt; &lt;/body&gt;&lt;/html&gt;\"\"\"s4 = etree.HTML(sample4) 获取 class = test 标签中的所有文字12s4.xpath('//p[@class=\"test\"]/text()')➜ ['\\n 编程语言', '\\n ', 'javascript\\n ', '\\n '] 使用String来获得文字段； strip() 移除字符串收尾字符，默认为空格12345print (s4.xpath('string(//p[@class=\"test\"])').strip())➜编程语言python javascript C#JAVA 获取所有class属性中以content开始的1s4.xpath('//p[starts-with(@class,\"content\")]/text()') ➜ ['a', 'b', 'c', 'd'] 获取所有class属性中包含content的1s4.xpath(('//*[contains(@class,\"content\")]/text()')) ➜ ['a', 'b', 'c', 'd', 'e']","tags":[{"name":"Wiki","slug":"Wiki","permalink":"https://charlesliuyx.github.io/tags/Wiki/"},{"name":"crawl","slug":"crawl","permalink":"https://charlesliuyx.github.io/tags/crawl/"}]},{"title":"PDF复制粘贴去除多余的回车符","date":"2017-07-30T06:03:08.000Z","path":"2017/07/29/PDF复制粘贴去除多余的回车符/","text":"直接上解决步骤，但是只能适用于Windows平台，Mac这边可以尝试用Alfred + workflow来对剪切板操作来解决，或者用BetterTouchTool的自带个性化功能来尝试。只是一个思路，没有在Mac系统尝试 下载 Autohotkey ，安装（这一步都卡住那估计救不了了） 桌面右键 ➜ 新建 ➜ 创建新的AutoHotkey Script 右键创建的文件 ➜ 选择 Edit Script 出来一个记事本 编辑记事本文件，在已经有的内容下直接加上 1234567891011121314151617#IfWinActive ahk_class classFoxitReader^c:: old := ClipboardAll clipboard := &quot;&quot; send ^c clipwait 0.1 if clipboard = clipboard := old else &#123; tmp := RegExReplace(clipboard, &quot;(\\S.*?)\\R(.*?\\S)&quot;, &quot;$1 $2&quot;) clipboard := tmp StringReplace clipboard, clipboard, % &quot; &quot;, % &quot; &quot;, A clipwait 0.1 &#125; old := &quot;&quot; tmp := &quot;&quot;return 这里有个问题 IfWinActive ahk_class classFoxitReader 第一行的classFoxitReader 是指的你用什么程序打开PDF 如果是FoxitReader就是classFoxitReader 如果是Acrobat Adobe就是AcrobatSDIWindow 可以用Autohotkey中的 WinGetClass 来获得某一个窗口的ahk_class 保存退出 桌面上双击你刚刚编辑的文件，可以看到右下角出现了一个H形状的图标 大功告成，这时候你再试试去PDF文档里面ctrl + c就没有回车符了（当然，段落还是无法区分的），也不一定，这一段既然是脚本语言，那就有无限的可能性，就看你的算法实现能力了对吧！","tags":[{"name":"Tools","slug":"Tools","permalink":"https://charlesliuyx.github.io/tags/Tools/"},{"name":"Autohotkey","slug":"Autohotkey","permalink":"https://charlesliuyx.github.io/tags/Autohotkey/"}]},{"title":"LeetcodeNote","date":"2017-07-01T07:18:41.000Z","path":"2017/07/01/LeetcodeNote/","text":"算法培训课程基本模型汇总笔记 线基本模型数学归纳法树基本模板 Draw/Equation -&gt; Tree shape Define TreeNode 本点信息必然是辅助变量，计入TreeNode 孩子信息决定TreeNode的形状 任何第一次走的节点，如果不能走，一定要画出来打一把叉 Binary Search123456789101112131415Public int func(T[] array, V tartget )&#123; int pos = -1; int start = 0; int end array.length - 1; while ( start &lt;= end )&#123; int mid = start + (end - start)/2; if ( f(a[mid]) &lt;= target )&#123; pos = mid; start = mid + 1; &#125; else &#123; end = mid - 1; &#125; &#125; return pos;&#125; Bottom up - Recursion123456789101112131415public &lt;T_P&gt; func(T_v_1, v1 …)&#123; checkhastreeNode(); return helper(root(T_v_1, v_1, …))&#125;private &lt;T_P&gt; helper(T_v_1, v1, …)&#123; resultchildfirst = helper(childFirst); … resultchildlast = helper(childLast); -&gt; result by childs //generate cur node's result; return result;&#125; DFS12345678910111213141516171819202122232425262728public class DFSTree &#123; public Type_R func(T_1, e1, T_2, e2)&#123; checkrootexists(); TreeNode[] array = new TreeNode[TREE_HEIGHT]; Stack&lt;TreeNode&gt; stack = Stack&lt;&gt;(); stack.push(root); while (!stack.Empty())&#123; TreeNode curNode = stack.pop(); Operation at node; stack.push(childLast); … stack.push(childFirst); &#125; return result; &#125; private class TreeNode&#123; T_V_1 field_1; … T_V_q field_q; int _height; &#125; BFS1234567891011121314151617181920212223242526272829303132public class BFS &#123; public TypeR func(T_1 v_1, T_p, v_p) &#123; checkexistroot(); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while ( !queue.isEmpty() )&#123; int size = queue.size(); for ( int I = 0; I &lt; size; i++ )&#123; TreeNode node = queue.remove(); op at node; queue.add(childFirst); … queue.add(childLast); &#125; update var_l,…,var_k for next level &#125; return result; &#125; private class TreeNode&#123; T_1 field_1; … &#125;&#125; 图基本模板","tags":[{"name":"Algorithm","slug":"Algorithm","permalink":"https://charlesliuyx.github.io/tags/Algorithm/"},{"name":"Leetcode","slug":"Leetcode","permalink":"https://charlesliuyx.github.io/tags/Leetcode/"}]},{"title":"幕布-全平台笔记思维导图工具","date":"2017-06-17T17:30:47.000Z","path":"2017/06/17/幕布-全平台笔记思维导图工具/","text":"利益相关：幕布深度使用用户 向大家强烈自来水一款从知乎上了解到的效率神器：幕布，简直相见恨晚，自从它4月正式上线后就一直使用，对我的日常生活、学习和计划帮助巨大（个人情况：硕士CE在读，ML方向，效率至上主义者，简约UI风格拥护者） 幕布是一款思维管理工具，可以用来做笔记，梳理思路，做待办事项等等等等。 人类的记忆是有缺陷的，计算机能帮助人进行记忆。我们可以记住大方向的条目，再借助一个笔记软件来唤起我们的记忆。树形结构是一种极为高效的模式及手段。 笔记软件很多，思维导图软件很多，但是能同时满足以下几点的我找了很久都没有找到，直到遇到幕布。如果你和我也有同样的需求，真心的希望这款优秀的软件能帮助到你，提高的你的日常效率，让每一次阅读，每一个计划都高效落地 UI简约，专注于层次输入本身幕布的官网是这样的： 幕布官网 幕布作为一款笔记软件编辑界面是这样的，幕布专注于层次化输入，每一个输入对于幕布来说都是一个条目，条目就是我们进行知识梳理的主干 幕布编辑界面 每一个操作都提倡使用快捷键，拒绝鼠标 + 键盘混用带来的输入思路打断的低效率 幕布快捷键页面 全平台，云存储和同步，客户端离线编辑答主因为同时使用各种设备：12.9寸iPad，Mac，Windows + Linux 台式机和iPhone手机几个工作平台，平常使用电脑进行笔记记录，碎片化时间使用个手机进行背诵记忆等，需要一款全平台的笔记软件。 而幕布，只要有浏览器，有网络就能流畅使用，有离线需求的用户，也可以通过客户端的形式满足日常编辑的需求。 幕布全平台 一键生成思维导图选择幕布的重要理由之一，废话不多说，上Gif！ 幕布快捷键页面 用过各种软件，Coggle是UI最漂亮的，但是基本的演示需求幕布完全可以满足，清楚明了 那么话说回来了，幕布可以用来干什么呢？下面就展示几个主要应用场景（有我自己的，也有来自于幕布使用趣味案例） 首先，官网给出了一份幕布产品引导，其中详细介绍了幕布的使用场景 读书笔记 方案计划 流程说明 等等 下面案例一些答主自己日常的一些特殊使用场景，基本应用比如做笔记，待办事项，做日程规划等不一一列出来了，就是基本的笔记需求。 TED演讲笔记没有遇到幕布之前，我经常看TED的各种演讲，用于开拓视野，进行英语表达的积累，做笔记的速度太慢，太不方便，遇到幕布之后，我将TED的视频中很关键的内容记录成幕布的条目层次，之后利用碎片化的时间使用手机客户端进行记忆和背诵，极大的丰富了我的谈资（记住的东西才能侃，有条理有依据的说辞才有说服力） 埃里克 哈世延：下一个科学界大突破是什么 对于这个TED笔记例子 演讲人大体思路 经典的单句和例子（中英文） 另一方面，因为幕布的全平台特性，我会用碎片化的时间利用电脑端的幕布来进行背诵（TED的演讲内容对于积累对应领域的英文表达方式有很大的帮助） 程序设计和Presentation编程前先走流程和功能设计是我平时的习惯，这里有一个很简单的Server-Client模式的练习设计用法：设计程序功能，直接一键思维导图展示，PPT完全不用做了，非常愉悦 程序Feature Dota2 Wiki在国外我发现Dota2维基十分的给力，作为Dota2玩家有一些施法距离，施法机制等有时候需要查看（进阶），但是使用网站一方面内容太多，国内访问实在太慢了，而且搜索功能也做的不好，至于我做了什么事情，各位看gif自行感受 Dota2 【利益相关：正在制作，预计Ti7前可以上线，希望也能通过数据帮助到中国军团吧，作为一个做计算机的程序员也希望贡献自己的一份力量】 期末考试复习幕布可以帮助我们把书读薄，我们知道所有的书的特点就是具有层次化，每一本非常优秀的教材都有一套自己对于本学科的知识体系的理解和层次化抽象，之前我进行期末复习需要的时间大概是7天左右，有了幕布可以把时间缩减为3天或者更短 期末复习笔记总览 期末复习笔记具体内容 幕布精选在幕布里，学习知乎模式，你也可以分享自己中意的作品，获得点赞，在后面讨论，甚至有打赏功能，因为软件本身还很年轻，一切还在发展阶段，对于我本人来说，幕布精选的内容只是锦上添花，我个人不太需求这个功能，但是其中还有一份驾考总结挺有用的，哈哈 幕布精选 幕布精选打赏功能 总结和杂七杂八我现在的习惯是，只要是读微信公众号的文章，做笔记，读书等，都会用幕布进行记录和整理，感觉提升效率十分明显（节省了我30-40%左右的时间，每天） 幕布提高我的三个能力 整理和总结的能力【如何把书读薄】 层次化思维能力【有组织的整理自己的知识体系和思路模式，加强效率，节省时间】 背诵能力【全平台（手机），我对碎片化时间能有效利用，我可以多次重复背诵需要背诵的内容】 最后，谢谢你阅读本答案到这个位置，对于我来说，幕布这种层次化的思维模式解决了我当年考高考时候的问题：什么学习方法是最好的？我觉得幕布的层次化整理知识的能力就是答案，幕布提供的是一张纸，一支笔，最后使用幕布能把你的学习生活提升到什么程度，完全取决于你的能力本身，幕布只是工具，帮助你整理你的大脑，帮你进行背诵，方便查阅。 工具永远是工作，创造效益的永远是你，未来也是人创造的，不是工具。 【利益相关，使用我的幕布分享链接可以获得15天的免费高级版试用机会，跪求点击注册！hohohohoho】 我的分享链接 幕布，绝对是一个神器，希望能帮助到各位，提升效率，创造更大的价值！","tags":[{"name":"Tools","slug":"Tools","permalink":"https://charlesliuyx.github.io/tags/Tools/"},{"name":"Mubu","slug":"Mubu","permalink":"https://charlesliuyx.github.io/tags/Mubu/"}]},{"title":"深入浅出看懂AlphaGo如何下棋","date":"2017-05-27T18:51:22.000Z","path":"2017/05/27/AlphaGo运行原理解析/","text":"问题分析围棋问题，棋盘 19 * 19 = 361 个交叉点可供落子，每个点三种状态，白（用1表示），黑（用-1表示），无子（用0表示），用 $\\vec s$ 描述此时棋盘的状态，即棋盘的状态向量记为 $ \\vec s$ （state首字母）。 $$\\vec s = (\\underbrace{1,0,-1,\\ldots}_{\\text{361}})\\tag {1-1}$$假设状态 $\\vec s$ 下，暂不考虑不能落子的情况， 那么下一步可走的位置空间也是361个。将下一步的落子行动也用一个361维的向量来表示，记为 $\\vec a$ （action首字母）。$$\\vec a = (0,\\ldots,0,1,0,\\ldots)\\tag {1-2}$$公式1.2 假设其中1在向量中位置为39，则 $\\vec a$ 表示在棋盘(3,1)位置落白子，3为横坐标，1为列坐标 有以上定义，我们就把围棋问题转化为。 任意给定一个状态 $\\vec s$ ，寻找最优的应对策略 $\\vec a$ ，最终可以获得棋盘上的最大地盘 总之 看到 $\\vec s$ ，脑海中就是一个棋盘，上面有很多黑白子 看到 $\\vec a$ ，脑海中就想象一个人潇洒的落子 接下来的问题是，如何解决这样一个问题呢？ 先上论文！干货第一 Mastering the game of Go with deep neural networks and tree search 问题解决首先想到，棋盘也是一幅图像，那么在当时最好用的图像处理算法就是深度卷积神经网络（Deep Convolutional Neural Network）。 深度卷积神经网络——策略函数（Policy Network）关于什么是CNN，这篇文章十分靠谱，深入浅出的讲解了什么是CNN An Intuitive Explanation of Convolutional Neural Networks （好像原地址挂了）（5.29更新，原地址已经恢复，原地址的排版更好，估计之前那个博主在进行博客的整理） 大致可以理解为： CNN例子 对一副图像进行处理，给定很多样本进行训练，使得最后的神经网络可以获得指定（具有分类效果）的输出。 比如，根据上图可以观察到（这是一个已经训练好的神经网络），最右侧的输出是[0.01 , 0.04 , 0.94 , 0.02]，其中第三个值0.94代表的是boat，接近1，所以我们判断这幅图片中有船这个物体（类似的，如果使用这幅图像进行训练，那么指定输出应该是[0, 0, 1, 0]，因为图中只有船这个物体） 在Deep Learning中，卷积层的中的Filter也需要训练，也就是说我们使用已有数据来学习图像的关键特征，这样，就可以把网络的规模大幅度的降低 总而言之，CNN可以帮助我们提取出图像中有实际含义的特征，那么这和围棋又有什么关系呢？我们来看看Deepmind团队是怎么运用CNN来解决围棋问题。 深度卷积神经网络解决围棋问题2015年，Aja Huang在ICLR的论文Move Evaluation in Go Using Deep Convolutional Neural Networks中就提出了如何使用CNN来解决围棋问题。 他从围棋对战平台KGS上获得了人类选手的围棋对弈棋谱，对于每一个状态 $ \\vec s$，都会有一个人类进行 $ \\vec a$ 的落子，这也就是一个天然训练样本 $ \\langle \\vec s,\\vec a\\rangle $，如此可以得到3000万个训练样本。 之后，将 $ \\vec s$ 看做一个19*19的二维图像（具体实现依据论文输入数据是19*19*48（48是这个位置的其他信息，比如气等信息，激励函数用的 tanh）使用CNN进行训练，目标函数就是人类落子向量 ${\\vec a}’$，通过使用海量的数据，不断让计算机接近人类落子的位置。就可以得到一个模拟人类棋手下棋的神经网络。 使用训练的结果，我们可以得到一个神经网络用来计算对于每一个当前棋盘状态 $ \\vec s$ ，所对应的落子向量 $ \\vec a$ 的概率分布（之所以是概率分布，是因为，计算好的神经网络，输出一般是一个0-1之间的浮点数，越接近1的点表示在这个位置越接近人类的风格，也可以等同于作为人类概率最大的落点。$$\\vec a=f(\\vec s) \\tag{2-1}$$根据公式2.1，我们记 $f()$ 为$P_{human}(\\vec s)$ ，论文中也叫做Policy Network，也称策略函数。表示的含义是 在状态 $\\vec s$ 下，进行哪一个落子 $\\vec a$ 是最接近人类风格的 计算出来的直观结果，对应到棋盘上如下图，可以看到，红色的区域的值有60%，次大值位于右方，是35%（此图来自于AlphaGo论文） Policy Network 还记得刚刚举得船图的例子嘛？可以类比一下，机器发现现在的状态 $ \\vec s$ 和之前的某一种类型有些类似，输出是一个1*361的向量，其中有几个值比较大（接近1就是100%），那么就用这个值当做下一个 $ \\vec a$ 的位置。不幸的，这种训练方法有很大的局限的，可以直观想到的是，如果对战平台上数据本身就都是俗手，那不是训练出来一个很蠢的神经网络嘛？棋力如何呢？ 深度卷积网络策略的棋力很不幸，据Aja Huang本人说，这个网络的棋力大概相当于业余6段所有的的人类选手。远远未能超过当时最强的围棋电脑程序CrazyStone。 既然比不过，那么就学习它，Aja Huang打算把 $P_{human}(\\vec s)$ 和CrazyStone结合一下，那么问题就来了， CrazyStone是怎么来解决围棋问题的呢？ 这是Aja Huang的老师Remi Colulum在2006年对围棋AI做出的另一大重要突破 干货论文送上 MCTS Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search MCTS 蒙特卡洛搜索树——走子演算（Rollout）蒙特卡洛搜索树（Monte-Carlo Tree Search）是一种大智若愚的方法，它的基本思想是： 首先模拟一盘对决，使用的思路很简单，随机 面对一个空白棋盘 $\\vec s_0$，最初我们对棋盘一无所知，假设所有落子的方法分值都相等，设为1 之后，【随机】从361种方法中选一种走法 $\\vec a_0$，在这一步后，棋盘状态变为 $\\vec s_1$。之后假设对方也和自己一样，【随机】走了一步，此时棋盘状态变为 $\\vec s_2$ 重复以上步骤直到 $\\vec s_n$并且双方分出胜负，此时便完整的模拟完了一盘棋，我们假设一个变量r，胜利记为1，失败则为0 那么问题就来了，如果这一盘赢了，那意味着这一连串的下法至少比对面那个二逼要明智一些，毕竟我最后赢了，那么我把这次落子方法 $(\\vec s_0, \\vec a_0)$ 记下来，并把它的分值变化：$$\\text{新分数} = \\text{初始分数} + r \\tag{2-2}$$同理，可以把之后所有随机出来的落子方法 $(\\vec s_i, \\vec a_i)$ 都应用2-2公式，即都加1分。之后开始第二次模拟，这一次，我们对棋盘不是一无所知了，至少在 $\\vec s_0$ 状态我们知道落子方法 $\\vec a_0$ 的分值是2，其他都是1，我们使用这个数据的方法是：在这次随机中，我们随机到 $\\vec a_0$ 状态的概率要比其他方法高一点。 之后，我们不断重复以上步骤，这样，那些看起来不错（以最后的胜负来作为判断依据）的落子方案的分数就会越来越高，并且这些落子方案也是比较有前途的，会被更多的选择。 $$ score(\\vec s) = \\begin{pmatrix} r_{11} & r_{12} & \\cdots & r_{1n} \\\\ r_{21} & r_{22} & \\cdots & r_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ r_{n1} & r_{n2} & \\cdots & r_{nn} \\end{pmatrix} $$ 如上述公式所述，n=19，每一个状态 $\\vec s$ 都有一个对应的每个落子点的分数，只要模拟量足够多，那么可以覆盖到的 $\\vec s$ 状态就越多，漏洞就越来越小（可以思考李世石的神之一手，是否触及到了AlphaGo1.0的软肋呢？即没有考虑到的状态 $\\vec s$ ） 最后，当进行了10万盘棋后，在此状态选择那个分数最高的方案落子，此时，才真正下了这步棋。这种过程在论文里被称为Rollout 蒙特卡洛搜索树的方法十分的深刻精巧，充满的创造力，它有一些很有意思的特点： 没有任何人工决策的if else逻辑，完全依照规则本身，通过不断的想象（随机）来进行自我对弈，最后提升这一步的质量。有意思的是，其实这也是遵照了人类下棋的思维模式（模仿，只是这一次模仿的不是下棋风格，而是人类思考的方式。十分奇妙，人从飞鸟中受到启发发明了飞机，从鱼身上受到启发发明了潜艇，现在，机器学习的程序，通过学习人类使自身发生进化），人类中，水平越高的棋手，算的棋越多，只是人类对于每一个落子的判断能力更加强大，思考中的棋路，也比随机方式有效的多，但是机器胜在量大，暴力的覆盖到了很多情况。注意，这一个特点也为之后的提高提供了思路。 MCTS可是持续运行。这种算法在对手思考对策的时候自己也可以思考对策。在对方思考落子的过程中，MCTS也可以继续进行演算，在对面落子后，在用现在棋盘的情况进行演算，并且之前计算的结果一定可以用在现在情况中，因为对手的下的这步棋，很可能也在之前演算的高分落子选择内。这一点十分像人类 MCTS是完全可并行的算法 Aja Huang很快意识到这种方法的缺陷在哪里：初始策略（或者说随机的落子方式）太过简单。就如同上面第一条特点所说，人类对每种 $\\vec s$ （棋型）都要更强的判断能力，那么我们是否可以用 $P_{human}(\\vec s)$ 来代替随机呢？ Aja Huang改进了MCTS，每一步不使用随机，而是现根据 $P_{human}(\\vec s)​$ 计算得到 $\\vec a​$ 可能的概率分布，以这儿概率为准来挑选下一个 $\\vec a​$。一次棋局下完之后，新分数按照下面的方式来更新$$\\text{新分数} = \\text{调整后的初始分} + \\text{通过模拟的赢棋概率} \\tag{2-3}$$如果某一步被随机到很多次，就应该主要依据模拟得到的概率而非 $P_{human}(\\vec s)$ ，就是说当盘数不断的增加，模拟得到的结果可能会好于 $P_{human}(\\vec s)$ 得到的结果。所以 $P_{human}(\\vec s)$ 的初始分会被打个折扣，这也是公式2-3中的调整后的初始分的由来 $$ \\text{调整后的初始分} = \\frac{P_{human}(\\vec s)}{(\\text{被随机到的次数} + 1)} \\tag{2-4} $$ 如此一来，就在整张地图上利用 $P_{human}(\\vec s)$ 快速定位了比较好的落子方案，也增加了其他位置的概率。实际操作中发现，此方案不可行，因为计算这个 $P_{human}(\\vec s)$ 太慢了太慢了 一次 $P_{human}(\\vec s)$ 的计算需要3ms，随机算法1us，慢了3000倍，所以，Aja huang训练了一个简化版本的 $P_{human-fast}(\\vec s)$ ，把神经网络层数、输入特征减少，耗时下降到2us，基本满足了要求。 更多的，策略是，先以 $P_{human}(\\vec s)$ 开局，走前面大概20步，之后再使用 $P_{human-fast}(\\vec s)$ 走完剩下的到最后。兼顾速度和准确性。 综合了深度卷积神经网络和MCTS两种方案，此时的围棋程序已经可以战胜所有其他电脑，虽然和其他人类职业选手还有一定的差距。 2015年2月，Aja Huang在Deepmind的同事在顶级学术期刊nature上发表的文章 Human-level control through deep reinforcement learning 用神经网络打游戏。这篇文章，给AlphaGo提供的了新的方向 强化学习——局面函数（Value Network）强化学习（Reinforcement learning）用来实现左右互搏和自我进化，首先说说这篇论文干了一件什么事情，Deepmind团队的大牛们使用强化学习的方法在红白机上打通了200多个游戏，大多数得分都要比人好。 什么是强化学习那什么是强化学习呢？这里推荐莫烦大神的 什么是强化学习 系列教程的知乎专栏，以及另一篇强化学习指南 后者对强化学习的基本概念，实现方法进行全面的讲解，含有公式推导。还有两篇我自己做的笔记，什么是强化学习，强化学习算法介绍 对于强化学习（Reinforcement learning），它是机器学习的一个分支，特别善於控制一只能够在某个环境下自主行动的个体 (autonomous agent)，透过和环境之间的互动，例如 sensory perception 和 rewards，而不断改进它的 行为。 比如，吃豆人游戏，自主行动的个体就是控制的吃豆人，环境就是迷宫，奖励就是吃到的豆子，行为就是上下左右的操作。 强化学习的输入是 状态 (States) = 环境，例如迷宫的每一格是一个 state 动作 (Actions) = 在每个状态下，有什么行动是容许的 奖励 (Rewards) = 进入每个状态时，能带来正面或负面的 价值 (utility) 输出是 方案 (Policy) = 在每个状态下，你会选择哪个行动？也是一个函数 所以，我们需要根据S，A，R，来确定什么样的P是比较好的，通过不断的进行游戏，获得大量的交互数据，我们可以确定在每一个状态下，进行什么动作能获得最好的分数，而强化学习也就是利用神经网络来拟合这个过程。 例如，打砖块游戏有一个秘诀是把求打到墙后，这样球能自己反弹得分，强化学习程序在玩了600盘后，学到了这个秘诀。也就是说程序会在每一个状态下选择那个更容易把球打到墙后面去的操作。如下图，球快要把墙打穿的时候，评价函数 $v$ 的值会大幅度上升 打墙游戏的评价函数图 我们可以发现，强化学习的基本思路和MCTS后异曲同工之妙，也是在对游戏完全没有了解的情况，通过不断的训练（进行多盘对弈，和获得进行行动后的分数反馈）来进行训练，自我提升。 利用强化学习增强棋力参考这种思路，Aja Huang给围棋也设计了一个评价函数 $v(\\vec s)$ 。此函数的功能是：量化评估围棋局面。使用$v(\\vec s)$可以让我们在MCTS的过程中不用走完全局（走完全盘耗时耗力，效率不高）就发现已经必败。 在利用 $P_{human}(\\vec s)$ 走了开局的20步后，如果有一个 $v(\\vec s_i)$ （i为当前状态）可以直接判断是否能赢，得到最后的结果r，不需要搜索到底，可以从效率（剪枝，优化算法时间复杂度）上进一步增加MCTS的威力。 很可惜的，现有的人类棋谱不足以得出这个评价函数。所以Aja Huang决定用机器和机器对弈的方法来创造新的对局，也就是AlphaGo的左右互搏。 自对弈 神经网络的训练过程和结构 先用 $P_{human}(\\vec s)$ 和 $P_{human}(\\vec s)$ 对弈，比如1万盘，得到1万个新棋谱，加入到训练集中，训练出 $P_{human-1}(\\vec s)$ 。 使用$P_{human-1}(\\vec s)$和$P_{human-1}(\\vec s)$对弈，得到另1万个新棋谱，加入训练集，训练出$P_{human-2}(\\vec s)$。 同理，进行多次的类似训练，训练出$P_{human-n}(\\vec s)$，给最后的新策略命名为$P_{human-plus}(\\vec s)$ （感觉一下，这个$P_{human-plus}(\\vec s)$ 应该挺强力的！这里回顾一下$P_{human}(\\vec s)$是什么：是一个函数，$\\vec a=f(\\vec s)$ 可以计算出当前 $\\vec s$ 下的落子 $\\vec a$ 的分布概率） 使用$P_{human-plus}(\\vec s)$和$P_{human}(\\vec s)$进行对弈，发现$P_{human-plus}(\\vec s)$胜率80%，自对弈的方法被证明是有效的。（这里有一个想法，我在之前，一直加粗随机，之所以自对弈有效，就是因为整过MCTS过程中从来没有放弃过随机，如此一来，大量的计算，就更可能覆盖到更多的可能性，对提高棋力可以产生有效的作用同时。因为概率的问题，不断的自我对弈肯定造成下棋的路数集中，后面也会有体现） 但是事实并没有那么美好，Aja Huang发现，使用$P_{human-plus}(\\vec s)$来代替$P_{human}(\\vec s)$进行MCTS反而棋力会下降。 Aja Huang认为是$P_{human-plus}(\\vec s)$走棋的路数太集中，而MCTS需要更加发散的选择才能有更好的效果。 计算局部评价函数（Value Network）考虑到$P_{human-plus}(\\vec s)$的下法太过集中，Aja Huang计算 $v(\\vec s)$ 的策略是： 开局先用$P_{human}(\\vec s)$走L步，有利于生成更多局面 即使如此，Aja Huang还是觉得局面不够多样，为了进一步扩大搜索空间，在L+1步时，完全随机一个 $\\vec a$ 落子，记下这个状态 $v(\\vec s_{L+1})$ 之后使用$P_{human-plus}(\\vec s)$来进行对弈，直到结束时获得结果r，如此不断对弈，由于L也是一个随机数，我们可以得到，开局、中盘、官子等不同阶段的很多局面 $\\vec s$，和这些局面对应的结果r 有了这些训练样本 $\\langle \\vec s,r\\rangle$，还是使用神经网络，把最后一层改成回归而非分类（这里不是用的分类，而是用的回归，拟合），就得到了一个 $v(\\vec s)$ 来输出赢棋的概率 如上图所示，$v(\\vec s)$ 可以给出下一步落在棋盘上任意位置后，如果双方都用$P_{human-plus}(\\vec s)$来走棋，我方赢棋的概率。实验表明，仅仅使用$P_{human}(\\vec s)$来训练 $v(\\vec s)$ 效果不如$P_{human-plus}(\\vec s)$，强化学习是确实有效的。 总结，强化学习的$P_{human-plus}(\\vec s)$主要是用来获得 $v(\\vec s)$ 局部评估函数。表示的含义是 在状态 $\\vec s$ 下，局面的优劣程度，或者说此时的胜率是多少 $v(\\vec s)$ 局部评估函数拥有在线下不断自我进化的能力（这也是AlphaGo可以随时间越来越强的最重要的部分） 感谢你看到这里，我们已经拥有： $P_{human}(\\vec s)$ 我的老师是人类！ MCTS 乱下，我只看输赢 $v(\\vec s)$ 我能判断局势 有了这些我们距离AlphaGo已经不远了 AlphaGo MTCS流程图解 Aja Huang使用MCTS框架融合局面评估函数 $v(\\vec s)$ 的策略是： 使用$P_{human}(\\vec s)$作为初始分开局，每局选择分数最高的方案落子 到第L步后，改用$P_{human-fast}(\\vec s)$把剩下的棋局走完，同时调用 $v(\\vec s_L)$，评估局面的获胜概率，按照如下规则更新整个树的分数​$$\\text{新分数} = \\text{调整后的初始分} + 0.5*\\text{通过模拟得到的赢棋概率} + 0.5*\\text{局面评估分} \\tag {3-1}$$ 前两项和原来一样 如果待更新的节点就是叶子节点，局面评估分就是 $v(\\vec s_L)$ 如果是待更新的节点是上级节点，局面评估分是该叶子节点 $v(\\vec s)$ 的平均值 如果 $v(\\vec s)$ 是表示大局观，$P_{human-fast}(\\vec s)$表示快速演算，那么上面的方法就是二者的并重，并且Aja Huang团队已经用实验证明0.5 0.5的权重对阵其他权重有95%的胜率 详解AlphaGo VS 樊麾 对局走下某一步的计算过程 详解AlphaGo走某一步棋的过程1 a图使用局部评估函数计算出 $\\vec s$ 状态下其他落子点的胜率 b图MCTS中使用局部评估函数加 $P_{human}(\\vec s)$ 得出的结果 c图MCTS中使用$P_{human}(\\vec s)$（复合算法）和$P_{human-fast}(\\vec s)$走子走到底的结果 详解AlphaGo走某一步棋的过程2 d图深度卷积神经网络使用策略函数计算出来的结果 e图使用公式3-1和相关流程计算出的落子概率 f图演示了AlphaGo和樊麾对弈的计算过程，AlphaGo执黑，樊麾执白。红圈是AlphaGo实际落子额地方。1，2，3和后面的数字表示他想象中的之后樊麾下一步落子的地方。白色方框是樊麾的实际落子。在复盘时，樊麾认为1的走法更好（这说明在樊麾落子后AlphaGo也在进行计算） 总结由于状态数有限和不存在随机性，象棋和五子棋这类游戏理论上可以由终局自底向上的推算出每一个局面的胜负情况，从而得到最优策略。例如五子棋就被验证为先手必胜。 AlphaGo的MCTS属于启发式搜索算法 启发式搜索算法：由当前局面开始，尝试看起来可靠的行动，达到终局或一定步数后停止，根据后续局面的优劣反馈，选择最有行动。通俗来说，就是”手下一招子，心想三步棋“ 围棋是一个NP问题，要穷举的话，解空间巨大。现代优化算法的经典之处在于，从围棋的规则来看，在某一个状态，必定有一个或几个较优解，整个AlphaGo就是想方设法的去找这个较优解。利用局面评估函数来对MCTS进行剪枝的思路十分精彩。利用上面的3个算法，结合庞大的并行运算能力，还有Aja Huang团队的辛苦付出，造就了AlphaGo的奇迹。 使用不同组件AlphGo1.0的棋力 最终棋力结果 上图显示了各种算法的棋力，Rollout是走棋演算，也就是MCTS，Value Network是 $v(\\vec s)$ 局面评估函数，Policy Network 是结合$P_{human-plus}(\\vec s)$和$P_{human}(\\vec s)$后计算的策略函数（下一步走在哪里胜率高的深度卷积神经网络） 整个AlphaGo使用的技术，深度卷积网络，强化学习神经网络，都是炙手可热的领域，近年来发展迅猛，日新月异。AlphaGo已经完成了自己历史使命，借助棋类的巅峰【围棋】为叩门砖打开了机器学习自我进化的大门 李世石 VS AlphaGo 1.0——第四局78手挖 赛后AlphaGo之父给出的关键信息：李世石78手“挖”是AlphaGo认为概率极小的点，这一手之后导致的状态 $\\vec s$ 进入到了AlphaGo能处理的范围之外，即之前AlphaGo的自对弈都是建立用自己觉得好的下法来搜索的，那么如果这一手AlphaGo1.0感觉可能性极小，那么用$P_{human}(\\vec s)$自对弈的棋谱中就更加难以覆盖。 但是也需要提到的是，根据比赛中柯洁等人的观战我们知道，如果不是后面AlphaGo进入了混乱模式，78手不一定是一个好棋。只能说这一手，顶到了AlphaGo的软肋，在真正和人的对局中不一定是“神之一手” 根据Deepmind团队给出的数据可以知道，一年前，AphaGo1.0的搜索空间，自对弈深度并不完美。所以Deepmind团队有意的在代码逻辑上让其避免打劫，或者说避免劫争，例如，有两个选择，一个胜率60%但需要打劫，另一个55%但不需要打劫，AlphaGo1.0会选择后者。 那么什么是打劫呢？解释这几个和”劫“有关的围棋术语是： 打劫围棋术语，一方制造事端，和另一方讨价还价的行为。劫材可以用来做价格谈判的筹码。通常是走一手没戏，但对手若不予置理，再走第二手会出棋的局部。寻劫通过目数计算，寻找一些有价值的局部制造事端强迫对手应答。通常价值至少需要和打劫的地方相当或者小不太多，否则对方很容易消劫。利用劫劫胜可杀死对方或者得到利益，劫败也应该让对方付出代价，除非双方劫材大小和数量相差悬殊。 通俗的说是，我在这一片已经处于劣势，我换一个战场，发动进攻，你应不应？可能在另外战场的角力中对这边战场的局势产生影响。可以类比于，五子棋中的冲四。 如果有人观看了这一盘棋，我们也可以听到柯洁在强调，AlphaGo在避免打劫，出现了几手莫名其妙的落子。 总结来说，AlphaGo依靠的是对局外的大量计算，无论是局部评估函数，还是$P_{human-plus}(\\vec s)$都十分依赖对局外的大量的计算。随着时间的推移，AlphaGo在对局过程需要的时间越来越固定，不需要在对局时进行太多的MCTS搜索就能获得AlphaGo的下一手位置，可以预见，MCTS的搜索深度不会太深。当计算量十分庞大的时候，依赖更多是那个120层的Policy Network。 从柯洁的第二盘可以发现，他已经努力的制造在中腹引入多方战斗的带劫争的复杂棋局，十分精彩。可惜，AlphaGo2.0貌似已经完善了自己的阿特留斯之踵。当真无敌，说到这里，我们来谈谈AlphaGo2.0 AlphaGo 2.0 VS 柯洁——虽败犹荣三盘对局，感觉到AlphaGo在这一年内进行了极为深度的训练。最可怕的是AlphaGo通过时间验证了机器学习对于解决NP问题的强大潜力（通过这三盘可以看出已经无限接近解决了这个问题，至少在对人类上）。甚至： 臆想一，是否可以利用AlphaGo来判断规则是否公平（中国和韩日规则的不同，7目半和6目半）。 臆想二，最终AlphaGo的自对弈是接近和棋。可惜AlphaGo已经退役。希望针对Deepmind放出的50盘自对弈棋谱可以研究出一些门道，使得围棋这门竞技本身有更大的突破。 局面函数和策略函数愈发强大，愈加的接近于”围棋之神“。 随着Google TPU的发布，跑在TPU阵列上的AlphaGo如虎添翼，MCTS的走子演算效率更高，速度更快（加速的其实就是$P_{human-plus}(\\vec s)$的落子速度。 关于TPU的设计思路和原理可以参考 In-Datacenter Performance Analysis of a Tensor Processor 对于围棋这个策略单步游戏，是存在N步最优解（不存在i+1步最优解），AlphaGo已经在正确的道路上无限的接近于这个N步最优解，仿佛在某一步已经看到了你无论怎么下都能走到的N步最优解。 人类的每一次失误都会使局部评估函数往胜率移动一点，这一点是十分可怕的，因为算法本身的优越性，大局观对于AlphaGo的逻辑来说本身就是一种刻在骨子里的基因 一是因为AlphaGo每次MCTS计算都会计算到接近分出胜负，具有前瞻性 二是因为局面函数本身就是为了来统计大局形势定义的，具有判断局面优劣的能力 所谓大局观，不就是这种走一步看N步的能力嘛。 对未来的展望——从AlphaGo想开去珍贵的并不是攻克了围棋问题本身，而是这种解决问题的基本模式，可以推而广之到很多领域。 先通过卷积网络学习人类的下法，算出策略函数（Policy Network），再通过模仿进行强化学习，左右互搏，不断自我进化，再加上MCTS的经典的解决问题的启发式搜索算法。 这俨然是一个 模仿➜学习➜优化的过程 或许，模仿人类，是机器学习最终的归途，至于应用领域方面 游戏AI是一个最容易想到的领域，只要能抽象出 State Action Judgement，那么这一套解决问题的方式就可以举一反三，让每一个1V1领域的游戏AI非常强大（OpenAI在Dota2 1V1 Solo上的结果更加证明了这一点），至于合作领域的AI可能需要更大的计算量去计算（OpenAI发布的论文MultiAgent很有启发性），对于实际问题来说获得这样的AI有多大的经济价值值得推敲。 游戏的乐趣就在于不确定性，适当的失误也是竞技类游戏的魅力所在，一个能看到N步最优解的AI会让一个游戏机制，游戏规则变得可数据化，这一点其实是游戏被创造出来的初衷相背离的。 其他方面，只要是人类可以学习出来的事物，比如翻译，编程，都是现在的这套体系可能解决的问题，我们期待未来这套解决问题的方法发挥出无穷的力量吧！ [Reference]知乎Tao Lei大神的回答知乎袁行远大神的回答知乎有关围棋打劫的回答其他文章中引用的论文，链接已经给出","tags":[{"name":"AlphaGo","slug":"AlphaGo","permalink":"https://charlesliuyx.github.io/tags/AlphaGo/"},{"name":"CNN","slug":"CNN","permalink":"https://charlesliuyx.github.io/tags/CNN/"},{"name":"MCTS","slug":"MCTS","permalink":"https://charlesliuyx.github.io/tags/MCTS/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://charlesliuyx.github.io/tags/Deep-Learning/"}]},{"title":"清欢 - 林清玄","date":"2017-05-14T16:26:55.000Z","path":"2017/05/14/清欢/","text":"1少年时代读到苏轼的一阕词，非常喜欢，到现在还能背诵： 细雨斜风作晓寒，淡烟疏柳媚晴滩，入淮清洛渐漫漫。雪沫乳花浮午盏，蓼茸蒿笋试春盘，人间有味是清欢。 这阕词，苏轼在旁边写着“元丰七年十二月二十四日，从泗州刘倩叔游南山”，原来是苏轼和朋友到郊外去玩，在南山里喝了浮着雪沫乳花的淡茶，配着春日山野里的蓼菜、茼蒿、新笋，以及野草的嫩芽等等，然后自己赞叹着：“人间有味是清欢！” 当时所以能深记这阕词，最主要的是爱极了后面这一句，因为试吃野菜的这种平凡的清欢，才使人间更有滋味。“清欢”是什么呢？清欢几乎是难以翻译的，可以说是“清淡的欢愉”，这种清淡的欢愉不是来自别处，正是来自对平静疏淡简朴生活的一种热爱。当一个人可以品味出野菜的清香胜过了山珍海味，或者一个人在路边的石头里看出了比钻石更引人的滋味，或者一个人听林间鸟鸣的声音感受到比提笼遛鸟更感动，或者体会了静静品一壶乌龙茶比起在喧闹的晚宴中更能清洗心灵……这些就是“清欢”。 清欢之所以好，是因为它对生活的无求，是它不讲求物质的条件，只讲究心灵的品味。“清欢”的境界很高，它不同于李白的人生在世不称意，明朝散发弄扁舟那样的自我放逐；或者人生得意须尽欢，莫使金樽空对月那种尽情的欢乐。它也不同于杜甫的人生有情泪沾臆，江水江花岂终极这样悲痛的心事，或者人生不相见，动如参与商；今夕复何夕，共此灯烛光那种无奈的感叹。 活在这个世界上，有千百种人生，文天祥的是人生自古谁无死，留取丹心照汗青，我们很容易体会到他的壮怀激烈。欧阳修的是人生自是有情痴，此恨不关风与月，我们很能体会到他的绵绵情恨。纳兰性德的是人到情多情转薄，而今真个不多情，我们也不难会意到他无奈的哀伤。甚至于像王国维的人生只似风前絮，欢也零星，悲也零星，都作连江点点萍！那种对人生无常所发出的刻骨的感触，也依然能够知悉。 2可是“清欢”就难了！ 尤其是生活在现代的人，差不多是没有清欢的。 什么样是清欢呢？我们想在路边好好的散个步，可是人声车声不断的呼吼而过，一天里，几乎没有纯然安静的一刻。 我们到馆子里，想要吃一些清淡的小菜，几乎是杳不可得，过多的油、过多的酱、过多的盐和味精已经成为中国菜最大的特色，有时害怕了那样的油腻，特别嘱咐厨子白煮一个菜，菜端出来时让人吓一跳，因为菜上挤的色拉比菜还多。 有时没有什么事，心情上只适合和朋友去啜一盅茶、饮一杯咖啡，可惜的是，心情也有了，朋友也有了，就是找不到地方，有茶有咖啡的地方总是嘈杂的。 俗世里没有清欢了，那么到山里去吧！到海边去吧！但是，山边和海湄也不纯净了，凡是人的足迹可以到的地方，就有了垃圾，就有了臭秽，就有了吵闹！ 有几个地方我以前常去的，像阳明山的白云山庄，叫一壶兰花茶，俯望着台北盆地里堆叠着的高楼与人欲，自己饮着茶，可以品到茶中有清欢。像在北投和阳明山间的山路边有一个小湖，湖畔有小贩卖工夫茶，小小的茶几、藤制的躺椅，独自开车去，走过石板的小路，叫一壶茶，在躺椅上静静的靠着，有时湖中的荷花开了，真是惊艳一山的沉默。有一次和朋友去，在躺椅上静静喝茶，一下午竟说不到几句话，那时我想，这大概是“人间有味是清欢”了。 现在这两个地方也不能去了，去了只有伤心。湖里的不是荷花了，是飘荡着的汽水罐子，池畔也无法静静躺着，因为人比草多，石板也被踏损了。到假日的时候，走路都很难不和别人推挤，更别说坐下来喝口茶，如果运气更坏，会遇到呼啸而过的飞车党，还有带伴唱机来跳舞的青年，那时所有的感官全部电路走火，不要说清欢，连欢也不剩了。 要找清欢，一日比一日更困难了。 当学生的时候，有一位朋友住在中和圆通寺的山下，我常常坐着颠踬的公交车去找她，两个人沿着上山的石阶，漫无速度的，走走、坐坐、停停、看看，那时圆通寺山道石阶的两旁，杂乱的长着朱槿花，我们一路走，顺手拈下一朵熟透的朱槿花，吸着花朵底部的花露，其甜如蜜，而清香胜蜜，轻轻的含着一朵花的滋味，心里遂有一种只有春天才会有的欢愉。 3圆通寺是一座全由坚固的石头砌成的寺院，那些黑而坚强的石头坐在山里仿佛一座不朽的城堡，绿树掩映，清风徐徐，站在用石板铺成的前院里，看着正在生长的小市镇，那时的寺院是澄明而安静的，让人感觉走了那样高的山路，能在那平台上看着远方，就是人生里的清欢了。 后来，朋友嫁人，到国外去了。我去过一趟圆通寺，山道已经开辟出来，车子可以环山而上，小山路已经很少人走，就在寺院的门口摆着满满的摊贩，有一摊是儿童乘坐的机器马，叽哩咕噜的童歌震撼半山，有两摊是打香肠的摊子，烤烘香肠的白烟正往那古寺的大佛飘去，有一位母亲因为不准孩子吃香肠而揍打着两个孩子，激烈的哭声尖亢而急促……我连圆通寺的寺门都没有进去，就沉默的转身离开，山还是原来的山，寺还是原来的寺，为什么感觉完全不同了，失去了什么吗？失去的正是清欢。 下山时的心情是不堪的，想到星散的朋友，心情也不是悲伤，只是惆怅，浮起的是一阕词和一首诗，词是李煜的：高楼谁与上？长记秋晴望。往事已成空，还如一梦中！诗是李觏的：人言落日是天涯，望极天涯不见家；已恨碧山相阻隔，碧山还被暮云遮！那时正是黄昏，在都市烟尘蒙蔽了的落日中，真的看到了一种悲剧似的橙色。 我二十岁心情很坏的时候，就跑到青年公园对面的骑马场去骑马，那些马虽然因驯服而动作缓慢，却都年轻高大，有着光滑的毛色。双腿用力一夹，它也会如箭一般呼噜向前窜去，急忙的风声就从两耳掠过，我最记得的是马跑的时候，迅速移动着的草的青色，青茸茸的，仿佛饱含生命的汁液，跑了几圈下来，一切恶的心情也就在风中、在绿草里、在马的呼啸中消散了。 尤其是冬日的早晨，勒着绳，马就立在当地，踢踏着长腿，鼻孔中冒着一缕缕的白气，那些气可以久久不散，当马的气息在空气中消弭的时候，人也好像得到某些舒放了。 骑完马，到青年公园去散步，走到成行的树荫下，冷而强悍的空气在林间流荡，可以放纵的、深深的呼吸，品味着空气里所含的元素，那元素不是别的，正是清欢。 4最近有一天，突然想到骑马，已经有十几年没骑了。到青年公园的骑马场时差一点吓昏，原来偌大的马场已经没有一根草了，一根草也没有的马场大概只有台湾才有，马跑起来的时候，灰尘滚滚，弥漫在空气里的尽是令人窒息的黄土，蒙蔽了人的眼睛。马也老了，毛色斑剥而失去光泽。 最可怕的是，不知道什么时候在马场搭了一个塑料棚子，铺了水泥地，其丑无比，里面则摆满了机器的小马，让人骑用，其吵无比。为什么为了些微的小利，而牺牲了这个马场呢？ 马会老是我知道的事，人会转变是我知道的事，而在有真马的地方放机器马，在马跑的地方没有一株草，则是我不能理解的事。 就在马场对面的青年公园，已经不能说是公园了，人比西门町还拥挤吵闹，空气比咖啡馆还坏，树也萎了，草也黄了，阳光也不灿烂了。从公园穿越过去，想到少年时代的这个公园，心痛如绞，别说清欢了，简直像极了佛经所说的“五浊恶世”！ 生在这个时代，为何“清欢”如此难觅。眼要清欢，找不到青山绿水；耳要清欢，找不到宁静和谐；鼻要清欢，找不到干净空气；舌要清欢，找不到蓼茸蒿笋；身要清欢，找不到清凉净土；意要清欢，找不到智慧明心。如果要享受清欢，唯一的方法是守在自己小小的天地，洗涤自己的心灵，因为在我们拥有愈多的物质世界，我们的清淡的欢愉就日渐失去了。 现代人的欢乐，是到油烟爆起、卫生堪虑的啤酒屋去吃炒蟋蟀；是到黑天暗地、不见天日的卡拉OK去乱唱一气；是到乡村野店、胡乱搭成的土鸡山庄去豪饮一番；以及到狭小的房间里做方城之戏，永远重复着摸牌的一个动作……这些放逸的生活以为是欢乐，想起来毋宁是可悲的。为什么现代人不能过清欢的生活，反而以浊为欢，以清为苦呢？ 一个人以浊为欢的时候，就很难体会到生命清明的滋味，而在欢乐已尽、浊心再起的时候，人间就愈来愈无味了。 5这使我想起东坡的另一首诗来： 梨花淡白柳深青，柳絮飞时花满城；惆怅东栏一株雪，人生看得几清明？ 苏轼凭着东栏看着栏杆外的梨花，满城都飞着柳絮时，梨花也开了遍地，东栏的那株梨花却从深青的柳树间伸了出来，仿佛雪一样的清丽，有一种惆怅之美，但是人生看这么清明可喜的梨花能有几回呢？这正是千古风流人物的性情，这正是清朝大画家盛大士在《溪山卧游录》中说的凡人多熟一分世故，即多一分机智。多一分机智，即少却一分高雅。 也有说山中何所有？岭上多白云，只可自怡悦，不堪持赠君，自是第一流人物。 第一流人物是什么人物？ 第一流人物是在清欢里也能体会人间有味的人物！ 第一流人物是在污浊滔滔的人间，也能找到清欢的人物！","tags":[{"name":"Article","slug":"Article","permalink":"https://charlesliuyx.github.io/tags/Article/"},{"name":"Literature","slug":"Literature","permalink":"https://charlesliuyx.github.io/tags/Literature/"}]},{"title":"English-abbreviation","date":"2017-05-14T00:29:00.000Z","path":"2017/05/13/English-abbreviation/","text":"一些常用的英语缩写的总结 日常生活篇 R.S.V.P: 源自于法语‘Répondez s’il vous plait’，英文解释为’Respond,if you please’.邀请函结尾写这个，表示‘敬请回复’； P.S: 意思是‘post script’,表示‘再多说一句’，一般写完要说的话之后结尾突然想起说什么可以写； ASAP: as soon as possible. 表示‘尽快’，注意听音频发音，可读成A-SAP; ETA: estimated time of arrival. 表示‘预计到达时间’； BYOB: bring your own bottle; 表示‘自带酒水，举办派对时常用’ 吃饭做菜篇 tsp or t : teaspoon 一茶匙 tbs / tbsp/ T: tablespoon 一汤匙 c: cup 一杯 gal: gallon 加仑 lb : pound 磅 pt：pint 品脱 qt: quart 夸脱 出国地图篇 Ave: avenue 大街 Blvd: boulevard 大道 Ln: lane 车道 Rd: road 公路 St: street 街道 教育工作篇 BA: Bachelor of Arts 文学士 BS: Bachelor of Science 理学士 MA: Master of Arts 文科硕士 PA: Personal Assistant 私人助理 VP: Vice President 副总统;副总裁 CEO: Chief Executive Officer 首席执行官 CFO: Chief Financial Officer 首席财务官 COO: Chief Operating Officer 首席运营官 CMO: Chief Marketing Officer 首席营销官 社交聊天篇 JK :just kidding 跟你开玩笑呢 TBD: to be determined 待定 AFAIK: as far as I know 据我所知 BRB: be right back 马上回来 CUL: see you later 回见 TTYL: talk to you later 回聊 CWYL: chat with you later 回聊 LOL: laugh out loud 哈哈 LMAO: laugh my ass off 笑死我了 ROTFL/ ROFL: rolling on the floor laughing 笑到在地上打滚 NP: no problem 没问题,没关系,不客气 IDK: I don’t know 我不知道 ILY: I love you 我爱你 TMI: too much information 信息量太大了； 说的太多了 OIC: Oh, I see. 我明白了 FYI: for your information 顺便告知你 BTW: by the way 顺便说一下 顺便问一下 MYOB: mind your own business 别多管闲事 FAQ: frequently asked questions 经常被问的问题20: WTF: what the fuck 搞毛阿…… 委婉的是WTH: what the hell/heck21: AKA: also known as. 也叫做 TGIF: thank god It’s Friday 谢天谢地又到礼拜五了 TBC: to be continued; to be confirmed 未完待续/ 有待确认 数字字母篇2: to/too4: forB: beC: seeI: eyeO: owe;R: are;U: you;ur: your/you’reY: why","tags":[{"name":"Wiki","slug":"Wiki","permalink":"https://charlesliuyx.github.io/tags/Wiki/"},{"name":"English","slug":"English","permalink":"https://charlesliuyx.github.io/tags/English/"}]},{"title":"有关中国诗的那些事","date":"2017-05-13T23:40:00.000Z","path":"2017/05/13/有关中国诗的那些事/","text":"没有沉淀，文字永远上不了档次。难得空闲，读了些诗，有些感受。 韦应物 独怜幽草涧边生，上有黄鹂深树鸣。春潮带雨晚来急，野渡无人舟自横。 记起这《滁州西涧》，听过一个故事。话说一次国画比赛，题目是以春潮带雨晚来急，野渡无人舟自横这句诗作画。国学博大精深，国画作为其中一支配起诗来，别有遐想。此题甚好，不仅考及画技，更有对国学中诗词的体悟和见解。大家不妨也想想如果是你，你会怎么画？这里先卖个关子。 从诗的字面来说，是这样一种通感：春天近了，潮气依稀可嗅。但谁能像你这样，对一棵在水边生长的小草也充满爱怜？黄鹂在密林深处的低语你都能听到？这需要多么细腻的一颗心。华灯初上，渡口上已经没有人，舟独自横于水上，那是一种空阔的感觉。映照你一生步履，你的细腻出于岁月。你当年49岁，50载，可能不长，但是我知道你的与众不同，你的50载甚至顶得别人几辈子。 韦应物年少荒唐，并未认真读书。安史乱起，韦应物扈从不及，流落秦中。乱后，韦应物折节读书，痛改前非，从一个富贵无赖纨绔一变而为忠厚仁爱的儒者。有些官运，在地方（苏州）任官。韦应物勤于吏职，简政爱民，在苏州刺史届满之后，一贫如洗，寄居无定寺，客死他乡。 享年五十五岁。 别人些许看出的是你不在其位，不得其用的无奈，忧伤。但我看到的，更多是你的豁达，你心中总是美好多于忧伤。 通往远方的路，没有哪条是你不能走的；走在路上的人，没有谁是你不能结交的；结交的朋友，没有谁是你不能推心置腹的。虽然那个时代远没有现在的复杂，但是能捧出一颗完整的心也并不是一件容易的事。韦苏州，你是一个充满诗情的人。 回头看看开头提到的国画比赛优胜者的作品：弥蒙的雾气用模糊的淡墨衬托，远处的群山，夕阳露出半个头。远远的有几簇灯火，近处，一条小舟在几根芦苇中飘荡，船上有位着布衣的蓑翁，嘴里叼根芦苇，帽檐下压，不知是否在闭目养神，两只杜鹃立于船头。起初不懂，“无人”的野渡为何有人呢？其中深意，结合了韦苏州的履历才恍然大悟。 “无人”并不是一只孤舟。韦应物闲居，船上舟子，好似当时的韦应物，在船头打盹，闻着草香，听着鹂鸣。韦应物虽然赋闲苏州，但他并不排斥官场，若有机会，他还是会出仕，只是满足于闲暇。无奈忧伤可能有，但经历了顽劣，奋起，战乱，官场，贬谪，闲居的韦应物，更多的，是看破人生的豁达和满足。 李白总觉中国诗总离不开一个“愁”字。思乡，思亲，忧国，羁旅等等，都和“愁”万缕千丝。我爱这些无奈，悲壮，不舍，甚至愤懑，嘲讽。他们仿佛缩影了人生，视角令人称奇，细腻的令你悸动。 抽刀断水，是最无奈的神话；举杯消愁，是最动情的悲歌。李白潇洒一生，他豪放，甚至一直清贫，有了几个钱，就豪饮一番，将诗情挥洒，更是对“愁”下了如此入理的定义。 拣尽寒枝不肯栖，寂寞沙洲岭李白就犹如谪仙，似乎从来没有受过来自这个世界的温暖。于是，在静夜里，李白写下了床前明月光，疑是地上霜。举头望明月，低头思故乡的千古“愁”词。可是李白的故乡在哪里呢？是陇西？是巴蜀？月华似霜的夜，浪迹天涯的游子李白在梦幻中寻觅故乡，但故乡却比梦幻更飘渺。 李白是复杂的，李白糅合着道家的“出世”和儒家的“入世”思想。所以，顺境时，他仰天大笑出门去，我辈岂是蓬蒿人的潇洒豪情；逆境时，他有弃我去者昨日之日不可留，乱我心者今日之日多烦忧的绵绵愁绪。 那些诗人感动于张谓笔下早梅傲雪不知近水花先发，疑是经冬雪未消的玄妙；陶醉于贺铸风中一川烟草，满城风絮，梅子黄时雨的飘愁；哀婉于苏轼眼中细看来，不是杨花，点点是，离人泪的破碎。 说起苏东坡，一个传奇。 人生如梦，东坡曾经迷惘过；早生华发，东坡曾经惋惜过；十年生死两茫茫，东坡曾经痛苦过。但他不屈，他平和，他豁达。 一蓑烟雨任平生，他淡泊；日啖荔枝三百颗，不辞长作岭南人，他自定；踏雪飞鸿，他淡然。问汝平生功业，黄州惠州儋州，三贬之地，还恰恰就是他留下许多不朽之作的地方。 读着这些诗，深深思索，你会感到作为一个中国人学会了中文，有着五千年的浩瀚历史文化，是多么令你振奋和自豪；国学，遗留的东西，值得我们用一生去参悟。常说高考诗词理解令人头痛，如果怀着这样的心情读诗，你还会怕吗？","tags":[{"name":"Article","slug":"Article","permalink":"https://charlesliuyx.github.io/tags/Article/"},{"name":"Original","slug":"Original","permalink":"https://charlesliuyx.github.io/tags/Original/"}]}]