[{"title":"什么是信息熵、交叉熵和相对熵","date":"2017-09-11T07:25:49.000Z","path":"2017/09/11/什么是信息熵、交叉熵和相对熵/","text":"要完成题目的最终解释，必须从熵这个神奇的概念开始讲起 什么是熵 - Entropy词源 - 最初来源于热力学Entropy来源于希腊语，原意：内向，即：一个系统不受外部干扰时往内部稳定状态发展的特性。定义的其实是一个热力学的系统变化的趋势 $$\\Delta S = \\frac{Q}{T} = \\frac{热量}{温度} \\tag{1-1}$$1923年，德国科学家普朗克来中国讲学用到entropy这个词，胡刚复教授看到这个公式，创造了“熵”字，因为“火”和热量有关，定义式又是热量比温度，相当自洽 信息论信息论中，熵是接受的每条消息中包含的信息的平均值。又被称为信息熵、信源熵、平均自信息量。可以被理解为不确定性的度量，熵越大，信源的分布越随机 1948年，由克劳德·爱尔伍德·香农将热力学中的熵引入信息论，所以也叫做：香农熵 生态学在生态学中，熵表示生物多样性的指标 广义的定义熵是描述一个系统的无序程度的变量；同样的表述还有，熵是系统混乱度的度量，一切自发的不可逆过程都是从有序到无序的变化过程，向熵增的方向进行 我们接下来要讨论的信息熵 交叉熵 相对熵 更多的着眼于信息论的角度，换句话说，更加关注概率和不全定性 什么是信息熵、交叉熵、相对熵可以将对熵的理解从简单到复杂依次分解成三个层次来理解 如何衡量不确定事物的发生？数学是一种工具，使用数学来描述现实中的各种事物是一个数学家本质的工作目标。而现实中不确定性，或者说不太确定是否会发生的事件必须要找到一种抽象的、符号化和公式化的手段去表示。 比如天气情况，假设可能有【阴、晴、雨、雪】四种情况，使用概率符号表示 $\\mathbf P = [p_1,p_2,p_3,p_4]$，接下来自然而然的思考：那么，什么条件（情况）会影响这些值呢？ 假设有一下三种描述，或者说条件 今天是晴天，所以明天可能也是晴天 天气预报说明天下雨 9月12日苹果公司举行发布会 那么这三个描述中，很明显，第二条的信息量更大，因为它可以使得不确定事件发生在 $p_3$ 的概率更大。类似的，第三条对判断毫无帮助，信息量为0。注意，信息量不等于信息熵，如果是这样，那么直接用概率来衡量就可以了，不需要在重新定义一个概念 其实信息熵是信息量的期望，它不是针对每条信息，而是针对整个不确定性结果集而言，信息熵越大，事件不确定性就越大。单条信息只能从某种程度上影响结果集概率的分布 考虑到信息冗余，信息量存储下来究竟需要多大空间？我们已经有了 $\\mathbf P = [p_1,p_2,p_3,p_4]$ 来表示天气情况，那么用计算机来存储每天的天气，那该如何编码呢？ 常见的做法是，4个不同的信息，只需要2bit就能做到，00 01 11 10，假设我们在南方城市，肯定要把00编码成雨天，这样可以节省存储空间，至于为什么能节省存储空间，这就要讨论编码方式。可以简单的理解为，如果一串信息一串0很多，可以通过编码压缩这一群0来节省空间 使用一个公式来计算记录n天数据需要的存储空间：Sn $$ S_n = n \\times \\sum_{i = 1}^4{\\left(P_i \\times F(P_i) \\right) } \\tag{2-1} $$ $P_i$ 表示第i个事件发生的概率；$F(P_i)$ 表示存储空间的存储因子 如何确定这个函数 $F(P_i)$ 的形式？考虑这个函数需要满足条件：概率大的事件对应小的存储空间，说人话，就是成反比，你的数学功底不错的话，脑海中第一反应出来满足这个条件最直观是反比例函数，说人话， $\\frac{1}{P_i}$ 。 之后我们发现这个公式中有个除法非常讨厌，我们想着去掉它，脑海中第一反应出来的满足这个条件的一定是取对数，至于为什么取对数，那说道就很多，取对数是指数的逆操作， 对数操作可以让原本不符合正态分布的模型符合正态分布，比如随着模型自变量的增加，因变量的方差也增大的模型取对数后会更加稳定 取对数操作可以rescale（原谅我，这里思前想后还是感觉一个英文单词更加生动）其实本质来说都是因为第一点。说人话版本，人不喜欢乘法，对数可以把乘法变加法 那么我们结束清楚之后，就很容易就可以定义出$$F(P_i) = \\log_a ({\\frac{1}{P_i}}) \\tag{2-2}$$ a作为底数，可以取2（处理2bit数据），10（万金油），e（处理正态分布相关的数据） 结合对信息熵的定义（第一节最后的粗体字）然后把（2-2）带入（2-1），就会发现，哦！看着有点眼熟啊$$H(P) = \\sum_i {P(i)log_a {\\frac{1}{P(i)}}} = - \\sum_i {P(i)log_a {P(i)}} \\tag{2-3}$$这这这，就是信息熵的定义式吧？总结就发现，信息熵其实从某种意义上反映了信息量存储下来需要多少存储空间 理解基于信息熵的交叉熵和相对熵因为是我们用2bit模式存储，为了计算方便，这里取a = 2 先计算刚刚有关天气问题 $\\mathbf P = [p_1,p_2,p_3,p_4]$ ：【阴、晴、雨、雪】的信息熵，假设我们对天气的概率一无所知，那么四种天气的发生概率为等概率（服从平均分布），即 $\\mathbf P = [\\frac {1}{4},\\frac {1}{4},\\frac {1}{4},\\frac {1}{4}]$ ，带入公式2-3，得到 $H(P) = 2$ ，存储信息需要的空间 $S_n = 2n$ 继续思考，假设我们考虑天气的城市是一个地处中国南方雨季的城市，那么阴天和雨天的概率从经验角度（先验概率）来看大于晴天雪天，把这种分布记为 $\\mathbf Q = [\\frac{1}{4},\\frac{1}{8},\\frac{1}{2},\\frac{1}{8}]$，带入公式2-3，信息熵 $H(Q) = 1.75$，存储信息需要的空间 $S_n = 1.75n$ 直观的来考虑上面不同的两种情况，明显当事件的不确定性变小时候，我们可以改变存储策略（00 雨天 01 阴天），再通过编码，节省存储空间。信息熵的大小就是用来度量这个不确定大小的 关于编码的方式，这里提一下，哈夫曼树与哈夫曼编码 ，有兴趣的读者可以去研究一下 交叉熵的由来我们把这个问题再扩展一下 天气【阴、晴、雨、雪】 信息熵 $\\mathbf P = [\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4}]$ $H(P) = 2$ $\\mathbf Q = [\\frac{1}{4},\\frac{1}{8},\\frac{1}{2},\\frac{1}{8}]$ $H(Q) = 1.75$ $\\mathbf Z = [\\frac{1}{8},\\frac{1}{16},\\frac{3}{4},\\frac{1}{16}]$ $H(Z) = \\frac{7}{8}+\\log_2 {\\frac{4}{3}} = 1.29$ $\\mathbf W = [0,0,1,0]$ $H(W) = 0$ 接下来，假定在确定性更大的概率分布情况下，用更不确定的存储策略来计算，比如使用 $\\mathbf Q$ 的概率乘上 $\\mathbf P$ 的存储因子，套用公式2-3$$H(\\mathbf Q,\\mathbf P) = \\sum_i {P(i) \\log_a {\\frac{1}{Q(i)}}} \\tag{3-1}$$顾名思义，看公式3-1的形式，就不难发现，这就是所谓的交叉熵，计算可得 交叉熵 P Q Z W P $H(P,P) = 2$ $H(P,Q) = 2.25$ $H(P,Z) = \\frac{11}{4}+\\frac{1}{4}\\log_2 {\\frac{4}{3}} = 2.85$ +inf Q $H(Q,P) = 2$ $H(Q,Q) = 1.75$ $H(Q,Z) = \\frac{7}{4}+\\frac{1}{2}\\log_2 {\\frac{4}{3}} = 1.96$ +inf Z $H(Z,P) = 2$ $H(Z,Q) = 1.375$ $H(Z,Z) = \\frac{7}{8}+\\log_2 {\\frac{4}{3}} = 1.29$ +inf W $H(W,P) = 2$ $H(W,Q) = 1$ $H(W,Z) = \\log_2 {\\frac{4}{3}} = 0.415$ $H(W,W) = 0$ 上表直观的展现的交叉熵的数值表现，PQZW依次不确定性越来越低，极端情况的W不确定性为0，即是确定的 总的来说，我们的目的是：让熵尽可能小，即存储空间小。（不要问为什么想要存储空间小，这都是钱更是效率和时间）通过上表我们发现一个规律，为了让熵小，解决方案是：是用确定性更大的额概率乘以确定性更小的存储因子，比如不确定性越大的概率分布，如P概率分布，其信息熵越大；基于同一确定性分布的情况下，套用不确定性更大的存储因子，如P的存储因子，得出的交叉熵越大 在机器学习中，即用测试结果集（样本结果集）的概率乘以训练出来的结果集存储因子，而在不断的训练过程中，我们要做的就是通过不断调整参数，降低这个值，使得模型更加的稳定，不确定性越来越小，即突出需要表征的数值的特点（白话文也就是分类的效果更好） 相对熵的由来有了信息熵和交叉熵后，相对熵是用来衡量两个概率分布之间的差异，记为 $D(P||Q) = H(P,Q) - H(P)$，也称之为KL散度$$D_{KL}(P||Q) = \\sum_i{P(i) \\log_a {\\frac{P(i)}{Q(i)}}}$$当 $P(i) = Q(i)$ 的时候，该值为0，深度学习过程也是一个降低该值的过程，该值越低，训练出来的概率Q越接近样本集概率P，即越准确，或者可以理解为相对熵一把标尺，用来衡量两个函数是否相似，一样就是0，当然，这种解释十分牵强，但是更直观","tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"},{"name":"Infomation Theory","slug":"Infomation-Theory","permalink":"https://charlesliuyx.github.io/tags/Infomation-Theory/"}]},{"title":"Dota2机制总结","date":"2017-09-06T06:50:56.000Z","path":"2017/09/05/Dota2机制总结/","text":"版本信息：更新到7.06f 金钱击杀英雄奖励获得金钱 = 110 + 连杀奖励 + （被击杀者等级 * 8） 连杀奖励 = 60 * （连杀数-2）[大于0] 助攻奖励 助攻英雄数 获得金钱 1 [140 + (5 × 被击杀者等级) + (0.0375 × 被击杀者净收入 × 净收入因子) + (100 × 团队净收入之差 / 4000)] 2 [70 + (4 × 被击杀者等级) + (0.0375 × 被击杀者净收入 × 净收入因子) + (75 × 团队净收入之差 / 4000)] 3 [35 + (3 × 被击杀者等级) + (0.0375 × 被击杀者净收入 × 净收入因子) + (50 × 团队净收入之差 / 4000)] 4 [25 + (2 × 被击杀者等级) + (0.03 × 被击杀者净收入 × 净收入因子) + (35 × 团队净收入之差 / 4000)] 5 [20 + (1 × 被击杀者等级) + (0.0225 × 被击杀者净收入 × 净收入因子) + (25 × 团队净收入之差 / 4000)] 净收入因子 = (敌方队伍净收入 / 友方队伍净收入) - 1，最小值为0，最大值为1 还需要 × [1.2 - 0.1 × (被击杀者净收入排名 -1)] × [净收入排名因子] Roshan200 团队奖励 150 - 400 击杀奖励 死亡掉钱损失不可靠金钱 = 50 + 财产总和 ÷ 40 5千经济 ➜ 175；1万经济 ➜ 290；2万经济 ➜ 550 复活复活时间 每级增加2秒 每到6级的倍数增加10秒 18级后每级增加4秒 买活花费买活金钱 = 100 + (英雄等级 英雄等级 1.5) + (游戏时间(s) * 0.25 ) 放弃比赛掉线超过5分钟后，所有金钱被队友平分 物理攻击伤害最终攻击伤害 $$ 最终攻击伤害 = \\\\ \\{ [\\text {MD} × (1 + \\sum \\text {PBD}) + \\text {FBD}] \\times \\text {CSM} - \\text {BD} \\}\\\\ \\times \\text {AVM} \\times \\text {ATM} \\times \\text {GDM} $$ MD(Main Damage) 主要攻击力【白字攻击力】主要攻击力 = 基础攻击力 + 主属性 除此之外，所有加成的攻击力都是【绿字攻击力】 PBD(Percentage bonus damage) 百分比攻击力加成这个加成是加法叠加 Tips: 圣者遗物可以增加60攻击力，一个出支配带头狼的VS，36%+30% = 66%，60 / 0.66 = 90，也就是说，白字攻击力达到90就等于这个英雄出了一个圣者遗物，相当的可怕。 技能 加成数值 头狼光环 30% 强化图腾 100%/200%/300%/400% 野性驱使 15%/26%/37%/48%（只影响狼人控制的单位） 授予力量 20%/30%/40%/50% (天赋 30%/40%/50%/60%) 神之力量 80%/120%/160% (友军A帐: 75%/100%/125%) 复仇光环 12%/20%/28%/36% (天赋 32%/40%/48%/56%) 祭品光环 15% FBD(Flat Bonus Damage) 固定值百分比加成 技能 加成数值 臂章-邪恶之力 31 嗜血渴望 最高攻击力加成/英雄: 16/24/32/40 战意 最高叠加层数: 5/7/9 每层攻击力加成: 18/24/30 极度饥渴 60/100/140 死亡契约 基于目标最大生命值的攻击力加成: 5%/7%/9% 精准光环 敏捷 20%/26%/32%/38% (天赋26%/32%/38%/44%) 星体游魂 小兵6/9/12/15 英雄 12/24/36/48 (天赋92/104/116/128) 灵动迅捷 10/25/40/55/70/85/100 A帐115 卡尔-火 4/8/12/16/20/24/28 * 3 决斗 10/14/18 (天赋50/54/58) 战斗嚎叫 70/100/130 月之祝福 14/22/30/38 嚎叫 英雄 10/15/20/25 非英雄 4/6/8/10 夜晚翻倍 静电连接 每秒偷取 7/14/21/28 (天赋21/28/35/42) 支配死灵 最大灵魂数: 18/24/30/36 (A帐 22/30/38/46) 折光 攻击次数: 3/4/5/6 (天赋6/7/8/9)攻击力加成: 20/40/60/80 魔化 20/40/60/80 长大 50/100/150 加主要攻击力 衰退光环 英雄死亡 30/35/40/45 小兵死亡 5 CSM(Critical Strike Multiplier) 致命一击倍数 致命一击来源 几率% 伤害% DPS期望% 头狼 - 致命一击 20 200 20 血棘 - 致命一击 20 175 15 酒仙 - 醉拳 10/15/20/25 230 13/19.5/26/32.5 混沌一击 12 125/175/225/275 3/9/15/21 水晶剑 - 致命一击 20 175 15 大炮 - 致命一击 30 235 40.5 剑舞 20/25/30/35 200% 20/25/30/35 狼人 - 变身 40 160/180/200 24/32/40 恩赐解脱 15 230/340/450 19.5/36/52.5 殊死一搏 15 英雄 150/200/250/300 7.5/15/22.5/30 血棘 - 灵魂撕裂 100 140 140 忍术 100 150/175/200/225 12/10/8/6 天赋 -5 棒击大地 100 150/175/200/225 天赋+100 触发条件 暗杀 100 A帐 280 距离内所有敌人 海象神拳 100 350/A帐500 冷却 36/24/12 游戏中出现的红字代表的是减少前的物理伤害 BD(Blocked Damage) 被格挡伤害 伤害格挡来源 几率% 格挡伤害 圆盾 50 近战16 远程8 穷鬼盾 英雄100 非英雄50 近战20 远程10 先锋盾 50 近战70 远程35 赤红甲 - 坚盾 100 60 海妖外壳 100 12/24/36/48 伤害格挡不格挡物理伤害技能，守卫的攻击也不格挡 AVM(Armor Value Multiplier) 护甲值倍数见护甲 ATM(Armor Type Multiplier) 护甲类型倍数见攻击类型，英雄打英雄100% GDM(General Damage Multipliers) 一般伤害倍数见伤害调整 攻击类型基础打英雄护甲75%伤害 穿刺打英雄护甲50%伤害，基础护甲（小兵护甲）150%伤害 伤害调整除了回光返照，幽灵船，魔法护盾之外，伤害减免和加深的叠加为加法叠加 技能 来源 数值% 血之狂暴 加深接受输出 25/30/35/40 远处 减半 赎罪 加深接受 18/24/30/36 灵魂猎手 加深接受 20/30/40/50 守卫冲刺 加深接受 15 血肉傀儡 加深接受 20/25/30 200范围内最高 回光返照 减免接受 0 全免 4/5/6 A帐+1 刚毛后背 减免接受 背后 16/24/32/40 侧面 减半 奔袭冲撞 减免接受 A帐 40 持续4秒 过载 减免接受 5/10/15/20 幽灵船 减免接受 40/45/50 持续10秒 决斗 减免接受 A帐 100 持续6/7/8秒 魔法护盾 减免接受 60 1.6/1.9/2.2/2.5 钻地 减免接受 40 折射 减免接受 10/14/18/22 激怒 减免接受 80 持续4秒 陵卫斗篷 减免接受 4层 8/12/16/20 冷却6/5/4/3 寒冬诅咒 减免接受 100 3.25/4/4.75 战斗饥渴 降低输出 A帐 30 持续10秒 白银之锋 降低输出 50 持续5秒 攻击速度基础攻击间隔 BAT英雄在没有额外攻速加成的情况下每两次攻击间的时间间隔 攻击速度 ISA 面板中英雄增加的攻击速度 由装备获得的攻击速度加成 每个英雄基础100点基础攻速 由Debuff造成的攻速减低 攻击速度计算公式 $$ 每秒攻击的次数 = \\frac{(100 + IAS) × 0.01} {BAT} $$ $$ 每次攻击的时间 = \\frac{1}{每秒攻击的次数} $$ 攻击速度 效果 -80 五分之一BAT时间来攻击 -75 四分之一BAT时间来攻击 -66 三分之一BAT时间来攻击 -50 二分之一BAT时间来攻击 +00 正常状态 +100 * n （1+n）倍攻击速度 根据表格我们可以知道减攻速的技能在基础攻速很高的情况下基本没有什么效果，但是越接近0速度，减速效果越明显 增加攻击速度技能列表 技能 增加数值 持续时间s 魔霭诅咒 10/20/30/40 4.5 雷肤兽 - 暴怒 75 8 雷肤兽 - 战鼓光环 15 光环范围 900 天穹守望者 - 磁场 50/60/70/80 3.5/4.5/5.5/6.5 淘汰之刃 30 6 A帐10 成功淘汰 野性之心 15/25/35/45 光环范围 900 扫射 130 天赋 +70 4/6/8/10 熊怪 - 迅捷光环 15 光环范围 900 飓风之力 100 5 狂战士之血 220/260/300/340 剩下10%生命值最高 灵动迅捷 10/25/40/55/70/85/100/A115 9 卡尔 - 雷 2/4/6/8/10/12/14 * 3 开关 过载 40/50/60/70 开关 强攻 65/90/115/140 5 狂暴 50/60/70/80 3/4/5/6 炽魂 每层40/55/70/85(天赋 75/90/105/120) 10 最高3层 德鲁伊 - 狂猛 10/20/30/40 18/22/26/30 跳跃 16/32/48/64 (天赋 +100) 5 死灵射手光环 5/7/9 光环范围 900 暗夜猎影 45/60/75/90 夜晚 嗜血术 30/40/50/60 (天赋 +40) 30 幻影突袭 130 4s or 4次攻击 战斗专注 60/120/180 5 热血战魂 15/20/25/30 (105/140/175/210) 每次攻击同个目标 超强力量 400 15 or 3/4/5/6次攻击 黄泉颤抖 64 3/4/5/6 集中火力 500 20 寒冬诅咒 70 3.25/4/4.75 降低攻击速度技能比较有效果的降低攻速的技能 烈火精灵：80/100/120/140 不可侵犯：40/70/100/130，蝮蛇突袭：40/60/80 重生：75 黄泉颤抖：64 小狼-致残：60 冰封魔印：30/40/50/60 雷霆一击：25/35/45/55 原始咆哮：50 冰霜新星：20/30/40/50 液态火：20/30/40/50 石化凝视：50 夜魔虚空：50 冰眼：45 豪猪：10/20/30/40 冰火交加：28/32/36/40 毒龙法球：10/20/30/40 全能光环：10/18/26/34 技能攻击伤害技能伤害计算魔法伤害受到魔法抗性影响，技能伤害可以由智力获得增强 $$ 增强数值 = [初始智力 + (当前等级 - 1) \\times 智力成长] / 14 / 100 + 技能增强天赋 $$ $$ 技能最终伤害 = 技能伤害数值 \\times (1 + 增强数值) \\times \\\\ \\prod_{i=1}^n{(1 - 魔法抗性增加_i)} \\times\\prod_{i=1}^n{(1 + 魔法抗性降低_i)} $$ 技能增强天赋远古冰魄10：8% 蝙蝠骑士15：5% 人马20：10% 死亡先知10：5% 干扰者20：10% 大地之灵20：15% 灰烬之灵10：8% 矮人直升机10：6% 杰奇洛10：8% 拉西克20：5% 莉娜20：6% 莱恩20：8% 马格纳斯10：15% 米拉娜15：5% 食人魔魔法师25：15% 殁境神蚀者25：8% 凤凰20：8% 帕克20：10% 拉比克20：8% 暗影恶魔15：8% 影魔15：6% 风暴之灵25：10% 伐木机20：5% 修补匠15：4% 孽主15：12% 维萨吉25：20% 风行者20：15% 技能伤害类型分为：魔法伤害，物理伤害，纯粹伤害 大部分伤害为魔法伤害 物理伤害技能炼金术士：酸性喷雾 炼金术士：不稳定化合物 敌法师：法力损毁 斧王：反击螺旋 兽王：野性飞斧 赏金猎人：暗影步 钢背兽：针刺扫射 人马：反击 克林克兹：灼热之箭 戴泽：剧毒之触 戴泽：暗影波 死亡先知：驱使恶灵 主宰：无敌斩 昆卡：潮汐使者 拉西克：恶魔的赦令 噬魂鬼：盛宴 剃刀：风暴之眼 斯拉达：鱼人碎击 斯拉达：深海重击 狙击手：爆头 工程师：感应地雷 工程师：爆破起飞 潮汐猎人：锚机 熊战士：怒意狂击 冥界亚龙：幽冥剧毒 编织者：虫群 纯粹伤害技能祸乱之源：蚀脑 祸乱之源：噩梦 刃甲：反弹伤害 嗜血狂魔：血之祭祀 嗜血狂魔：割裂 陈：忠诚考验 死亡先知：吸魂巫术 末日使者：末日 魅惑魔女：推进 谜团：午夜凋零 祈求者：电磁脉冲 祈求者：阳炎冲击 莉娜：神灭斩A帐 美杜莎：石化后秘术异蛇 司夜刺客：尖刺外壳 全能骑士：洗礼 殁境神蚀者：奥术天球 帕吉：肉狗 痛苦女王：超声波冲击 沉默术士：智慧之刃 幽鬼：荒芜 圣堂刺客：灵能之刃 伐木机：锯齿飞轮 伐木机：伐木锯链 伐木机：带树木死亡旋风 修补匠：激光 骨灰 护甲白字护甲$$敏捷 = 基础敏捷 + (等级 - 1) * 敏捷成长$$ $$白字护甲 = 基础护甲 + ( \\frac{敏捷}{7})$$ 护甲值倍数$$护甲值倍数 = 1 - \\frac{0.06 \\times 护甲值}{1 + 0.06 \\times |护甲值| }$$ 护甲值倍数倍数和护甲值的相关曲线 相关曲线 纵坐标是护甲值倍数，横坐标是现在英雄的护甲，不同颜色的线是此时减少的护甲（越上面的线减的越多） 有效生命值 （EHP）有效生命值 = 总生命值 ÷ 护甲值倍数 $$ 实时有效物理生命值 = 当前生命值 \\div (1 - \\frac{0.06 \\times 当前总护甲值}{1 + 0.06 \\times |当前总护甲值| }) $$ $$ 实时有效魔法生命值 = 当前生命值 \\div (0.75 \\times (1 - 装备提供抗性_1) \\times \\ldots \\times (1 - 装备提供抗性_n)) $$ 护甲调整增加护甲的技能 技能 加成数值 持续时间s 黑龙 - 龙肤光环 3 光环范围 900 狂战士怒吼 40 2/2.4/2.8/3.2 编织 0.75/1.0/1.25 每秒 18/24/30 24 龙族血统 3/6/9/12(天赋 翻倍) 永久 霜冻护甲 3/5/7/9 40 战斗嚎叫 10/15/20 6 变形术 4/6/8 变形状态 寒冰盔甲 8 45 战吼 5/10/15/20 8 活性护甲 5/10/15/20 每层 1/1.2/1.4/1.6 10/13/16/19 崎岖外表 3/4/5/6 永久 巨魔 - 狂战士之怒 6 切换 减低护甲的技能 技能 降低数值 持续时间s 酸性喷雾 4/5/6/7 (天赋 +4) 16 远古 - 亵渎 50% 6 粘稠鼻液 1/1.4/1.8/2.2 最高层数4(8) 英雄5 小兵10 实相裂隙 3/4/5/6 8 编织 0.75/1/1.25每秒 (18/24/30) 24 自然秩序 基础护甲：40%/60%/80%/100% 光环范围 275 火人 - 攻击 每次1点 上限10 5 击中刷新时间 激流 2/3/4/5 8 范围 320 风暴之眼 0.7/0.6/0.5 (天赋 -0.1) 打击1次1点 30 魔王降临 3/4/5/6 光环范围 900 侵蚀雾霭 10/15/20 18 隐匿 2/4/6/8 10 巨浪 3/4/5/6 (天赋 +5) 4 死亡旋风 敏捷损失 * 0.14 恐怖波动 3/4/5/6 1400距离 300范围 15 虫群 1.4/1.25/1.1/0.95 攻击一次1点 16 护甲相关装备强袭 +5 玄冥盾牌系列 +2 勋章 +7 天鹰 +2 炎阳纹章 +10 祭品 +4 黯灭 -7 勋章 -7 炎阳纹章 -10 强袭 -5 枯萎之石 -2 疯脸 -5 闪避机制闪避与致盲都会在攻击完成（弹道击中）时有一定几率触发 叠加与计算多个闪避来源乘法叠加 上下坡落空几率如果攻击者处于比目标更低的位置时，远程攻击会有25%的几率落空。 攻击者和目标之间的地形的高低差异实在击中目标时决定的，中路对线过程中，可以使用弹道飞行过程位移来保持和目标的同样地形高度保证必中 飞行单位无上下坡落空几率 计算公式$\\prod_{i=0}^n$ 的含义是把i=0到n所有的项相乘 $$ 落空几率 = \\prod_{i=0}^n (1 - 闪避来源_i) \\times \\prod_{j=0}^n (1 - 致盲来源_j) \\times 上下坡落空几率 $$ $$ 命中几率 = 1 - \\prod_{i = 0}^n{(1 - 必中/克敌先机来源_i)} $$ $$ 最终命中几率 = 1 - 落空几率 \\times (1 - 命中几率) $$ $$ 最终落空几率 = 落空几率 \\times (1 - 命中几率) $$ 公式只是为了程序数值计算使用，是需要记住：每一次攻击要绕过所有的闪避成功命中，只有当所有的闪避都失败了，这次攻击才可以造成伤害。所以说，出很多个闪避装备，在一定程度上对物理核心非常克制，这时候物理核心必须出金箍棒 闪避来源 技能或物品名称 闪避几率% 敌法师 - 20级右天赋 15 磁场 100 3.5/4.5/5.5/6.5s 醉拳 10/15/20/25 一段时间的100%闪避 赏金猎人 - 25级右天赋 25 蝴蝶 35 人马 - 15级右天赋 10 克林克兹 - 20级右天赋 20 虚空 - 25级右天赋 20 黑暗贤者 - 10级右天赋 12 天堂之戟 25 噬魂鬼 - 20级右天赋 15 狼人 - 20级右天赋 15 美杜莎 - 15级左天赋 15 米波 - 20级右天赋 10 大圣 - 10级左天赋 12 模糊 20/30/40/50 猴子 - 20级左天赋 15 炎阳纹章 20 炎阳纹章- 队友使用 - 日耀 20 7s 斯温 - 20级左天赋 20 闪避护肤 20 圣堂刺客 - 15级左天赋 12 风行 100 3/4/5/6a 致盲来源 技能或物品名称 落空几率% 醉酒云雾 70 4s 麻痹之咬 30/40/50/60 2s 致盲之光 80 3/4/5s 伤残恐惧 白天10 3s 夜晚50 5/6/7/8s 辉耀 - 辉耀灼烧 17 烟雾 40/50/60/70 6s 激光 100 3/3.5/4/4.5s 小兵 6s 近战旋风飞斧 60 4/5/6/7s 克敌机先来源为一种攻击特效，防止该次攻击落空，用来反制闪避，致盲，以及远程单位上下坡的25%几率落空，也能够防止近战攻击由于目标在攻击之前超过了350距离而落空 但是攻击弹道依旧可以躲避 对建筑物无效 技能或物品名称 备注 不会落空为100% 强化图腾 带有Buff的一次攻击不会落空 棒击大地 不会落空的即时攻击 金箍棒 每次攻击带有克敌先机 复仇 破影一击不会落空 窒息之刃 不会落空的即时攻击 白银之锋 - 暗影步 破影一击不会落空 暗杀 需要A帐 自然庇护 破影一击不会落空 海象神拳！ 不会落空 死亡守卫 需要A帐 不会落空 必中来源必中防止一个单位受到的任何攻击落空 血棘的灵魂撕裂，岗哨守卫，炎阳纹章给敌方使用提供35%的必中效果 移动速度叠加相似的装备提供的移动速度不叠加，除了风帐 多个鞋类物品不叠加 夜叉 散夜对剑 幻影斧不叠加 多个战鼓或风灵之纹不叠加 风灵之纹和战鼓鞋类物品叠加 公式移动速度 = （基础移动速度 + 具体移动速度加成） * （1 + 百分比移动速度加成和减速的和） 魔法抗性魔法抗性除了米波35%，维萨吉10%魔法抗性外，其他英雄都为25%基础魔法抗性 魔法抗性乘法叠加，不同的提高魔法抗性的装备可以叠加 魔法抗性加成来源 技能或物品名称 加成数值%及备注 法术护盾 26/34/42/50 小马or小熊怪光环 英雄5 非英雄20 可叠加 魔抗斗篷 15 微光披风 15 被动 微光披风 - 微光 45 5s 0.6s渐隐时间 挑战头巾 25 狂战士之血 20/30/40/50 最大10%生命值 洞察烟斗 30 被动 洞察烟斗光环 10 腐肉堆积 6/8/10/12 失效力场 10/14/18/22 腐蚀皮肤 10/15/20/25 魔法抗性减少来源 技能或物品名称 减少数值% 备注 冰霜漩涡 15/20/25/30 16s 0.5s粘滞时间 自然秩序 40/60/80/100 光环范围350 1s粘滞时间 虚化冲击 40 敌方3s 友方4s 幽灵形态 40 4s 幽魂护罩 20 3/3.5/4/4.5 衰老 30/40/50/60 3.5 上古封印 30/35/40/45 3/4/5/6 纷争面纱 25 16 魔法抗性100%来源 技能或物品名称 备注 黑皇杖 10/9/8/7/6/5 牺牲 跳跃时间or持续5s 剑刃风暴 5s 狂暴 3/4/5/6 (天赋+1s) 石化凝视 3s 天赋5s 驱逐 4/5/6/7 命运赦令 3/3.5/4/4.5 魔法吸收护盾魔法吸收护盾计算是计算魔抗后的吸收数值，魔抗越高，护盾效果越好 任何类型魔法护盾无法叠加，同时吸收伤害 技能或物品名称 吸收数值 烈火罩 50/200/350/500 （天赋 +500） 挑战头巾 - 绝缘 325 持续12s 洞察烟斗 - 法术护盾 400 持续12s 物品被动效果叠加独立叠加 攻击力 属性加成 魔法值/生命值 生命恢复速率/魔法恢复速率（基础速率 * 加成倍数） 攻击速度加成 护甲加成 分裂区域 移动速度加成 乘法叠加出现边缘递减效应$$加成 = 1 - (1-x) \\times (1-y) \\times (1-z) \\times \\ldots$$其中 $x y z$ 都表示一个百分比 魔法抗性乘法叠加 一个100点魔法伤害的技能 英雄本身25%魔法抗性，伤害变为 100 * （1 - 25%） = 75 再装备挑战头巾，再降低30%，伤害变为 75 * （1 - 30%） = 52.5 躲避躲避是一种躲避弹道的行为，更确切的说，是使弹道完全失去跟踪目标能力的行为。白话文就是：秀操作，骚 躲避技能的方式技能以下技能在施法时能躲避弹道 炼金术士：化学狂暴 酒仙：元素分离 混沌骑士：混沌之军 噬魂鬼：感染``幻影斧：镜像 变体精灵：波浪形态 娜迦海妖：镜像 幻影长矛手：神行百变``凤凰：超新星 帕克：相位转移 力丸：绝杀秘技 风暴之灵：球状闪电 传送所有的真闪烁都能躲避弹道，躲避发生在使用技能移动时 敌法师：闪烁 闪烁匕首：闪烁 远行鞋：传送 艾欧：传送(只有艾欧传送过去时可以躲避) 先知：传送 帕克：灵动之翼 痛苦女王：闪烁 熊灵：回归 回城卷轴：传送 孽主：黑暗之门 编织者：时光倒流 陈：忠诚考验 光之守卫：召回 变体精灵：替换复制品 隐身所有能获得隐身状态的技能技能都能躲避弹道，除非敌人的在弹道到达之前使用了反隐，但是必须要注意不同技能的渐隐时间 隐藏变为临时性的隐藏不能躲避弹道。 躲避与变为隐藏无关，而是与技能本身有关。这意味着隐藏技能不一定都能躲避弹道， 但是，利用合适的时机，可阻止弹道或一般技能，击中施法者或目标。 隐藏来源有一下技能 酒仙：元素分离 混沌骑士：混沌之军 大地之灵：残炎魔咒 噬魂鬼：吸收 幻影斧：镜像 娜迦海妖：镜像 殁境神蚀者：星体禁锢 幻影长矛手：神行百变 凤凰：超新星 帕克：相位转移 力丸：绝杀秘技 暗影恶魔：崩裂禁锢 巨牙海民：雪球 无敌变为无敌不能躲避弹道，但是可以在击中时减轻或使其效果无效。 攻击伤害和技能伤害会被忽略。有一些技能可以影响无敌单位。 无敌来源 祸乱之源：噩梦 酒仙：元素分离 混沌骑士：混沌之军 大地之灵：残岩魔咒 灰烬之灵：无影拳 灰烬之灵：激活残焰 风帐：龙卷风 虚空假面：时间漫游 佣兽：石像形态 祈求者：强袭飓风 主宰：无敌斩 噬魂鬼：吸收 噬魂鬼：感染 幻影斧：镜像 变体精灵：波浪形态 娜迦海妖：镜像 娜迦海妖：海妖之歌 殁境神蚀者：星体禁锢 幻影长矛手：神行百变 凤凰：超新星 帕克：相位转移 力丸：绝杀秘技 暗影恶魔：崩裂禁锢 狂风：龙卷风 风暴之灵：球状闪电 巨牙海民：雪球 可以被躲避的弹道任何单位和英雄的所有物理攻击的弹道都可以躲避 可以被躲避的技能亚巴顿：迷雾缠绕 赏金猎人：投掷飞镖 酒仙：醉酒云雾 钢背兽：粘稠鼻涕 育母蜘蛛：孵化蜘蛛 混沌骑士：混乱之箭 陈：赎罪 戴泽：剧毒之触 龙骑士：神龙摆尾 大地：投掷巨石 撼地者：回音击 虚灵之刃：虚化冲击 变体精灵：变体攻击 泥土傀儡：投石 娜迦海妖：诱捕 食人魔魔法师：引燃 神谕者：气运之末 幻影刺客：窒息之刃 幻影长矛手：灵魂之矛 痛苦女王：暗影突袭 阿托斯：致残 天怒法师：震荡光弹 狙击手：暗杀 斯温：风暴之拳 潮汐猎人：巨浪 修补匠：导热飞弹 复仇之魂：魔法箭 冥界亚龙：蝮蛇突袭 维萨吉：灵魂超度 风行者：束缚击 寒冬飞龙：碎裂冲击 冥魂大帝：冥火暴击 不可以被躲避的技能炼金术士：不稳定化合物 天穹守望者：闪光幽魂 爱人直升机：追踪导弹 哈斯卡：牺牲 拉西克：闪电风暴 巫妖：连环霜冻 莉娜：神灭斩 莱恩：死亡一指 美杜莎：秘术异蛇 米拉娜：流星风暴 瘟疫法师：死亡脉冲 痛苦女王：痛苦尖叫 拉比克：技能窃取 天怒法师：奥法鹰隼 幽鬼：幽鬼之刃 小小：投掷 树精卫士：寄生种子 巨牙海民：雪球 寒冬飞龙：碎裂冲击弹射 巫医：麻痹药剂","tags":[{"name":"Dota2","slug":"Dota2","permalink":"https://charlesliuyx.github.io/tags/Dota2/"},{"name":"DataAnalysis","slug":"DataAnalysis","permalink":"https://charlesliuyx.github.io/tags/DataAnalysis/"}]},{"title":"【理论】Logistic Regression学习笔记","date":"2017-09-04T07:21:50.000Z","path":"2017/09/04/LogisticRegression学习笔记/","text":"什么是【回归（Regression）】回归（Regression）是一项模拟技术，用来从一个或多个解释变量中预测输出变量的值 什么是及为什么【Logistic Regression】回归（Regression）是用来预测的，比如给你一组虫子的腿长和翅膀长数据，让你判断虫子是A类虫还是B类虫。 逻辑回归则是用来预测二进制输出变量取值（如：是/不是）的预测技术 即输出变量只有两个值得预测技术 下文中将会从不同的角度 概率论角度首先，需要回忆一下几个概念 【大数定理】 $$ \\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{i=1}^n {X_i} = \\mu $$ 不断的采样一个随机变量，得到n个值，当n趋向于正无穷的时候，这个平均值就收敛于随机变量的期望 【中心极限定理】 大量相互独立{条件1}的随机变量，其均值的分布以正态分布{结论}为极限{条件2} 【贝叶斯公式】 默认你已经对条件概率了若指掌（在某件事情已经发生的情况下另一件事发生的概率），关于贝叶斯方法的前世今生，这个链接或许可以帮到你。 那贝叶斯公式是如何推出来的？ 问题描述我们需要求的问题是：你在校园里面随机游走，遇到了N个穿长裤的人（但是可能因为你高度近视你无法看出他们的性别），问，这N个人里面有多少个女生，多少个男生，即，穿裤子的人里面有多少个女生 解决过程 $$ 穿裤子的人中的女生比例 = \\frac{穿长裤的女生人数}{穿长裤的总人数} =\\\\ \\frac {U\\times P(Girl)\\times P(Paints|Girl)}{U\\times P(Boy)\\times P(Paints|Boy) + U\\times P(Girl)\\times P(Paints|Girl)}\\tag{1-1} $$ 化简上式，可以发现其实分母合起来就是 $P(Paints)$ ，分子其实就是既穿裤子又是女孩，整理得 $$ P(Girl|Paints) = \\frac{P(Girl) \\times P(Paints|Girl)}{P(Paints)} $$ 再一般化，用A表示穿裤子的，B表示女生$$P(B|A) = \\frac{P(B)\\times P(A|B)}{P(A)} = \\frac{P(AB)}{P(A)}\\tag{1-2}$$上式就是贝叶斯公式的一般形式，我们在推导中发现，正常人类对频率的感知和理解速度要高于对概率的。 比如“穿长裤的女生人数”这个概念，用总人数乘以女人比例，得出女生人数，再用女生人数乘以女生中穿裤子人数的比例得到穿裤子的女生人数。这一串推导感觉毫无困难。但如果读成：在A发生条件下，发成B的概率，会让人乍看下，感到有一定的理解困难。 我们常说Sense，我觉得这就是一种敏感，对条件概率表达方式的敏感，在你看到的时候，抓住那个最关键的点，不存在任何的迷惑 那Logistic Function和贝叶斯公式有什么联系呢？ 如果我们把公式（1-1）也符号化，$B_1$ 表示女生，$B_2$表示男生，$A$ 表示穿裤子$$P(B_1|A) = \\frac {P(B_1)P(A|B_1)}{P(B_2)P(A|B_2) + P(B_1)P(A|B_1)}\\tag{1-3}$$右边同时除以 $P(B_1)\\times P(A|B_1)$ ，并定义 $a = \\ln{\\left( \\frac{P(B_1)P(A|B_1)}{P(B_2)P(A|B_2)}\\right)}$ 直接由公式(1-3)可得到$$f(a) = \\frac{1}{1 + e^{-a}} \\tag{1-4}$$很熟悉的形式，其实就是logistic函数的一般形式（对数几率函数），而这个函数的值就是 $f(a)$ ，很明显，是一个概率 另一个很重要超级重要的常识就是：正态分布的的累计分布函数（就是从负无穷到x积分）和概率分布函数长得样子很像Logistic概率密度函数，可能看到这句话很多人就已经真相大白了，应给无论从中心极限定理出发，还是从统计学概率论角度来看，概率分布存在的价值是为了描述自然界（显示）中的随机事件，构造函数本身就十分重要，不同的规律需要不同的函数去拟合 正太分布概率密度函数（左）累计密度函数（右） Logistic函数概率密度函数（左）累计密度函数（右） 统计学角度动机 - 需要解决什么问题在现实生活中，有时候需要探究某一事件 $A$ 发生的概率 $P$ （0 - 1 之间的一个数）与某些因素 $\\mathbf X = (X_1, X_2, \\ldots, X_p)’$ 之间的关系。（其中1到p是各种不同的因素） ☆ 【核心问题】考虑到很多情况下，$P$ 对 $\\mathbf X$ 的变化并不敏感，即 $\\mathbf X$ 需要发生很大的变化才能引起 $P$ 的微弱改变 比如，农药的用量和杀死害虫的概率之间，在农药用量在很小的范围内增长的时候，因为药效不够，杀死害虫的概率增长很慢。 因此，我们要构造一个关于 $P$ 的函数 $\\theta(P)$ ，使得它在 $P = 0$ 或 $P = 1$ 附近，$P$ 的微小变化对应 $\\theta(P)$ 的较大改变，同时，$\\theta(P)$ 要尽可能的简单。于是，我们可以构造一个函数（注意：构造函数是数学中很有效的手段，我们需要什么特性就用什么方法来构造一个满足我们需求的函数）c$$\\frac {\\partial \\theta(P)}{\\partial P} =\\frac{1}{P} +\\frac{1}{1-P}$$根据上述公式可以解得$$\\theta(P) =\\ln\\left(\\frac{P}{1-P}\\right)$$ 可视化 这个 $\\theta(P)​$ 就是Logit变换，可以看到，这个函数很符合我们的要求： $P = 0​$ 或 $P = 1​$ 附近，$P​$ 的微小变化对应 $\\theta(P)​$ 的较大改变 方案 - 如何解决这个问题为了建立因变量 $P$ 与自变量 $\\mathbf X$ 之间的合理变动关系，一个很自然的假设就是线性关系，也就是：$$P = \\mathbf X’ \\boldsymbol{\\beta}$$其中 $\\boldsymbol \\beta = (\\beta_1,\\beta_1,\\ldots,\\beta_p)$ 表示每一个不同因素对最终概率 $P$ 产生的影响（这个也可以写作，权重weight） 由需求可知，在某些情况下，$P = 0$ 或 $P = 1$ 附近，$P$ 对 $\\mathbf X$ 的变化并不敏感，简单的线性关系不能反映这一特征。此时，构造的 $\\theta(P)$ 就派上用场了$$\\ln\\left(\\frac{P}{1-P}\\right) = \\mathbf X’ \\boldsymbol{\\beta}$$进行一系列的公式推导有$$\\ln\\left(\\frac{P}{1-P}\\right) = \\mathbf X^\\mathrm T \\boldsymbol{\\beta} \\implies \\frac{P}{1-P} = e^{\\mathbf X^\\mathrm T \\boldsymbol{\\beta}} \\implies P = \\frac{e^{\\mathbf X^\\mathrm T \\boldsymbol{\\beta}}}{1 + e^{\\mathbf X^\\mathrm T \\boldsymbol{\\beta}}}$$则上述最后推出的就是Logistic回归模型 机器学习角度周志华《机器学习》，3.3 对数几率回归笔记 和统计学角度相同，我们的目的是依旧是完成一个二分类任务，输出标记 $y \\in {0,1}$ ，而线性回归模型产生的预测值 $z = \\boldsymbol w^{T}\\boldsymbol x + b$ 是实值，于是，我们需要把 z 转换为0/1值，最理想的是单位阶跃函数（unit-step function z &gt; 0➜y=1，z&lt;0➜y=1） 单单位阶跃函数不连续，不能微分，积分，求逆，于是我们希望找到能在一定程度上近似单位阶跃函数的替代函数（surrogate function），并希望它单调可微，答案很明显，就是对数几率函数（logistic function）$$y = \\frac{1}{1+e^{-z}}$$ z 为预测值，y 为输出，对数几率函数是一种Sigmoid函数【一种形状类似S的函数】，将$z = \\boldsymbol w^{T}\\boldsymbol x + b$ 带入上面的公式 $$y = \\frac{1}{1+e^{-(\\boldsymbol w^{T}\\boldsymbol x + b)}} \\implies \\ln(\\frac{y}{1-y}) = \\boldsymbol w^{T}\\boldsymbol x + b$$如果将 $y$ 作为 $\\mathbf x$ 作为正例的可能性，$1-y$ 为其反例的可能性$$\\frac {y}{1-y}$$上面的式子成为“几率”(odds)：表示 $\\mathbf x$ 是正例的相对可能性，对odds取对数得到“几率对数”(log odds，也就做logit) 生态学角度可以换一个角度来解读这个问题的前世今生 1798年的时候一个叫Malthus的英国牧师发现人口的变化率和人口的数目成正比，需要用数学的手法建立一个公式来表征这个现象，则，使用 $N(t)$ 这个函数来表示t时刻某个地区的总人口数（根据成正比）$$\\frac{dN(t)}{dt} = {rN(t)}$$ 其中，r是常数，表示 $N(t)$ 的变化率 直接解出这个方程$$N(t) = N_0e^{rt}$$这很明显是一个指数增长函数，其实也是种群增长的函数表示 但是问题也是很明显的：种群因为环境容量的限制一定是不能无限增长的，即，这个模型非常不靠谱，需要重新设计模型来复合现实中的情况。Pierre-François Verhulst 在1838年提出，构造一个函数$$\\frac{dN(t)}{dt} = {rN(t)}\\left(1 - \\frac{N(t)}{K}\\right)$$ K是一个常数，表示系统的容量（capacity） 令 $f(t) = \\frac{N(t)}{K}$ ，在方程两边同时除以 $K$ ，上述方程变为：$$\\frac{df(t)}{dt} = rf(1 - f)$$这也是Logistic方程的一般形式 总结从不同的角度来研究问题就会发现，其实很多时候我们解决一个问题具有一个相似的模式，包括大数定律，贝叶斯全概率公式是一切的基石和解决问题的主要工具 一个模型的建立规则依据数据的分布特征，而这里依托的一个关键信息就是：在靠近输入0，1两点的时候，y随x的变化不明显，线性模型没法很好的反应这个特征，所以就构造了一个逻辑回归模型来表示这个特征 并且Logistic回归模型的本质是一个概率模型，因为在描述该分类时，我们其实是以概率来衡量的 重要概念均方误差 Mean Squre Error MSE指参数估计值与参数真值之差平方的期望值，是一种目标函数（Objective Function），常用于线性回归$$MSE = \\frac{1}{n} \\sum_{t = 1}^n{(observed_t - predicted_t)}^2$$ 交叉熵 Cross Entropy又称为logloss，是Objective function的一种，也称Loss function or Coss Function 什么是熵我觉得这个问题必须搞明白一件事就是：什么是熵 Entropy 广义的定义是：熵是描述一个系统的无序程度的变量；同样的表述还有，熵是系统混乱度的度量，一切自发的不可逆过程都是从有序到无序的变化过程，向熵增的方向进行 有一个很神奇的解释是：熵字为火字旁加商。当时有位姓胡的学者作为普朗克的防疫。S(entropy)定义为热量Q与温度的比值，所以造字：熵 至于信息论上熵的概念更有意思，有兴趣可以转到 要理解这个Cross Entropy，必须了解它是用来干啥的？ 延伸：信息熵 交叉熵 相对熵的理解，需要跳转到另一篇笔记：什么是信息熵、交叉熵和相对熵 简单来说Cross Entropy可以表示可以度量最终训练结果于测试集的差异程度，MSE也是同样的作用。 换种更具体的说法：我们用p表示真实标记（训练样本标记）的分布，q是训练后的模型的预测标记（输出值标记）的分布，而交叉熵损失函数可以衡量p与q的相似性。 似然函数定义：给定联合样本值 $x$ 关于（未知 - 因为也是一边的自变量）参数 $\\theta$ 的函数$$L(\\theta|x) = f(x;\\theta)$$ $x$ 指联合样本随机变量 $X$ 取到的值，比如天气取值 $X$ =【晴，阴，雨，雪】$x$ = 晴 $\\theta$ 指未知参数，属于参数空间，比如正态分布的均值，方差等 $f(x;\\theta)$ 是密度函数，表示 $\\theta$ 参数下联合样本值 $x$ 的联合密度函数（所以这里不用|符号，|符号表达的意思是条件概率或条件分布） 从定义上，似然函数和密度函数是完全不同的两个数学对象：前者是关于 $\\theta$ 的函数，后者是关于 $x$ 的函数。中间的等号理解成函数值形式相等 这个等式表示的是对于事件发生的两种角度的看法。左边表示概率，右边表示可能性。要表达的含义都是：给定一个样本 $x$ 后，我们去测度这个样本出现的可能性到底有多大。说人话，比如样本空间是 $X$ =【晴，阴，雨，雪】，函数表达的就是样本 $x$ = 晴在这个样本空间下发生的概率或可能性 从统计学的角度来说，这个样本的出现一定是基于一个分布的（比如二项分布，只正态分布等等），那么我们假设这个分布为 $f(x;\\theta)$ ，对于不同的 $\\theta$ 样本的分布不一样。 $f(x;\\theta)$ 函数表示的就是在参数 $\\theta$ 下 $x$ 出现的概率有多大（可以带入天气例子思考） $L(\\theta|x)$ 表示在给定样本 $x$ ，哪个参数 $\\theta$ 使得 $x$ 出现的可能性有多大。说人话，我们已经知道天气是晴天，哪个参数（可能是 $\\theta_1$ $\\theta_2$）使得这个函数值最大 对于Logistic Regression 为什么要用LogLoss - Cross Entropy了解了熵，和似然函数，我们可以开始看看在Logistic Regression的条件下为什么要用LogLoss，换句话也就是说，它一定有它的优势，我们采用，那么它有什么优势？ Logistic Regression的本质还是一个二分类问题，即Y = 0，or Y = 1 令 $P(Y=1|x) = \\pi(x)$ $$P(Y=1|x) = 1 - \\pi(x)$ $y_i$ 表示i次试验，取值就是0 or 1（二分类问题） $\\pi(x) = \\frac{1}{1 + e^{-wx}}$ 是Logistic Function的表现形式，其中w相当于似然函数一节提到的 $\\theta$ 是需要求的参数（加深理解，其实在二分类问题中，Logistic函数就是一种形式上的概率分布的表现形式） 所以使用基本概率方法可以求解二分类的问题的似然函数 $$ \\ell(w) = \\prod_{i = 1}^{N} [\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i} $$ 注解：说白就和算扔N次硬币，一个连续正反事件串的概率是多少一个含义 看到乘法和指数，第一反应取对数，得到对数似然函数$$L(w) = \\sum_{i=1}^N{[y_ilog\\pi(x_i) + (1-y_i)log(1-\\pi(x_i))]}$$ 如果跟随我的步伐走到这一步，你会发现，这个形式，前半部分是“正例成立”的熵，后半部是“反例成立”的熵，两者合称交叉熵，指“正例”和“反例”之间交叉，即第一类和第二类之间交叉说实话，叫做交叉熵还是和二项分布，伯努利过程分不开联系。在上面不远的地方已经详细定义了这几个符号代表的意思，不记得了可以再看看，加深印象。 我们发现，$-\\frac{L(w)}{N}$ 就是我们一直使用的Objective function or Loss Function or Cost Function，加负号是为了求最小值。总之，训练的目的就是要求能够使得这个函数达到最小的参数，最终的目的还是参数，这里对于参数就是 $w$ ，这个参数在上方的统计学角度，和机器学习角度都进行的讨论，重复阅读可以链接这些知识点 至于LogLoss的好处，一是取对数之后，乘法边加法，指数放下来，是凸函数，方便可以寻找最优解。二是加快了收敛速度，这里有个形象的步长比喻，可以想象成去了对数后，缩小了尺度，可以让最快梯度下降法要走的距离变短","tags":[{"name":"BitTiger","slug":"BitTiger","permalink":"https://charlesliuyx.github.io/tags/BitTiger/"},{"name":"Theory","slug":"Theory","permalink":"https://charlesliuyx.github.io/tags/Theory/"},{"name":"Model","slug":"Model","permalink":"https://charlesliuyx.github.io/tags/Model/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://charlesliuyx.github.io/tags/Machine-Learning/"}]},{"title":"Xpath使用指南","date":"2017-08-28T19:00:33.000Z","path":"2017/08/28/Xpath使用指南/","text":"XPath 相关例子Note 例子1123456789101112131415from lxml import etreesample1 = \"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;My page&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;Welcome to my &lt;a href=\"#\" src=\"x\"&gt;page&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;This is the first paragraph.&lt;/p&gt; &lt;!-- this is the end --&gt; &lt;/body&gt;&lt;/html&gt;\"\"\"def getxpath(html): return etree.HTML(html)s1 = getxpath(sample1) //绝对路径 text() 获取内容中的文字信息 1s1.xpath('//title/text()') 1[&apos;My page&apos;] / 相对路径 1s1.xpath('/html/head/title/text()') 1[&apos;My page&apos;] 获取属性src的值 1s1.xpath('//h2/a/@src') 1[&apos;x&apos;] 获取所有属性href的值 1s1.xpath('//@href') 1[&apos;#&apos;] 获取网页中的所有文本 1s1.xpath('//text()') 12345678910111213[&apos;\\n &apos;, &apos;\\n &apos;, &apos;My page&apos;, &apos;\\n &apos;, &apos;\\n &apos;, &apos;\\n &apos;, &apos;Welcome to my &apos;, &apos;page&apos;, &apos;\\n &apos;, &apos;This is the first paragraph.&apos;, &apos;\\n &apos;, &apos;\\n &apos;, &apos;\\n&apos;] 获取网页中的所有注释 1s1.xpath('//comment()') 1[&lt;!-- this is the end --&gt;] 例子212345678910111213sample2 = \"\"\"&lt;html&gt; &lt;body&gt; &lt;ul&gt; &lt;li&gt;Quote 1&lt;/li&gt; &lt;li&gt;Quote 2 with &lt;a href=\"...\"&gt;link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Quote 3 with &lt;a href=\"...\"&gt;another link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;h2&gt;Quote 4 title&lt;/h2&gt;Something here.&lt;/li&gt; &lt;/ul&gt; &lt;/body&gt;&lt;/html&gt;\"\"\"s2 = getxpath(sample2) 获取所有li中的文本 1s2.xpath('//li/text()') 1[&apos;Quote 1&apos;, &apos;Quote 2 with &apos;, &apos;Quote 3 with &apos;, &apos;Something here.&apos;] 获取第一个 第二个li中的文本，两种写法均可 1s2.xpath('//li[position() = 1]/text()') 1[&apos;Quote 1&apos;] 1s2.xpath('//li[1]/text()') 1[&apos;Quote 1&apos;] 1s2.xpath('//li[position() = 2]/text()') 1[&apos;Quote 2 with &apos;] 1s2.xpath('//li[2]/text()') 1[&apos;Quote 2 with &apos;] 奇数 偶数 最后一个 1s2.xpath('//li[position() mod2 = 1]/text()') 1[&apos;Quote 1&apos;, &apos;Quote 3 with &apos;] 1s2.xpath('//li[position() mod2 = 0]/text()') 1[&apos;Quote 2 with &apos;, &apos;Something here.&apos;] 1s2.xpath('//li[last()]/text()') 1[&apos;Something here.&apos;] li下面a中的文本 1s2.xpath('//li[a]/text()') 1[&apos;Quote 2 with &apos;, &apos;Quote 3 with &apos;] li下a或者h2的文本 1s2.xpath('//li[a or h2]/text()') 1[&apos;Quote 2 with &apos;, &apos;Quote 3 with &apos;, &apos;Something here.&apos;] 使用 | 同时获取 a 和 h2 中的内容 1s2.xpath('//a/text()|//h2/text()') 1[&apos;link&apos;, &apos;another link&apos;, &apos;Quote 4 title&apos;] 例子312345678910111213sample3 = \"\"\"&lt;html&gt; &lt;body&gt; &lt;ul&gt; &lt;li id=\"begin\"&gt;&lt;a href=\"https://scrapy.org\"&gt;Scrapy&lt;/a&gt;begin&lt;/li&gt; &lt;li&gt;&lt;a href=\"https://scrapinghub.com\"&gt;Scrapinghub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\"https://blog.scrapinghub.com\"&gt;Scrapinghub Blog&lt;/a&gt;&lt;/li&gt; &lt;li id=\"end\"&gt;&lt;a href=\"http://quotes.toscrape.com\"&gt;Quotes To Scrape&lt;/a&gt;end&lt;/li&gt; &lt;li data-xxxx=\"end\" abc=\"abc\"&gt;&lt;a href=\"http://quotes.toscrape.com\"&gt;Quotes To Scrape&lt;/a&gt;end&lt;/li&gt; &lt;/ul&gt; &lt;/body&gt;&lt;/html&gt;\"\"\"s3 = getxpath(sample3) 获取 a 标签下 href 以https开始的 1s3.xpath('//a[starts-with(@href, \"https\")]/text()') 1[&apos;Scrapy&apos;, &apos;Scrapinghub&apos;, &apos;Scrapinghub Blog&apos;] 获取 href=https://scrapy.org 1s3.xpath('//li/a[@href=\"https://scrapy.org\"]/text()') 1[&apos;Scrapy&apos;] 获取 id = begin 1s3.xpath('//li[@id=\"begin\"]/text()') 1[&apos;begin&apos;] 获取text = Scrapinghub 1s3.xpath('//li/a[text()=\"Scrapinghub\"]/text()') 1[&apos;Scrapinghub&apos;] 获取某个标签下 某个参数 = xx 1s3.xpath('//li[@data-xxxx=\"end\"]/text()') 1[&apos;end&apos;] 1s3.xpath('//li[@abc=\"abc\"]/text()') 1[&apos;end&apos;] 例子41234567891011121314151617181920212223sample4 = u\"\"\"&lt;html&gt; &lt;head&gt; &lt;title&gt;My page&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;Welcome to my &lt;a href=\"#\" src=\"x\"&gt;page&lt;/a&gt;&lt;/h2&gt; &lt;p&gt;This is the first paragraph.&lt;/p&gt; &lt;p class=\"test\"&gt; 编程语言&lt;a href=\"#\"&gt;python&lt;/a&gt; &lt;img src=\"#\" alt=\"test\"/&gt;javascript &lt;a href=\"#\"&gt;&lt;strong&gt;C#&lt;/strong&gt;JAVA&lt;/a&gt; &lt;/p&gt; &lt;p class=\"content-a\"&gt;a&lt;/p&gt; &lt;p class=\"content-b\"&gt;b&lt;/p&gt; &lt;p class=\"content-c\"&gt;c&lt;/p&gt; &lt;p class=\"content-d\"&gt;d&lt;/p&gt; &lt;p class=\"econtent-e\"&gt;e&lt;/p&gt; &lt;!-- this is the end --&gt; &lt;/body&gt;&lt;/html&gt;\"\"\"s4 = etree.HTML(sample4) 获取 class = test 标签中的所有文字 1s4.xpath('//p[@class=\"test\"]/text()') 1[&apos;\\n 编程语言&apos;, &apos;\\n &apos;, &apos;javascript\\n &apos;, &apos;\\n &apos;] 使用String来获得文字段； strip() 移除字符串收尾字符，默认为空格 1print (s4.xpath('string(//p[@class=\"test\"])').strip()) 123编程语言python javascript C#JAVA 获取所有class属性中以content开始的 1s4.xpath('//p[starts-with(@class,\"content\")]/text()') 1[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;] 获取所有class属性中包含content的 1s4.xpath(('//*[contains(@class,\"content\")]/text()')) 1[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;]","tags":[{"name":"crawl","slug":"crawl","permalink":"https://charlesliuyx.github.io/tags/crawl/"}]},{"title":"PDF复制粘贴去除多余的回车符","date":"2017-07-30T06:03:08.000Z","path":"2017/07/29/PDF复制粘贴去除多余的回车符/","text":"直接上解决步骤，但是只能适用于Windows平台，Mac这边可以尝试用Alfred + workflow来对剪切板操作来解决，或者用BetterTouchTool的自带个性化功能来尝试。只是一个思路，没有在Mac系统尝试 下载 Autohotkey ，安装（这一步都卡住那估计救不了了） 桌面右键 ➜ 新建 ➜ 创建新的AutoHotkey Script 右键创建的文件 ➜ 选择 Edit Script 出来一个记事本 编辑记事本文件，在已经有的内容下直接加上 1234567891011121314151617#IfWinActive ahk_class classFoxitReader^c:: old := ClipboardAll clipboard := &quot;&quot; send ^c clipwait 0.1 if clipboard = clipboard := old else &#123; tmp := RegExReplace(clipboard, &quot;(\\S.*?)\\R(.*?\\S)&quot;, &quot;$1 $2&quot;) clipboard := tmp StringReplace clipboard, clipboard, % &quot; &quot;, % &quot; &quot;, A clipwait 0.1 &#125; old := &quot;&quot; tmp := &quot;&quot;return 这里有个问题 IfWinActive ahk_class classFoxitReader 第一行的classFoxitReader 是指的你用什么程序打开PDF 如果是FoxitReader就是classFoxitReader 如果是Acrobat Adobe就是AcrobatSDIWindow 可以用Autohotkey中的 WinGetClass 来获得某一个窗口的ahk_class 保存退出 桌面上双击你刚刚编辑的文件，可以看到右下角出现了一个H形状的图标 大功告成，这时候你再试试去PDF文档里面ctrl + c就没有回车符了（当然，段落还是无法区分的），也不一定，这一段既然是脚本语言，那就有无限的可能性，就看你的算法实现能力了对吧！","tags":[{"name":"Tools","slug":"Tools","permalink":"https://charlesliuyx.github.io/tags/Tools/"},{"name":"Autohotkey","slug":"Autohotkey","permalink":"https://charlesliuyx.github.io/tags/Autohotkey/"}]},{"title":"到底什么是比特币","date":"2017-07-26T07:11:31.000Z","path":"2017/07/26/到底什么是比特币/","text":"其实 江卓尔 知乎回答","tags":[{"name":"BTcoin","slug":"BTcoin","permalink":"https://charlesliuyx.github.io/tags/BTcoin/"},{"name":"Blockchain","slug":"Blockchain","permalink":"https://charlesliuyx.github.io/tags/Blockchain/"}]},{"title":"LeetcodeNote","date":"2017-07-01T07:18:41.000Z","path":"2017/07/01/LeetcodeNote/","text":"算法培训课程基本模型汇总笔记 线基本模型数学归纳法树基本模板 Draw/Equation -&gt; Tree shape Define TreeNode 本点信息必然是辅助变量，计入TreeNode 孩子信息决定TreeNode的形状 任何第一次走的节点，如果不能走，一定要画出来打一把叉 Binary Search123456789101112131415Public int func(T[] array, V tartget )&#123; int pos = -1; int start = 0; int end array.length - 1; while ( start &lt;= end )&#123; int mid = start + (end - start)/2; if ( f(a[mid]) &lt;= target )&#123; pos = mid; start = mid + 1; &#125; else &#123; end = mid - 1; &#125; &#125; return pos;&#125; Bottom up - Recursion123456789101112131415public &lt;T_P&gt; func(T_v_1, v1 …)&#123; checkhastreeNode(); return helper(root(T_v_1, v_1, …))&#125;private &lt;T_P&gt; helper(T_v_1, v1, …)&#123; resultchildfirst = helper(childFirst); … resultchildlast = helper(childLast); -&gt; result by childs //generate cur node's result; return result;&#125; DFS12345678910111213141516171819202122232425262728public class DFSTree &#123; public Type_R func(T_1, e1, T_2, e2)&#123; checkrootexists(); TreeNode[] array = new TreeNode[TREE_HEIGHT]; Stack&lt;TreeNode&gt; stack = Stack&lt;&gt;(); stack.push(root); while (!stack.Empty())&#123; TreeNode curNode = stack.pop(); Operation at node; stack.push(childLast); … stack.push(childFirst); &#125; return result; &#125; private class TreeNode&#123; T_V_1 field_1; … T_V_q field_q; int _height; &#125; BFS1234567891011121314151617181920212223242526272829303132public class BFS &#123; public TypeR func(T_1 v_1, T_p, v_p) &#123; checkexistroot(); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while ( !queue.isEmpty() )&#123; int size = queue.size(); for ( int I = 0; I &lt; size; i++ )&#123; TreeNode node = queue.remove(); op at node; queue.add(childFirst); … queue.add(childLast); &#125; update var_l,…,var_k for next level &#125; return result; &#125; private class TreeNode&#123; T_1 field_1; … &#125;&#125; 图基本模板","tags":[{"name":"算法","slug":"算法","permalink":"https://charlesliuyx.github.io/tags/算法/"},{"name":"Leetcode","slug":"Leetcode","permalink":"https://charlesliuyx.github.io/tags/Leetcode/"}]},{"title":"幕布-全平台笔记思维导图工具","date":"2017-06-17T17:30:47.000Z","path":"2017/06/17/幕布-全平台笔记思维导图工具/","text":"利益相关：幕布深度使用用户 向大家强烈自来水一款从知乎上了解到的效率神器：幕布，简直相见恨晚，自从它4月正式上线后就一直使用，对我的日常生活、学习和计划帮助巨大（个人情况：硕士CE在读，ML方向，效率至上主义者，简约UI风格拥护者） 幕布是一款思维管理工具，可以用来做笔记，梳理思路，做待办事项等等等等。 人类的记忆是有缺陷的，计算机能帮助人进行记忆。我们可以记住大方向的条目，再借助一个笔记软件来唤起我们的记忆。树形结构是一种极为高效的模式及手段。 笔记软件很多，思维导图软件很多，但是能同时满足以下几点的我找了很久都没有找到，直到遇到幕布。如果你和我也有同样的需求，真心的希望这款优秀的软件能帮助到你，提高的你的日常效率，让每一次阅读，每一个计划都高效落地 UI简约，专注于层次输入本身幕布的官网是这样的： 幕布官网 幕布作为一款笔记软件编辑界面是这样的，幕布专注于层次化输入，每一个输入对于幕布来说都是一个条目，条目就是我们进行知识梳理的主干 幕布编辑界面 每一个操作都提倡使用快捷键，拒绝鼠标 + 键盘混用带来的输入思路打断的低效率 幕布快捷键页面 全平台，云存储和同步，客户端离线编辑答主因为同时使用各种设备：12.9寸iPad，Mac，Windows + Linux 台式机和iPhone手机几个工作平台，平常使用电脑进行笔记记录，碎片化时间使用个手机进行背诵记忆等，需要一款全平台的笔记软件。 而幕布，只要有浏览器，有网络就能流畅使用，有离线需求的用户，也可以通过客户端的形式满足日常编辑的需求。 幕布全平台 一键生成思维导图选择幕布的重要理由之一，废话不多说，上Gif！ 幕布快捷键页面 用过各种软件，Coggle是UI最漂亮的，但是基本的演示需求幕布完全可以满足，清楚明了 那么话说回来了，幕布可以用来干什么呢？下面就展示几个主要应用场景（有我自己的，也有来自于幕布使用趣味案例） 首先，官网给出了一份幕布产品引导，其中详细介绍了幕布的使用场景 读书笔记 方案计划 流程说明 等等 下面案例一些答主自己日常的一些特殊使用场景，基本应用比如做笔记，待办事项，做日程规划等不一一列出来了，就是基本的笔记需求。 TED演讲笔记没有遇到幕布之前，我经常看TED的各种演讲，用于开拓视野，进行英语表达的积累，做笔记的速度太慢，太不方便，遇到幕布之后，我将TED的视频中很关键的内容记录成幕布的条目层次，之后利用碎片化的时间使用手机客户端进行记忆和背诵，极大的丰富了我的谈资（记住的东西才能侃，有条理有依据的说辞才有说服力） 埃里克 哈世延：下一个科学界大突破是什么 对于这个TED笔记例子 演讲人大体思路 经典的单句和例子（中英文） 另一方面，因为幕布的全平台特性，我会用碎片化的时间利用电脑端的幕布来进行背诵（TED的演讲内容对于积累对应领域的英文表达方式有很大的帮助） 程序设计和Presentation编程前先走流程和功能设计是我平时的习惯，这里有一个很简单的Server-Client模式的练习设计用法：设计程序功能，直接一键思维导图展示，PPT完全不用做了，非常愉悦 程序Feature Dota2 Wiki在国外我发现Dota2维基十分的给力，作为Dota2玩家有一些施法距离，施法机制等有时候需要查看（进阶），但是使用网站一方面内容太多，国内访问实在太慢了，而且搜索功能也做的不好，至于我做了什么事情，各位看gif自行感受 Dota2 【利益相关：正在制作，预计Ti7前可以上线，希望也能通过数据帮助到中国军团吧，作为一个做计算机的程序员也希望贡献自己的一份力量】 期末考试复习幕布可以帮助我们把书读薄，我们知道所有的书的特点就是具有层次化，每一本非常优秀的教材都有一套自己对于本学科的知识体系的理解和层次化抽象，之前我进行期末复习需要的时间大概是7天左右，有了幕布可以把时间缩减为3天或者更短 期末复习笔记总览 期末复习笔记具体内容 幕布精选在幕布里，学习知乎模式，你也可以分享自己中意的作品，获得点赞，在后面讨论，甚至有打赏功能，因为软件本身还很年轻，一切还在发展阶段，对于我本人来说，幕布精选的内容只是锦上添花，我个人不太需求这个功能，但是其中还有一份驾考总结挺有用的，哈哈 幕布精选 幕布精选打赏功能 总结和杂七杂八我现在的习惯是，只要是读微信公众号的文章，做笔记，读书等，都会用幕布进行记录和整理，感觉提升效率十分明显（节省了我30-40%左右的时间，每天） 幕布提高我的三个能力 整理和总结的能力【如何把书读薄】 层次化思维能力【有组织的整理自己的知识体系和思路模式，加强效率，节省时间】 背诵能力【全平台（手机），我对碎片化时间能有效利用，我可以多次重复背诵需要背诵的内容】 最后，谢谢你阅读本答案到这个位置，对于我来说，幕布这种层次化的思维模式解决了我当年考高考时候的问题：什么学习方法是最好的？我觉得幕布的层次化整理知识的能力就是答案，幕布提供的是一张纸，一支笔，最后使用幕布能把你的学习生活提升到什么程度，完全取决于你的能力本身，幕布只是工具，帮助你整理你的大脑，帮你进行背诵，方便查阅。 工具永远是工作，创造效益的永远是你，未来也是人创造的，不是工具。 【利益相关，使用我的幕布分享链接可以获得15天的免费高级版试用机会，跪求点击注册！hohohohoho】 我的分享链接 幕布，绝对是一个神器，希望能帮助到各位，提升效率，创造更大的价值！","tags":[{"name":"效率工具","slug":"效率工具","permalink":"https://charlesliuyx.github.io/tags/效率工具/"},{"name":"幕布","slug":"幕布","permalink":"https://charlesliuyx.github.io/tags/幕布/"}]},{"title":"深入浅出看懂AlphaGo如何下棋","date":"2017-05-27T18:51:22.000Z","path":"2017/05/27/AlphaGo运行原理解析/","text":"问题分析围棋问题，棋盘 19 * 19 = 361 个交叉点可供落子，每个点三种状态，白（用1表示），黑（用-1表示），无子（用0表示），用 $\\vec s$ 描述此时棋盘的状态，即棋盘的状态向量记为 $ \\vec s$ （state首字母）。 $$\\vec s = (\\underbrace{1,0,-1,\\ldots}_{\\text{361}})\\tag {1-1}$$假设状态 $\\vec s$ 下，暂不考虑不能落子的情况， 那么下一步可走的位置空间也是361个。将下一步的落子行动也用一个361维的向量来表示，记为 $\\vec a$ （action首字母）。$$\\vec a = (0,\\ldots,0,1,0,\\ldots)\\tag {1-2}$$公式1.2 假设其中1在向量中位置为39，则 $\\vec a$ 表示在棋盘(3,1)位置落白子，3为横坐标，1为列坐标 有以上定义，我们就把围棋问题转化为。 任意给定一个状态 $\\vec s$ ，寻找最优的应对策略 $\\vec a$ ，最终可以获得棋盘上的最大地盘 总之 看到 $\\vec s$ ，脑海中就是一个棋盘，上面有很多黑白子 看到 $\\vec a$ ，脑海中就想象一个人潇洒的落子 接下来的问题是，如何解决这样一个问题呢？ 先上论文！干货第一 Mastering the game of Go with deep neural networks and tree search 问题解决首先想到，棋盘也是一幅图像，那么在当时最好用的图像处理算法就是深度卷积神经网络（Deep Convolutional Neural Network）。 深度卷积神经网络——策略函数（Policy Network）关于什么是CNN，这篇文章十分靠谱，深入浅出的讲解了什么是CNN An Intuitive Explanation of Convolutional Neural Networks （好像原地址挂了）（5.29更新，原地址已经恢复，原地址的排版更好，估计之前那个博主在进行博客的整理） 大致可以理解为： CNN例子 对一副图像进行处理，给定很多样本进行训练，使得最后的神经网络可以获得指定（具有分类效果）的输出。 比如，根据上图可以观察到（这是一个已经训练好的神经网络），最右侧的输出是[0.01 , 0.04 , 0.94 , 0.02]，其中第三个值0.94代表的是boat，接近1，所以我们判断这幅图片中有船这个物体（类似的，如果使用这幅图像进行训练，那么指定输出应该是[0, 0, 1, 0]，因为图中只有船这个物体） 在Deep Learning中，卷积层的中的Filter也需要训练，也就是说我们使用已有数据来学习图像的关键特征，这样，就可以把网络的规模大幅度的降低 总而言之，CNN可以帮助我们提取出图像中有实际含义的特征，那么这和围棋又有什么关系呢？我们来看看Deepmind团队是怎么运用CNN来解决围棋问题。 深度卷积神经网络解决围棋问题2015年，Aja Huang在ICLR的论文Move Evaluation in Go Using Deep Convolutional Neural Networks中就提出了如何使用CNN来解决围棋问题。 他从围棋对战平台KGS上获得了人类选手的围棋对弈棋谱，对于每一个状态 $ \\vec s$，都会有一个人类进行 $ \\vec a$ 的落子，这也就是一个天然训练样本 $ \\langle \\vec s,\\vec a\\rangle $，如此可以得到3000万个训练样本。 之后，将 $ \\vec s$ 看做一个19*19的二维图像（具体实现依据论文输入数据是19*19*48（48是这个位置的其他信息，比如气等信息，激励函数用的 tanh）使用CNN进行训练，目标函数就是人类落子向量 ${\\vec a}’$，通过使用海量的数据，不断让计算机接近人类落子的位置。就可以得到一个模拟人类棋手下棋的神经网络。 使用训练的结果，我们可以得到一个神经网络用来计算对于每一个当前棋盘状态 $ \\vec s$ ，所对应的落子向量 $ \\vec a$ 的概率分布（之所以是概率分布，是因为，计算好的神经网络，输出一般是一个0-1之间的浮点数，越接近1的点表示在这个位置越接近人类的风格，也可以等同于作为人类概率最大的落点。$$\\vec a=f(\\vec s) \\tag{2-1}$$根据公式2.1，我们记 $f()$ 为$P_{human}(\\vec s)$ ，论文中也叫做Policy Network，也称策略函数。表示的含义是 在状态 $\\vec s$ 下，进行哪一个落子 $\\vec a$ 是最接近人类风格的 计算出来的直观结果，对应到棋盘上如下图，可以看到，红色的区域的值有60%，次大值位于右方，是35%（此图来自于AlphaGo论文） Policy Network 还记得刚刚举得船图的例子嘛？可以类比一下，机器发现现在的状态 $ \\vec s$ 和之前的某一种类型有些类似，输出是一个1*361的向量，其中有几个值比较大（接近1就是100%），那么就用这个值当做下一个 $ \\vec a$ 的位置。不幸的，这种训练方法有很大的局限的，可以直观想到的是，如果对战平台上数据本身就都是俗手，那不是训练出来一个很蠢的神经网络嘛？棋力如何呢？ 深度卷积网络策略的棋力很不幸，据Aja Huang本人说，这个网络的棋力大概相当于业余6段所有的的人类选手。远远未能超过当时最强的围棋电脑程序CrazyStone。 既然比不过，那么就学习它，Aja Huang打算把 $P_{human}(\\vec s)$ 和CrazyStone结合一下，那么问题就来了， CrazyStone是怎么来解决围棋问题的呢？ 这是Aja Huang的老师Remi Colulum在2006年对围棋AI做出的另一大重要突破 干货论文送上 MCTS Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search MCTS 蒙特卡洛搜索树——走子演算（Rollout）蒙特卡洛搜索树（Monte-Carlo Tree Search）是一种大智若愚的方法，它的基本思想是： 首先模拟一盘对决，使用的思路很简单，随机 面对一个空白棋盘 $\\vec s_0$，最初我们对棋盘一无所知，假设所有落子的方法分值都相等，设为1 之后，【随机】从361种方法中选一种走法 $\\vec a_0$，在这一步后，棋盘状态变为 $\\vec s_1$。之后假设对方也和自己一样，【随机】走了一步，此时棋盘状态变为 $\\vec s_2$ 重复以上步骤直到 $\\vec s_n$并且双方分出胜负，此时便完整的模拟完了一盘棋，我们假设一个变量r，胜利记为1，失败则为0 那么问题就来了，如果这一盘赢了，那意味着这一连串的下法至少比对面那个二逼要明智一些，毕竟我最后赢了，那么我把这次落子方法 $(\\vec s_0, \\vec a_0)$ 记下来，并把它的分值变化：$$\\text{新分数} = \\text{初始分数} + r \\tag{2-2}$$同理，可以把之后所有随机出来的落子方法 $(\\vec s_i, \\vec a_i)$ 都应用2-2公式，即都加1分。之后开始第二次模拟，这一次，我们对棋盘不是一无所知了，至少在 $\\vec s_0$ 状态我们知道落子方法 $\\vec a_0$ 的分值是2，其他都是1，我们使用这个数据的方法是：在这次随机中，我们随机到 $\\vec a_0$ 状态的概率要比其他方法高一点。 之后，我们不断重复以上步骤，这样，那些看起来不错（以最后的胜负来作为判断依据）的落子方案的分数就会越来越高，并且这些落子方案也是比较有前途的，会被更多的选择。 $$ score(\\vec s) = \\begin{pmatrix} r_{11} & r_{12} & \\cdots & r_{1n} \\\\ r_{21} & r_{22} & \\cdots & r_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ r_{n1} & r_{n2} & \\cdots & r_{nn} \\end{pmatrix} $$ 如上述公式所述，n=19，每一个状态 $\\vec s$ 都有一个对应的每个落子点的分数，只要模拟量足够多，那么可以覆盖到的 $\\vec s$ 状态就越多，漏洞就越来越小（可以思考李世石的神之一手，是否触及到了AlphaGo1.0的软肋呢？即没有考虑到的状态 $\\vec s$ ） 最后，当进行了10万盘棋后，在此状态选择那个分数最高的方案落子，此时，才真正下了这步棋。这种过程在论文里被称为Rollout 蒙特卡洛搜索树的方法十分的深刻精巧，充满的创造力，它有一些很有意思的特点： 没有任何人工决策的if else逻辑，完全依照规则本身，通过不断的想象（随机）来进行自我对弈，最后提升这一步的质量。有意思的是，其实这也是遵照了人类下棋的思维模式（模仿，只是这一次模仿的不是下棋风格，而是人类思考的方式。十分奇妙，人从飞鸟中受到启发发明了飞机，从鱼身上受到启发发明了潜艇，现在，机器学习的程序，通过学习人类使自身发生进化），人类中，水平越高的棋手，算的棋越多，只是人类对于每一个落子的判断能力更加强大，思考中的棋路，也比随机方式有效的多，但是机器胜在量大，暴力的覆盖到了很多情况。注意，这一个特点也为之后的提高提供了思路。 MCTS可是持续运行。这种算法在对手思考对策的时候自己也可以思考对策。在对方思考落子的过程中，MCTS也可以继续进行演算，在对面落子后，在用现在棋盘的情况进行演算，并且之前计算的结果一定可以用在现在情况中，因为对手的下的这步棋，很可能也在之前演算的高分落子选择内。这一点十分像人类 MCTS是完全可并行的算法 Aja Huang很快意识到这种方法的缺陷在哪里：初始策略（或者说随机的落子方式）太过简单。就如同上面第一条特点所说，人类对每种 $\\vec s$ （棋型）都要更强的判断能力，那么我们是否可以用 $P_{human}(\\vec s)$ 来代替随机呢？ Aja Huang改进了MCTS，每一步不使用随机，而是现根据 $P_{human}(\\vec s)​$ 计算得到 $\\vec a​$ 可能的概率分布，以这儿概率为准来挑选下一个 $\\vec a​$。一次棋局下完之后，新分数按照下面的方式来更新$$\\text{新分数} = \\text{调整后的初始分} + \\text{通过模拟的赢棋概率} \\tag{2-3}$$如果某一步被随机到很多次，就应该主要依据模拟得到的概率而非 $P_{human}(\\vec s)$ ，就是说当盘数不断的增加，模拟得到的结果可能会好于 $P_{human}(\\vec s)$ 得到的结果。所以 $P_{human}(\\vec s)$ 的初始分会被打个折扣，这也是公式2-3中的调整后的初始分的由来 $$ \\text{调整后的初始分} = \\frac{P_{human}(\\vec s)}{(\\text{被随机到的次数} + 1)} \\tag{2-4} $$ 如此一来，就在整张地图上利用 $P_{human}(\\vec s)$ 快速定位了比较好的落子方案，也增加了其他位置的概率。实际操作中发现，此方案不可行，因为计算这个 $P_{human}(\\vec s)$ 太慢了太慢了 一次 $P_{human}(\\vec s)$ 的计算需要3ms，随机算法1us，慢了3000倍，所以，Aja huang训练了一个简化版本的 $P_{human-fast}(\\vec s)$ ，把神经网络层数、输入特征减少，耗时下降到2us，基本满足了要求。 更多的，策略是，先以 $P_{human}(\\vec s)$ 开局，走前面大概20步，之后再使用 $P_{human-fast}(\\vec s)$ 走完剩下的到最后。兼顾速度和准确性。 综合了深度卷积神经网络和MCTS两种方案，此时的围棋程序已经可以战胜所有其他电脑，虽然和其他人类职业选手还有一定的差距。 2015年2月，Aja Huang在Deepmind的同事在顶级学术期刊nature上发表的文章 Human-level control through deep reinforcement learning 用神经网络打游戏。这篇文章，给AlphaGo提供的了新的方向 强化学习——局面函数（Value Network）强化学习（Reinforcement learning）用来实现左右互搏和自我进化，首先说说这篇论文干了一件什么事情，Deepmind团队的大牛们使用强化学习的方法在红白机上打通了200多个游戏，大多数得分都要比人好。 什么是强化学习那什么是强化学习呢？这里推荐莫烦大神的 什么是强化学习 系列教程的知乎专栏，以及另一篇强化学习指南 后者对强化学习的基本概念，实现方法进行全面的讲解，含有公式推导。还有两篇我自己做的笔记，什么是强化学习，强化学习算法介绍 对于强化学习（Reinforcement learning），它是机器学习的一个分支，特别善於控制一只能够在某个环境下自主行动的个体 (autonomous agent)，透过和环境之间的互动，例如 sensory perception 和 rewards，而不断改进它的 行为。 比如，吃豆人游戏，自主行动的个体就是控制的吃豆人，环境就是迷宫，奖励就是吃到的豆子，行为就是上下左右的操作。 强化学习的输入是 状态 (States) = 环境，例如迷宫的每一格是一个 state 动作 (Actions) = 在每个状态下，有什么行动是容许的 奖励 (Rewards) = 进入每个状态时，能带来正面或负面的 价值 (utility) 输出是 方案 (Policy) = 在每个状态下，你会选择哪个行动？也是一个函数 所以，我们需要根据S，A，R，来确定什么样的P是比较好的，通过不断的进行游戏，获得大量的交互数据，我们可以确定在每一个状态下，进行什么动作能获得最好的分数，而强化学习也就是利用神经网络来拟合这个过程。 例如，打砖块游戏有一个秘诀是把求打到墙后，这样球能自己反弹得分，强化学习程序在玩了600盘后，学到了这个秘诀。也就是说程序会在每一个状态下选择那个更容易把球打到墙后面去的操作。如下图，球快要把墙打穿的时候，评价函数 $v$ 的值会大幅度上升 打墙游戏的评价函数图 我们可以发现，强化学习的基本思路和MCTS后异曲同工之妙，也是在对游戏完全没有了解的情况，通过不断的训练（进行多盘对弈，和获得进行行动后的分数反馈）来进行训练，自我提升。 利用强化学习增强棋力参考这种思路，Aja Huang给围棋也设计了一个评价函数 $v(\\vec s)$ 。此函数的功能是：量化评估围棋局面。使用$v(\\vec s)$可以让我们在MCTS的过程中不用走完全局（走完全盘耗时耗力，效率不高）就发现已经必败。 在利用 $P_{human}(\\vec s)$ 走了开局的20步后，如果有一个 $v(\\vec s_i)$ （i为当前状态）可以直接判断是否能赢，得到最后的结果r，不需要搜索到底，可以从效率（剪枝，优化算法时间复杂度）上进一步增加MCTS的威力。 很可惜的，现有的人类棋谱不足以得出这个评价函数。所以Aja Huang决定用机器和机器对弈的方法来创造新的对局，也就是AlphaGo的左右互搏。 自对弈 神经网络的训练过程和结构 先用 $P_{human}(\\vec s)$ 和 $P_{human}(\\vec s)$ 对弈，比如1万盘，得到1万个新棋谱，加入到训练集中，训练出 $P_{human-1}(\\vec s)$ 。 使用$P_{human-1}(\\vec s)$和$P_{human-1}(\\vec s)$对弈，得到另1万个新棋谱，加入训练集，训练出$P_{human-2}(\\vec s)$。 同理，进行多次的类似训练，训练出$P_{human-n}(\\vec s)$，给最后的新策略命名为$P_{human-plus}(\\vec s)$ （感觉一下，这个$P_{human-plus}(\\vec s)$ 应该挺强力的！这里回顾一下$P_{human}(\\vec s)$是什么：是一个函数，$\\vec a=f(\\vec s)$ 可以计算出当前 $\\vec s$ 下的落子 $\\vec a$ 的分布概率） 使用$P_{human-plus}(\\vec s)$和$P_{human}(\\vec s)$进行对弈，发现$P_{human-plus}(\\vec s)$胜率80%，自对弈的方法被证明是有效的。（这里有一个想法，我在之前，一直加粗随机，之所以自对弈有效，就是因为整过MCTS过程中从来没有放弃过随机，如此一来，大量的计算，就更可能覆盖到更多的可能性，对提高棋力可以产生有效的作用同时。因为概率的问题，不断的自我对弈肯定造成下棋的路数集中，后面也会有体现） 但是事实并没有那么美好，Aja Huang发现，使用$P_{human-plus}(\\vec s)$来代替$P_{human}(\\vec s)$进行MCTS反而棋力会下降。 Aja Huang认为是$P_{human-plus}(\\vec s)$走棋的路数太集中，而MCTS需要更加发散的选择才能有更好的效果。 计算局部评价函数（Value Network）考虑到$P_{human-plus}(\\vec s)$的下法太过集中，Aja Huang计算 $v(\\vec s)$ 的策略是： 开局先用$P_{human}(\\vec s)$走L步，有利于生成更多局面 即使如此，Aja Huang还是觉得局面不够多样，为了进一步扩大搜索空间，在L+1步时，完全随机一个 $\\vec a$ 落子，记下这个状态 $v(\\vec s_{L+1})$ 之后使用$P_{human-plus}(\\vec s)$来进行对弈，直到结束时获得结果r，如此不断对弈，由于L也是一个随机数，我们可以得到，开局、中盘、官子等不同阶段的很多局面 $\\vec s$，和这些局面对应的结果r 有了这些训练样本 $\\langle \\vec s,r\\rangle$，还是使用神经网络，把最后一层改成回归而非分类（这里不是用的分类，而是用的回归，拟合），就得到了一个 $v(\\vec s)$ 来输出赢棋的概率 如上图所示，$v(\\vec s)$ 可以给出下一步落在棋盘上任意位置后，如果双方都用$P_{human-plus}(\\vec s)$来走棋，我方赢棋的概率。实验表明，仅仅使用$P_{human}(\\vec s)$来训练 $v(\\vec s)$ 效果不如$P_{human-plus}(\\vec s)$，强化学习是确实有效的。 总结，强化学习的$P_{human-plus}(\\vec s)$主要是用来获得 $v(\\vec s)$ 局部评估函数。表示的含义是 在状态 $\\vec s$ 下，局面的优劣程度，或者说此时的胜率是多少 $v(\\vec s)$ 局部评估函数拥有在线下不断自我进化的能力（这也是AlphaGo可以随时间越来越强的最重要的部分） 感谢你看到这里，我们已经拥有： $P_{human}(\\vec s)$ 我的老师是人类！ MCTS 乱下，我只看输赢 $v(\\vec s)$ 我能判断局势 有了这些我们距离AlphaGo已经不远了 AlphaGo MTCS流程图解 Aja Huang使用MCTS框架融合局面评估函数 $v(\\vec s)$ 的策略是： 使用$P_{human}(\\vec s)$作为初始分开局，每局选择分数最高的方案落子 到第L步后，改用$P_{human-fast}(\\vec s)$把剩下的棋局走完，同时调用 $v(\\vec s_L)$，评估局面的获胜概率，按照如下规则更新整个树的分数​$$\\text{新分数} = \\text{调整后的初始分} + 0.5*\\text{通过模拟得到的赢棋概率} + 0.5*\\text{局面评估分} \\tag {3-1}$$ 前两项和原来一样 如果待更新的节点就是叶子节点，局面评估分就是 $v(\\vec s_L)$ 如果是待更新的节点是上级节点，局面评估分是该叶子节点 $v(\\vec s)$ 的平均值 如果 $v(\\vec s)$ 是表示大局观，$P_{human-fast}(\\vec s)$表示快速演算，那么上面的方法就是二者的并重，并且Aja Huang团队已经用实验证明0.5 0.5的权重对阵其他权重有95%的胜率 详解AlphaGo VS 樊麾 对局走下某一步的计算过程 详解AlphaGo走某一步棋的过程1 a图使用局部评估函数计算出 $\\vec s$ 状态下其他落子点的胜率 b图MCTS中使用局部评估函数加 $P_{human}(\\vec s)$ 得出的结果 c图MCTS中使用$P_{human}(\\vec s)$（复合算法）和$P_{human-fast}(\\vec s)$走子走到底的结果 详解AlphaGo走某一步棋的过程2 d图深度卷积神经网络使用策略函数计算出来的结果 e图使用公式3-1和相关流程计算出的落子概率 f图演示了AlphaGo和樊麾对弈的计算过程，AlphaGo执黑，樊麾执白。红圈是AlphaGo实际落子额地方。1，2，3和后面的数字表示他想象中的之后樊麾下一步落子的地方。白色方框是樊麾的实际落子。在复盘时，樊麾认为1的走法更好（这说明在樊麾落子后AlphaGo也在进行计算） 总结由于状态数有限和不存在随机性，象棋和五子棋这类游戏理论上可以由终局自底向上的推算出每一个局面的胜负情况，从而得到最优策略。例如五子棋就被验证为先手必胜。 AlphaGo的MCTS属于启发式搜索算法 启发式搜索算法：由当前局面开始，尝试看起来可靠的行动，达到终局或一定步数后停止，根据后续局面的优劣反馈，选择最有行动。通俗来说，就是”手下一招子，心想三步棋“ 围棋是一个NP问题，要穷举的话，解空间巨大。现代优化算法的经典之处在于，从围棋的规则来看，在某一个状态，必定有一个或几个较优解，整个AlphaGo就是想方设法的去找这个较优解。利用局面评估函数来对MCTS进行剪枝的思路十分精彩。利用上面的3个算法，结合庞大的并行运算能力，还有Aja Huang团队的辛苦付出，造就了AlphaGo的奇迹。 使用不同组件AlphGo1.0的棋力 最终棋力结果 上图显示了各种算法的棋力，Rollout是走棋演算，也就是MCTS，Value Network是 $v(\\vec s)$ 局面评估函数，Policy Network 是结合$P_{human-plus}(\\vec s)$和$P_{human}(\\vec s)$后计算的策略函数（下一步走在哪里胜率高的深度卷积神经网络） 整个AlphaGo使用的技术，深度卷积网络，强化学习神经网络，都是炙手可热的领域，近年来发展迅猛，日新月异。AlphaGo已经完成了自己历史使命，借助棋类的巅峰【围棋】为叩门砖打开了机器学习自我进化的大门 李世石 VS AlphaGo 1.0——第四局78手挖 李世石78手 赛后AlphaGo之父给出的关键信息：李世石78手“挖”是AlphaGo认为概率极小的点，这一手之后导致的状态 $\\vec s$ 进入到了AlphaGo能处理的范围之外，即之前AlphaGo的自对弈都是建立用自己觉得好的下法来搜索的，那么如果这一手AlphaGo1.0感觉可能性极小，那么用$P_{human}(\\vec s)$自对弈的棋谱中就更加难以覆盖。 但是也需要提到的是，根据比赛中柯洁等人的观战我们知道，如果不是后面AlphaGo进入了混乱模式，78手不一定是一个好棋。只能说这一手，顶到了AlphaGo的软肋，在真正和人的对局中不一定是“神之一手” 根据Deepmind团队给出的数据可以知道，一年前，AphaGo1.0的搜索空间，自对弈深度并不完美。所以Deepmind团队有意的在代码逻辑上让其避免打劫，或者说避免劫争，例如，有两个选择，一个胜率60%但需要打劫，另一个55%但不需要打劫，AlphaGo1.0会选择后者。 那么什么是打劫呢？解释这几个和”劫“有关的围棋术语是： 打劫围棋术语，一方制造事端，和另一方讨价还价的行为。劫材可以用来做价格谈判的筹码。通常是走一手没戏，但对手若不予置理，再走第二手会出棋的局部。寻劫通过目数计算，寻找一些有价值的局部制造事端强迫对手应答。通常价值至少需要和打劫的地方相当或者小不太多，否则对方很容易消劫。利用劫劫胜可杀死对方或者得到利益，劫败也应该让对方付出代价，除非双方劫材大小和数量相差悬殊。 通俗的说是，我在这一片已经处于劣势，我换一个战场，发动进攻，你应不应？可能在另外战场的角力中对这边战场的局势产生影响。可以类比于，五子棋中的冲四。 如果有人观看了这一盘棋，我们也可以听到柯洁在强调，AlphaGo在避免打劫，出现了几手莫名其妙的落子。 总结来说，AlphaGo依靠的是对局外的大量计算，无论是局部评估函数，还是$P_{human-plus}(\\vec s)$都十分依赖对局外的大量的计算。随着时间的推移，AlphaGo在对局过程需要的时间越来越固定，不需要在对局时进行太多的MCTS搜索就能获得AlphaGo的下一手位置，可以预见，MCTS的搜索深度不会太深。当计算量十分庞大的时候，依赖更多是那个120层的Policy Network。 从柯洁的第二盘可以发现，他已经努力的制造在中腹引入多方战斗的带劫争的复杂棋局，十分精彩。可惜，AlphaGo2.0貌似已经完善了自己的阿特留斯之踵。当真无敌，说到这里，我们来谈谈AlphaGo2.0 AlphaGo 2.0 VS 柯洁——虽败犹荣三盘对局，感觉到AlphaGo在这一年内进行了极为深度的训练。最可怕的是AlphaGo通过时间验证了机器学习对于解决NP问题的强大潜力（通过这三盘可以看出已经无限接近解决了这个问题，至少在对人类上）。甚至： 臆想一，是否可以利用AlphaGo来判断规则是否公平（中国和韩日规则的不同，7目半和6目半）。 臆想二，最终AlphaGo的自对弈是接近和棋。可惜AlphaGo已经退役。希望针对Deepmind放出的50盘自对弈棋谱可以研究出一些门道，使得围棋这门竞技本身有更大的突破。 局面函数和策略函数愈发强大，愈加的接近于”围棋之神“。 随着Google TPU的发布，跑在TPU阵列上的AlphaGo如虎添翼，MCTS的走子演算效率更高，速度更快（加速的其实就是$P_{human-plus}(\\vec s)$的落子速度。 关于TPU的设计思路和原理可以参考 In-Datacenter Performance Analysis of a Tensor Processor 对于围棋这个策略单步游戏，是存在N步最优解（不存在i+1步最优解），AlphaGo已经在正确的道路上无限的接近于这个N步最优解，仿佛在某一步已经看到了你无论怎么下都能走到的N步最优解。 人类的每一次失误都会使局部评估函数往胜率移动一点，这一点是十分可怕的，因为算法本身的优越性，大局观对于AlphaGo的逻辑来说本身就是一种刻在骨子里的基因 一是因为AlphaGo每次MCTS计算都会计算到接近分出胜负，具有前瞻性 二是因为局面函数本身就是为了来统计大局形势定义的，具有判断局面优劣的能力 所谓大局观，不就是这种走一步看N步的能力嘛。 对未来的展望——从AlphaGo想开去珍贵的并不是攻克了围棋问题本身，而是这种解决问题的基本模式，可以推而广之到很多领域。 先通过卷积网络学习人类的下法，算出策略函数（Policy Network），再通过模仿进行强化学习，左右互搏，不断自我进化，再加上MCTS的经典的解决问题的启发式搜索算法。 这俨然是一个 模仿➜学习➜优化的过程 或许，模仿人类，是机器学习最终的归途，至于应用领域方面 游戏AI是一个最容易想到的领域，只要能抽象出 State Action Judgement，那么这一套解决问题的方式就可以举一反三，让每一个1V1领域的游戏AI非常强大（OpenAI在Dota2 1V1 Solo上的结果更加证明了这一点），至于合作领域的AI可能需要更大的计算量去计算（OpenAI发布的论文MultiAgent很有启发性），对于实际问题来说获得这样的AI有多大的经济价值值得推敲。 游戏的乐趣就在于不确定性，适当的失误也是竞技类游戏的魅力所在，一个能看到N步最优解的AI会让一个游戏机制，游戏规则变得可数据化，这一点其实是游戏被创造出来的初衷相背离的。 其他方面，只要是人类可以学习出来的事物，比如翻译，编程，都是现在的这套体系可能解决的问题，我们期待未来这套解决问题的方法发挥出无穷的力量吧！ [Reference]知乎Tao Lei大神的回答知乎袁行远大神的回答知乎有关围棋打劫的回答其他文章中引用的论文，链接已经给出","tags":[{"name":"AlphaGo","slug":"AlphaGo","permalink":"https://charlesliuyx.github.io/tags/AlphaGo/"},{"name":"CNN","slug":"CNN","permalink":"https://charlesliuyx.github.io/tags/CNN/"},{"name":"MCTS","slug":"MCTS","permalink":"https://charlesliuyx.github.io/tags/MCTS/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://charlesliuyx.github.io/tags/Deep-Learning/"}]},{"title":"清欢 - 林清玄","date":"2017-05-14T16:26:55.000Z","path":"2017/05/14/清欢/","text":"1少年时代读到苏轼的一阕词，非常喜欢，到现在还能背诵： 细雨斜风作晓寒，淡烟疏柳媚晴滩，入淮清洛渐漫漫。 雪沫乳花浮午盏，蓼茸蒿笋试春盘，人间有味是清欢。 这阕词，苏轼在旁边写着“元丰七年十二月二十四日，从泗州刘倩叔游南山”，原来是苏轼和朋友到郊外去玩，在南山里喝了浮着雪沫乳花的淡茶，配着春日山野里的蓼菜、茼蒿、新笋，以及野草的嫩芽等等，然后自己赞叹着：“人间有味是清欢！” 当时所以能深记这阕词，最主要的是爱极了后面这一句，因为试吃野菜的这种平凡的清欢，才使人间更有滋味。“清欢”是什么呢？清欢几乎是难以翻译的，可以说是“清淡的欢愉”，这种清淡的欢愉不是来自别处，正是来自对平静疏淡简朴生活的一种热爱。当一个人可以品味出野菜的清香胜过了山珍海味，或者一个人在路边的石头里看出了比钻石更引人的滋味，或者一个人听林间鸟鸣的声音感受到比提笼遛鸟更感动，或者体会了静静品一壶乌龙茶比起在喧闹的晚宴中更能清洗心灵……这些就是“清欢”。 清欢之所以好，是因为它对生活的无求，是它不讲求物质的条件，只讲究心灵的品味。“清欢”的境界很高，它不同于李白的人生在世不称意，明朝散发弄扁舟那样的自我放逐；或者人生得意须尽欢，莫使金樽空对月那种尽情的欢乐。它也不同于杜甫的人生有情泪沾臆，江水江花岂终极这样悲痛的心事，或者人生不相见，动如参与商；今夕复何夕，共此灯烛光那种无奈的感叹。 活在这个世界上，有千百种人生，文天祥的是人生自古谁无死，留取丹心照汗青，我们很容易体会到他的壮怀激烈。欧阳修的是人生自是有情痴，此恨不关风与月，我们很能体会到他的绵绵情恨。纳兰性德的是人到情多情转薄，而今真个不多情，我们也不难会意到他无奈的哀伤。甚至于像王国维的人生只似风前絮，欢也零星，悲也零星，都作连江点点萍！那种对人生无常所发出的刻骨的感触，也依然能够知悉。 2可是“清欢”就难了！ 尤其是生活在现代的人，差不多是没有清欢的。 什么样是清欢呢？我们想在路边好好的散个步，可是人声车声不断的呼吼而过，一天里，几乎没有纯然安静的一刻。 我们到馆子里，想要吃一些清淡的小菜，几乎是杳不可得，过多的油、过多的酱、过多的盐和味精已经成为中国菜最大的特色，有时害怕了那样的油腻，特别嘱咐厨子白煮一个菜，菜端出来时让人吓一跳，因为菜上挤的色拉比菜还多。 有时没有什么事，心情上只适合和朋友去啜一盅茶、饮一杯咖啡，可惜的是，心情也有了，朋友也有了，就是找不到地方，有茶有咖啡的地方总是嘈杂的。 俗世里没有清欢了，那么到山里去吧！到海边去吧！但是，山边和海湄也不纯净了，凡是人的足迹可以到的地方，就有了垃圾，就有了臭秽，就有了吵闹！ 有几个地方我以前常去的，像阳明山的白云山庄，叫一壶兰花茶，俯望着台北盆地里堆叠着的高楼与人欲，自己饮着茶，可以品到茶中有清欢。像在北投和阳明山间的山路边有一个小湖，湖畔有小贩卖工夫茶，小小的茶几、藤制的躺椅，独自开车去，走过石板的小路，叫一壶茶，在躺椅上静静的靠着，有时湖中的荷花开了，真是惊艳一山的沉默。有一次和朋友去，在躺椅上静静喝茶，一下午竟说不到几句话，那时我想，这大概是“人间有味是清欢”了。 现在这两个地方也不能去了，去了只有伤心。湖里的不是荷花了，是飘荡着的汽水罐子，池畔也无法静静躺着，因为人比草多，石板也被踏损了。到假日的时候，走路都很难不和别人推挤，更别说坐下来喝口茶，如果运气更坏，会遇到呼啸而过的飞车党，还有带伴唱机来跳舞的青年，那时所有的感官全部电路走火，不要说清欢，连欢也不剩了。 要找清欢，一日比一日更困难了。 当学生的时候，有一位朋友住在中和圆通寺的山下，我常常坐着颠踬的公交车去找她，两个人沿着上山的石阶，漫无速度的，走走、坐坐、停停、看看，那时圆通寺山道石阶的两旁，杂乱的长着朱槿花，我们一路走，顺手拈下一朵熟透的朱槿花，吸着花朵底部的花露，其甜如蜜，而清香胜蜜，轻轻的含着一朵花的滋味，心里遂有一种只有春天才会有的欢愉。 3圆通寺是一座全由坚固的石头砌成的寺院，那些黑而坚强的石头坐在山里仿佛一座不朽的城堡，绿树掩映，清风徐徐，站在用石板铺成的前院里，看着正在生长的小市镇，那时的寺院是澄明而安静的，让人感觉走了那样高的山路，能在那平台上看着远方，就是人生里的清欢了。 后来，朋友嫁人，到国外去了。我去过一趟圆通寺，山道已经开辟出来，车子可以环山而上，小山路已经很少人走，就在寺院的门口摆着满满的摊贩，有一摊是儿童乘坐的机器马，叽哩咕噜的童歌震撼半山，有两摊是打香肠的摊子，烤烘香肠的白烟正往那古寺的大佛飘去，有一位母亲因为不准孩子吃香肠而揍打着两个孩子，激烈的哭声尖亢而急促……我连圆通寺的寺门都没有进去，就沉默的转身离开，山还是原来的山，寺还是原来的寺，为什么感觉完全不同了，失去了什么吗？失去的正是清欢。 下山时的心情是不堪的，想到星散的朋友，心情也不是悲伤，只是惆怅，浮起的是一阕词和一首诗，词是李煜的：高楼谁与上？长记秋晴望。往事已成空，还如一梦中！诗是李觏的：人言落日是天涯，望极天涯不见家；已恨碧山相阻隔，碧山还被暮云遮！那时正是黄昏，在都市烟尘蒙蔽了的落日中，真的看到了一种悲剧似的橙色。 我二十岁心情很坏的时候，就跑到青年公园对面的骑马场去骑马，那些马虽然因驯服而动作缓慢，却都年轻高大，有着光滑的毛色。双腿用力一夹，它也会如箭一般呼噜向前窜去，急忙的风声就从两耳掠过，我最记得的是马跑的时候，迅速移动着的草的青色，青茸茸的，仿佛饱含生命的汁液，跑了几圈下来，一切恶的心情也就在风中、在绿草里、在马的呼啸中消散了。 尤其是冬日的早晨，勒着绳，马就立在当地，踢踏着长腿，鼻孔中冒着一缕缕的白气，那些气可以久久不散，当马的气息在空气中消弭的时候，人也好像得到某些舒放了。 骑完马，到青年公园去散步，走到成行的树荫下，冷而强悍的空气在林间流荡，可以放纵的、深深的呼吸，品味着空气里所含的元素，那元素不是别的，正是清欢。 4最近有一天，突然想到骑马，已经有十几年没骑了。到青年公园的骑马场时差一点吓昏，原来偌大的马场已经没有一根草了，一根草也没有的马场大概只有台湾才有，马跑起来的时候，灰尘滚滚，弥漫在空气里的尽是令人窒息的黄土，蒙蔽了人的眼睛。马也老了，毛色斑剥而失去光泽。 最可怕的是，不知道什么时候在马场搭了一个塑料棚子，铺了水泥地，其丑无比，里面则摆满了机器的小马，让人骑用，其吵无比。为什么为了些微的小利，而牺牲了这个马场呢？ 马会老是我知道的事，人会转变是我知道的事，而在有真马的地方放机器马，在马跑的地方没有一株草，则是我不能理解的事。 就在马场对面的青年公园，已经不能说是公园了，人比西门町还拥挤吵闹，空气比咖啡馆还坏，树也萎了，草也黄了，阳光也不灿烂了。从公园穿越过去，想到少年时代的这个公园，心痛如绞，别说清欢了，简直像极了佛经所说的“五浊恶世”！ 生在这个时代，为何“清欢”如此难觅。眼要清欢，找不到青山绿水；耳要清欢，找不到宁静和谐；鼻要清欢，找不到干净空气；舌要清欢，找不到蓼茸蒿笋；身要清欢，找不到清凉净土；意要清欢，找不到智慧明心。如果要享受清欢，唯一的方法是守在自己小小的天地，洗涤自己的心灵，因为在我们拥有愈多的物质世界，我们的清淡的欢愉就日渐失去了。 现代人的欢乐，是到油烟爆起、卫生堪虑的啤酒屋去吃炒蟋蟀；是到黑天暗地、不见天日的卡拉OK去乱唱一气；是到乡村野店、胡乱搭成的土鸡山庄去豪饮一番；以及到狭小的房间里做方城之戏，永远重复着摸牌的一个动作……这些放逸的生活以为是欢乐，想起来毋宁是可悲的。为什么现代人不能过清欢的生活，反而以浊为欢，以清为苦呢？ 一个人以浊为欢的时候，就很难体会到生命清明的滋味，而在欢乐已尽、浊心再起的时候，人间就愈来愈无味了。 5这使我想起东坡的另一首诗来： 梨花淡白柳深青，柳絮飞时花满城； 惆怅东栏一株雪，人生看得几清明？ 苏轼凭着东栏看着栏杆外的梨花，满城都飞着柳絮时，梨花也开了遍地，东栏的那株梨花却从深青的柳树间伸了出来，仿佛雪一样的清丽，有一种惆怅之美，但是人生看这么清明可喜的梨花能有几回呢？这正是千古风流人物的性情，这正是清朝大画家盛大士在《溪山卧游录》中说的凡人多熟一分世故，即多一分机智。多一分机智，即少却一分高雅。 也有说山中何所有？岭上多白云，只可自怡悦，不堪持赠君，自是第一流人物。 第一流人物是什么人物？ 第一流人物是在清欢里也能体会人间有味的人物！ 第一流人物是在污浊滔滔的人间，也能找到清欢的人物！","tags":[{"name":"随笔","slug":"随笔","permalink":"https://charlesliuyx.github.io/tags/随笔/"}]},{"title":"English-abbreviation","date":"2017-05-14T00:29:00.000Z","path":"2017/05/13/English-abbreviation/","text":"一些常用的英语缩写的总结 日常生活篇 R.S.V.P: 源自于法语‘Répondez s’il vous plait’，英文解释为’Respond,if you please’.邀请函结尾写这个，表示‘敬请回复’； P.S: 意思是‘post script’,表示‘再多说一句’，一般写完要说的话之后结尾突然想起说什么可以写； ASAP: as soon as possible. 表示‘尽快’，注意听音频发音，可读成A-SAP; ETA: estimated time of arrival. 表示‘预计到达时间’； BYOB: bring your own bottle; 表示‘自带酒水，举办派对时常用’ 吃饭做菜篇 tsp or t : teaspoon 一茶匙 tbs / tbsp/ T: tablespoon 一汤匙 c: cup 一杯 gal: gallon 加仑 lb : pound 磅 pt：pint 品脱 qt: quart 夸脱 出国地图篇 Ave: avenue 大街 Blvd: boulevard 大道 Ln: lane 车道 Rd: road 公路 St: street 街道 教育工作篇 BA: Bachelor of Arts 文学士 BS: Bachelor of Science 理学士 MA: Master of Arts 文科硕士 PA: Personal Assistant 私人助理 VP: Vice President 副总统;副总裁 CEO: Chief Executive Officer 首席执行官 CFO: Chief Financial Officer 首席财务官 COO: Chief Operating Officer 首席运营官 CMO: Chief Marketing Officer 首席营销官 社交聊天篇 JK :just kidding 跟你开玩笑呢 TBD: to be determined 待定 AFAIK: as far as I know 据我所知 BRB: be right back 马上回来 CUL: see you later 回见 TTYL: talk to you later 回聊 CWYL: chat with you later 回聊 LOL: laugh out loud 哈哈 LMAO: laugh my ass off 笑死我了 ROTFL/ ROFL: rolling on the floor laughing 笑到在地上打滚 NP: no problem 没问题,没关系,不客气 IDK: I don’t know 我不知道 ILY: I love you 我爱你 TMI: too much information 信息量太大了； 说的太多了 OIC: Oh, I see. 我明白了 FYI: for your information 顺便告知你 BTW: by the way 顺便说一下 顺便问一下 MYOB: mind your own business 别多管闲事 FAQ: frequently asked questions 经常被问的问题20: WTF: what the fuck 搞毛阿…… 委婉的是WTH: what the hell/heck21: AKA: also known as. 也叫做 TGIF: thank god It’s Friday 谢天谢地又到礼拜五了 TBC: to be continued; to be confirmed 未完待续/ 有待确认 数字字母篇2: to/too4: forB: beC: seeI: eyeO: owe;R: are;U: you;ur: your/you’reY: why","tags":[{"name":"英语积累","slug":"英语积累","permalink":"https://charlesliuyx.github.io/tags/英语积累/"}]},{"title":"有关中国诗的那些事","date":"2017-05-13T23:40:00.000Z","path":"2017/05/13/有关中国诗的那些事/","text":"没有沉淀，文字永远上不了档次。难得空闲，读了些诗，有些感受。 韦应物 独怜幽草涧边生，上有黄鹂深树鸣。春潮带雨晚来急，野渡无人舟自横。 记起这《滁州西涧》，听过一个故事。话说一次国画比赛，题目是以春潮带雨晚来急，野渡无人舟自横这句诗作画。国学博大精深，国画作为其中一支配起诗来，别有遐想。此题甚好，不仅考及画技，更有对国学中诗词的体悟和见解。大家不妨也想想如果是你，你会怎么画？这里先卖个关子。 从诗的字面来说，是这样一种通感：春天近了，潮气依稀可嗅。但谁能像你这样，对一棵在水边生长的小草也充满爱怜？黄鹂在密林深处的低语你都能听到？这需要多么细腻的一颗心。华灯初上，渡口上已经没有人，舟独自横于水上，那是一种空阔的感觉。映照你一生步履，你的细腻出于岁月。你当年49岁，50载，可能不长，但是我知道你的与众不同，你的50载甚至顶得别人几辈子。 韦应物年少荒唐，并未认真读书。安史乱起，韦应物扈从不及，流落秦中。乱后，韦应物折节读书，痛改前非，从一个富贵无赖纨绔一变而为忠厚仁爱的儒者。有些官运，在地方（苏州）任官。韦应物勤于吏职，简政爱民，在苏州刺史届满之后，一贫如洗，寄居无定寺，客死他乡。 享年五十五岁。 别人些许看出的是你不在其位，不得其用的无奈，忧伤。但我看到的，更多是你的豁达，你心中总是美好多于忧伤。 通往远方的路，没有哪条是你不能走的；走在路上的人，没有谁是你不能结交的；结交的朋友，没有谁是你不能推心置腹的。虽然那个时代远没有现在的复杂，但是能捧出一颗完整的心也并不是一件容易的事。韦苏州，你是一个充满诗情的人。 回头看看开头提到的国画比赛优胜者的作品：弥蒙的雾气用模糊的淡墨衬托，远处的群山，夕阳露出半个头。远远的有几簇灯火，近处，一条小舟在几根芦苇中飘荡，船上有位着布衣的蓑翁，嘴里叼根芦苇，帽檐下压，不知是否在闭目养神，两只杜鹃立于船头。起初不懂，“无人”的野渡为何有人呢？其中深意，结合了韦苏州的履历才恍然大悟。 “无人”并不是一只孤舟。韦应物闲居，船上舟子，好似当时的韦应物，在船头打盹，闻着草香，听着鹂鸣。韦应物虽然赋闲苏州，但他并不排斥官场，若有机会，他还是会出仕，只是满足于闲暇。无奈忧伤可能有，但经历了顽劣，奋起，战乱，官场，贬谪，闲居的韦应物，更多的，是看破人生的豁达和满足。 李白总觉中国诗总离不开一个“愁”字。思乡，思亲，忧国，羁旅等等，都和“愁”万缕千丝。我爱这些无奈，悲壮，不舍，甚至愤懑，嘲讽。他们仿佛缩影了人生，视角令人称奇，细腻的令你悸动。 抽刀断水，是最无奈的神话；举杯消愁，是最动情的悲歌。李白潇洒一生，他豪放，甚至一直清贫，有了几个钱，就豪饮一番，将诗情挥洒，更是对“愁”下了如此入理的定义。 拣尽寒枝不肯栖，寂寞沙洲岭李白就犹如谪仙，似乎从来没有受过来自这个世界的温暖。于是，在静夜里，李白写下了床前明月光，疑是地上霜。举头望明月，低头思故乡的千古“愁”词。可是李白的故乡在哪里呢？是陇西？是巴蜀？月华似霜的夜，浪迹天涯的游子李白在梦幻中寻觅故乡，但故乡却比梦幻更飘渺。 李白是复杂的，李白糅合着道家的“出世”和儒家的“入世”思想。所以，顺境时，他仰天大笑出门去，我辈岂是蓬蒿人的潇洒豪情；逆境时，他有弃我去者昨日之日不可留，乱我心者今日之日多烦忧的绵绵愁绪。 那些诗人感动于张谓笔下早梅傲雪不知近水花先发，疑是经冬雪未消的玄妙；陶醉于贺铸风中一川烟草，满城风絮，梅子黄时雨的飘愁；哀婉于苏轼眼中细看来，不是杨花，点点是，离人泪的破碎。 说起苏东坡，一个传奇。 人生如梦，东坡曾经迷惘过；早生华发，东坡曾经惋惜过；十年生死两茫茫，东坡曾经痛苦过。但他不屈，他平和，他豁达。 一蓑烟雨任平生，他淡泊；日啖荔枝三百颗，不辞长作岭南人，他自定；踏雪飞鸿，他淡然。问汝平生功业，黄州惠州儋州，三贬之地，还恰恰就是他留下许多不朽之作的地方。 读着这些诗，深深思索，你会感到作为一个中国人学会了中文，有着五千年的浩瀚历史文化，是多么令你振奋和自豪；国学，遗留的东西，值得我们用一生去参悟。常说高考诗词理解令人头痛，如果怀着这样的心情读诗，你还会怕吗？","tags":[{"name":"随笔","slug":"随笔","permalink":"https://charlesliuyx.github.io/tags/随笔/"}]}]